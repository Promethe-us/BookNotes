{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1、2 介绍、MNIST(cifar10)全流程\n",
    "> 目录  \n",
    "> 1,numpy实现波士顿房价预测  \n",
    "> 2,paddlle.fluid实现波士顿房价预测  \n",
    "> 3,cifar10(简单CNN)  \n",
    ">>   3.1 异步读取数据  \n",
    "    3.2 分布式训练  \n",
    "    3.3 动态学习率  \n",
    "    3.4 VisualDL\n",
    "    3.5 恢复训练\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参数W与输入X组成的公式的基本结构称为假设H\n",
    "- 模型假设H、评价函数(损失)、优化算法 是构成一个模型的三个部分\n",
    "- 假设——>评价——>优化 \n",
    "- 训练过程中的学习就是参数估计\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- (1) 神经网络思想的提出已经是75年前的事情了，但是直到2010年， 大数据涌现、硬件发展、算法优化 使得深度学习真正进入兴起时期。\n",
    "\n",
    "基于深度学习的顶级会议 ICLR 统计，深度学习相关论文数量呈逐年递增的状态。\n",
    "\n",
    "\n",
    "|ICML、KDD|与数据和模型技术相关的论文|\n",
    "|--|--|\n",
    "|CVPR|专注视觉|\n",
    "|EMNLP|自然语言处理|\n",
    "\n",
    "- (2) 实现了端到端的学习\n",
    "\n",
    "在深度学习兴起之前，很多领域建模思路是投入大量精力做特征工程，将专家对某个领域的“人工理解”做成特征表达，然后使用简单的分类或回归网络完成任务。\n",
    "\n",
    "但是在数据充足的情况下，深度学习模型可以实现端到端的学习，不需要做特征工程，将“原始特征”输入到“端到端模型”中，模型输出即可完成任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1,波士顿房价预测\n",
    "\n",
    "[np.fromfile详解](https://zhuanlan.zhihu.com/p/136744114)\n",
    "\n",
    "|||\n",
    "|--|--|\n",
    "|np.savetxt('a.csv',a,fmt='%.1f',delimiter=',')|将数组中的数据写入txt或者csv,多是逗号分隔|\n",
    "|np.loadtxt('a.csv',dtype=np.int,delimiter=\",\")|读取csv或者txt文件中的数据到数组中|\n",
    "|np.tofile(\"a.bat\",sep=\",\",format='%d')|将数据压缩成一维再存储|\n",
    "|np.fromfile(\"a.bat\",dtype=np.int,sep=',').reshape(4,5)|读入二进制文件|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-09T01:44:07.664699Z",
     "iopub.status.busy": "2022-01-09T01:44:07.664525Z",
     "iopub.status.idle": "2022-01-09T01:44:09.074824Z",
     "shell.execute_reply": "2022-01-09T01:44:09.074231Z",
     "shell.execute_reply.started": "2022-01-09T01:44:07.664680Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0 of epoch:0: loss:2.2345300046313477\n",
      "iter:0 of epoch:1: loss:1.6970206220391038\n",
      "iter:0 of epoch:2: loss:1.0467343732853\n",
      "iter:0 of epoch:3: loss:1.6628371792032337\n",
      "iter:0 of epoch:4: loss:1.441134310777762\n",
      "iter:0 of epoch:5: loss:1.5239516787217393\n",
      "iter:0 of epoch:6: loss:1.739225536395752\n",
      "iter:0 of epoch:7: loss:1.7576329005945586\n",
      "iter:0 of epoch:8: loss:1.6940953401760865\n",
      "iter:0 of epoch:9: loss:1.817939488307556\n",
      "iter:0 of epoch:10: loss:1.8290803017518784\n",
      "iter:0 of epoch:11: loss:1.6583785573956162\n",
      "iter:0 of epoch:12: loss:1.4389650424676246\n",
      "iter:0 of epoch:13: loss:1.521185091630357\n",
      "iter:0 of epoch:14: loss:1.3624862454097246\n",
      "iter:0 of epoch:15: loss:1.3695605635644308\n",
      "iter:0 of epoch:16: loss:1.2771501757166654\n",
      "iter:0 of epoch:17: loss:1.0037817804303306\n",
      "iter:0 of epoch:18: loss:1.5575464710794287\n",
      "iter:0 of epoch:19: loss:1.095856996023564\n",
      "iter:0 of epoch:20: loss:1.132534877668208\n",
      "iter:0 of epoch:21: loss:1.2181318746069247\n",
      "iter:0 of epoch:22: loss:2.162389608224804\n",
      "iter:0 of epoch:23: loss:0.8596097182923552\n",
      "iter:0 of epoch:24: loss:1.0779109424254512\n",
      "iter:0 of epoch:25: loss:0.8543257207538988\n",
      "iter:0 of epoch:26: loss:1.0794246102300935\n",
      "iter:0 of epoch:27: loss:0.7389892422542049\n",
      "iter:0 of epoch:28: loss:0.9741969136943007\n",
      "iter:0 of epoch:29: loss:1.1608588986143877\n",
      "iter:0 of epoch:30: loss:0.8655492845184172\n",
      "iter:0 of epoch:31: loss:0.9415054200981878\n",
      "iter:0 of epoch:32: loss:1.0590599470831141\n",
      "iter:0 of epoch:33: loss:0.6816391637237964\n",
      "iter:0 of epoch:34: loss:1.2074094691337356\n",
      "iter:0 of epoch:35: loss:0.8459657413160319\n",
      "iter:0 of epoch:36: loss:0.6054845649326741\n",
      "iter:0 of epoch:37: loss:0.6681199122231403\n",
      "iter:0 of epoch:38: loss:0.9027563375371523\n",
      "iter:0 of epoch:39: loss:0.6497968768602822\n",
      "iter:0 of epoch:40: loss:0.41811956009451284\n",
      "iter:1 of epoch:0: loss:0.4733069673741964\n",
      "iter:1 of epoch:1: loss:0.6311520034788133\n",
      "iter:1 of epoch:2: loss:0.8252185583836583\n",
      "iter:1 of epoch:3: loss:1.2612062505384911\n",
      "iter:1 of epoch:4: loss:0.5514374827238103\n",
      "iter:1 of epoch:5: loss:0.5109739198117063\n",
      "iter:1 of epoch:6: loss:1.0861008418185492\n",
      "iter:1 of epoch:7: loss:0.4182341621485565\n",
      "iter:1 of epoch:8: loss:1.220374120856\n",
      "iter:1 of epoch:9: loss:0.9384449715066252\n",
      "iter:1 of epoch:10: loss:0.5013067233862591\n",
      "iter:1 of epoch:11: loss:0.8547567244909949\n",
      "iter:1 of epoch:12: loss:0.6308690709902157\n",
      "iter:1 of epoch:13: loss:0.4295065498211067\n",
      "iter:1 of epoch:14: loss:0.6754736344179312\n",
      "iter:1 of epoch:15: loss:0.5178681182897338\n",
      "iter:1 of epoch:16: loss:0.520459095692187\n",
      "iter:1 of epoch:17: loss:0.7155727693347107\n",
      "iter:1 of epoch:18: loss:0.8398344332746577\n",
      "iter:1 of epoch:19: loss:0.509596136246741\n",
      "iter:1 of epoch:20: loss:0.6731499064515927\n",
      "iter:1 of epoch:21: loss:1.283445682906096\n",
      "iter:1 of epoch:22: loss:0.5544916815656012\n",
      "iter:1 of epoch:23: loss:0.6433349569677693\n",
      "iter:1 of epoch:24: loss:0.30948834911757256\n",
      "iter:1 of epoch:25: loss:0.865077292305355\n",
      "iter:1 of epoch:26: loss:1.7421151049647006\n",
      "iter:1 of epoch:27: loss:0.5570958980978503\n",
      "iter:1 of epoch:28: loss:0.41463970407149037\n",
      "iter:1 of epoch:29: loss:0.3001250242661585\n",
      "iter:1 of epoch:30: loss:0.48433607016501173\n",
      "iter:1 of epoch:31: loss:0.2926801519263228\n",
      "iter:1 of epoch:32: loss:2.5220303831786497\n",
      "iter:1 of epoch:33: loss:0.5376583971409549\n",
      "iter:1 of epoch:34: loss:0.49594701824887777\n",
      "iter:1 of epoch:35: loss:0.44131818124963973\n",
      "iter:1 of epoch:36: loss:0.47632064687895026\n",
      "iter:1 of epoch:37: loss:0.5637226059985271\n",
      "iter:1 of epoch:38: loss:1.1549132810198803\n",
      "iter:1 of epoch:39: loss:0.41492223376266163\n",
      "iter:1 of epoch:40: loss:0.9188290794196211\n",
      "iter:2 of epoch:0: loss:0.6913214460337849\n",
      "iter:2 of epoch:1: loss:0.42984838673307807\n",
      "iter:2 of epoch:2: loss:0.28020889422796746\n",
      "iter:2 of epoch:3: loss:0.8539476726637016\n",
      "iter:2 of epoch:4: loss:0.2661039967607142\n",
      "iter:2 of epoch:5: loss:0.46420315646628163\n",
      "iter:2 of epoch:6: loss:0.8238186627924422\n",
      "iter:2 of epoch:7: loss:0.7460920655739759\n",
      "iter:2 of epoch:8: loss:0.42040053671835664\n",
      "iter:2 of epoch:9: loss:0.9178482526912809\n",
      "iter:2 of epoch:10: loss:0.5368660685085083\n",
      "iter:2 of epoch:11: loss:0.37526674540478516\n",
      "iter:2 of epoch:12: loss:1.1297429596376372\n",
      "iter:2 of epoch:13: loss:0.5407251822552298\n",
      "iter:2 of epoch:14: loss:0.35445001083680394\n",
      "iter:2 of epoch:15: loss:0.890764962851342\n",
      "iter:2 of epoch:16: loss:0.7632769654565608\n",
      "iter:2 of epoch:17: loss:0.47177587470812216\n",
      "iter:2 of epoch:18: loss:0.280213894073623\n",
      "iter:2 of epoch:19: loss:0.5691742083194058\n",
      "iter:2 of epoch:20: loss:0.4218008615932196\n",
      "iter:2 of epoch:21: loss:0.6678827168168489\n",
      "iter:2 of epoch:22: loss:0.3795477302393103\n",
      "iter:2 of epoch:23: loss:0.6575709575082632\n",
      "iter:2 of epoch:24: loss:0.352460882702915\n",
      "iter:2 of epoch:25: loss:0.7731995542393426\n",
      "iter:2 of epoch:26: loss:0.7696521392807882\n",
      "iter:2 of epoch:27: loss:0.19259860008897653\n",
      "iter:2 of epoch:28: loss:0.2803399947794737\n",
      "iter:2 of epoch:29: loss:1.3435653637634912\n",
      "iter:2 of epoch:30: loss:0.6599612113676036\n",
      "iter:2 of epoch:31: loss:1.351835076229\n",
      "iter:2 of epoch:32: loss:0.4603676148371413\n",
      "iter:2 of epoch:33: loss:0.3641493030330106\n",
      "iter:2 of epoch:34: loss:0.40872641571404406\n",
      "iter:2 of epoch:35: loss:0.23749850490554758\n",
      "iter:2 of epoch:36: loss:0.5729760720125447\n",
      "iter:2 of epoch:37: loss:0.3512494706436735\n",
      "iter:2 of epoch:38: loss:0.9307111133737516\n",
      "iter:2 of epoch:39: loss:1.7644623292399775\n",
      "iter:2 of epoch:40: loss:0.21096812340484403\n",
      "iter:3 of epoch:0: loss:0.29667538101941615\n",
      "iter:3 of epoch:1: loss:0.24125230417305338\n",
      "iter:3 of epoch:2: loss:0.31541401716372136\n",
      "iter:3 of epoch:3: loss:0.44158442489907357\n",
      "iter:3 of epoch:4: loss:0.7061703820297346\n",
      "iter:3 of epoch:5: loss:0.4681293972663273\n",
      "iter:3 of epoch:6: loss:1.138955078691259\n",
      "iter:3 of epoch:7: loss:0.2713205417905387\n",
      "iter:3 of epoch:8: loss:0.6749800568516934\n",
      "iter:3 of epoch:9: loss:0.37851269282763833\n",
      "iter:3 of epoch:10: loss:0.768112067292802\n",
      "iter:3 of epoch:11: loss:0.9066011984248823\n",
      "iter:3 of epoch:12: loss:0.7478318187547558\n",
      "iter:3 of epoch:13: loss:0.765537631244323\n",
      "iter:3 of epoch:14: loss:0.5857589466570732\n",
      "iter:3 of epoch:15: loss:0.5015481063642565\n",
      "iter:3 of epoch:16: loss:0.2663887405408971\n",
      "iter:3 of epoch:17: loss:0.42863912477550503\n",
      "iter:3 of epoch:18: loss:0.24000829574733093\n",
      "iter:3 of epoch:19: loss:0.3690512775746653\n",
      "iter:3 of epoch:20: loss:1.0392281012435443\n",
      "iter:3 of epoch:21: loss:0.6836422415537792\n",
      "iter:3 of epoch:22: loss:0.7155824947950581\n",
      "iter:3 of epoch:23: loss:0.16079457213426723\n",
      "iter:3 of epoch:24: loss:0.45609015898314303\n",
      "iter:3 of epoch:25: loss:0.31413212848463185\n",
      "iter:3 of epoch:26: loss:0.31216476115091313\n",
      "iter:3 of epoch:27: loss:0.5132752929706836\n",
      "iter:3 of epoch:28: loss:0.349112524162695\n",
      "iter:3 of epoch:29: loss:0.4014201790427016\n",
      "iter:3 of epoch:30: loss:0.22470383505256697\n",
      "iter:3 of epoch:31: loss:0.15146984658608023\n",
      "iter:3 of epoch:32: loss:0.4864140250004237\n",
      "iter:3 of epoch:33: loss:1.0775651741974956\n",
      "iter:3 of epoch:34: loss:0.5892420170839077\n",
      "iter:3 of epoch:35: loss:0.30217138141790556\n",
      "iter:3 of epoch:36: loss:0.18209549177683887\n",
      "iter:3 of epoch:37: loss:0.9505118510013929\n",
      "iter:3 of epoch:38: loss:2.028993418005471\n",
      "iter:3 of epoch:39: loss:0.09845604513601178\n",
      "iter:3 of epoch:40: loss:0.7511184051653427\n",
      "iter:4 of epoch:0: loss:0.40049063016278863\n",
      "iter:4 of epoch:1: loss:1.7390472757442814\n",
      "iter:4 of epoch:2: loss:0.21196520280631753\n",
      "iter:4 of epoch:3: loss:0.35986581384720023\n",
      "iter:4 of epoch:4: loss:0.29123201432775414\n",
      "iter:4 of epoch:5: loss:0.9396780159043763\n",
      "iter:4 of epoch:6: loss:0.8081237971128067\n",
      "iter:4 of epoch:7: loss:0.44188771079741135\n",
      "iter:4 of epoch:8: loss:0.2816341200371409\n",
      "iter:4 of epoch:9: loss:0.6952330886178817\n",
      "iter:4 of epoch:10: loss:1.3552152634122077\n",
      "iter:4 of epoch:11: loss:0.7172573311868877\n",
      "iter:4 of epoch:12: loss:0.16643282131879084\n",
      "iter:4 of epoch:13: loss:0.20741937847084482\n",
      "iter:4 of epoch:14: loss:0.5451100761566385\n",
      "iter:4 of epoch:15: loss:0.21625994048913427\n",
      "iter:4 of epoch:16: loss:0.5683504604606793\n",
      "iter:4 of epoch:17: loss:0.37008070004422355\n",
      "iter:4 of epoch:18: loss:0.7336793776452359\n",
      "iter:4 of epoch:19: loss:0.23351626325423328\n",
      "iter:4 of epoch:20: loss:0.2768634061223528\n",
      "iter:4 of epoch:21: loss:0.42813040455497553\n",
      "iter:4 of epoch:22: loss:0.2522195985357728\n",
      "iter:4 of epoch:23: loss:0.12177327667439411\n",
      "iter:4 of epoch:24: loss:0.4426942917218975\n",
      "iter:4 of epoch:25: loss:0.568490439198342\n",
      "iter:4 of epoch:26: loss:0.16220103570143965\n",
      "iter:4 of epoch:27: loss:0.33666747283882553\n",
      "iter:4 of epoch:28: loss:0.8241520023122364\n",
      "iter:4 of epoch:29: loss:0.8460467816087511\n",
      "iter:4 of epoch:30: loss:0.1901915149393278\n",
      "iter:4 of epoch:31: loss:0.7934263158763486\n",
      "iter:4 of epoch:32: loss:0.5463152646929573\n",
      "iter:4 of epoch:33: loss:0.5349889563970558\n",
      "iter:4 of epoch:34: loss:0.18608838286166313\n",
      "iter:4 of epoch:35: loss:0.4368225808588531\n",
      "iter:4 of epoch:36: loss:0.47686724566885613\n",
      "iter:4 of epoch:37: loss:0.22576484546017558\n",
      "iter:4 of epoch:38: loss:0.034215408890799964\n",
      "iter:4 of epoch:39: loss:0.27871310101173463\n",
      "iter:4 of epoch:40: loss:0.3196942461800902\n",
      "iter:5 of epoch:0: loss:1.1656681704437784\n",
      "iter:5 of epoch:1: loss:0.11785082815754673\n",
      "iter:5 of epoch:2: loss:0.3278592892628531\n",
      "iter:5 of epoch:3: loss:0.5623421392119392\n",
      "iter:5 of epoch:4: loss:0.2512406053796524\n",
      "iter:5 of epoch:5: loss:0.28961339756387233\n",
      "iter:5 of epoch:6: loss:0.4314520443657779\n",
      "iter:5 of epoch:7: loss:1.5601830197641435\n",
      "iter:5 of epoch:8: loss:0.5001819749851397\n",
      "iter:5 of epoch:9: loss:0.19906601134883567\n",
      "iter:5 of epoch:10: loss:0.31277272590335\n",
      "iter:5 of epoch:11: loss:0.3908607491021039\n",
      "iter:5 of epoch:12: loss:0.23795529902703905\n",
      "iter:5 of epoch:13: loss:0.34629856882434507\n",
      "iter:5 of epoch:14: loss:0.4325705058747804\n",
      "iter:5 of epoch:15: loss:0.2395486167722949\n",
      "iter:5 of epoch:16: loss:0.3578978927906718\n",
      "iter:5 of epoch:17: loss:0.25756161434915686\n",
      "iter:5 of epoch:18: loss:0.16858930928802515\n",
      "iter:5 of epoch:19: loss:0.4321801311196543\n",
      "iter:5 of epoch:20: loss:0.760258969495695\n",
      "iter:5 of epoch:21: loss:0.146835649242339\n",
      "iter:5 of epoch:22: loss:0.6750563499641712\n",
      "iter:5 of epoch:23: loss:0.21483001220430556\n",
      "iter:5 of epoch:24: loss:0.3011736955515811\n",
      "iter:5 of epoch:25: loss:0.30717905488048647\n",
      "iter:5 of epoch:26: loss:0.7133033737893645\n",
      "iter:5 of epoch:27: loss:0.4171596760326534\n",
      "iter:5 of epoch:28: loss:0.6589668996694056\n",
      "iter:5 of epoch:29: loss:0.3532754578908869\n",
      "iter:5 of epoch:30: loss:0.28985934492028714\n",
      "iter:5 of epoch:31: loss:0.3645457859102404\n",
      "iter:5 of epoch:32: loss:0.44041922951617973\n",
      "iter:5 of epoch:33: loss:0.9288229743377883\n",
      "iter:5 of epoch:34: loss:0.6656962396896848\n",
      "iter:5 of epoch:35: loss:0.3022052891995603\n",
      "iter:5 of epoch:36: loss:0.5090122043976786\n",
      "iter:5 of epoch:37: loss:0.118096434352708\n",
      "iter:5 of epoch:38: loss:0.2590159081116085\n",
      "iter:5 of epoch:39: loss:0.3126521740607961\n",
      "iter:5 of epoch:40: loss:0.049570989081412\n",
      "iter:6 of epoch:0: loss:0.1117848395848318\n",
      "iter:6 of epoch:1: loss:0.3826169349963845\n",
      "iter:6 of epoch:2: loss:0.3078569238747111\n",
      "iter:6 of epoch:3: loss:0.0894041694416619\n",
      "iter:6 of epoch:4: loss:0.14532032137877343\n",
      "iter:6 of epoch:5: loss:0.5374488081698564\n",
      "iter:6 of epoch:6: loss:0.2430416063217798\n",
      "iter:6 of epoch:7: loss:0.2145213793672002\n",
      "iter:6 of epoch:8: loss:0.3742050150753894\n",
      "iter:6 of epoch:9: loss:0.7523315147856338\n",
      "iter:6 of epoch:10: loss:0.9168953656918658\n",
      "iter:6 of epoch:11: loss:0.6687055065996448\n",
      "iter:6 of epoch:12: loss:0.39693313069984515\n",
      "iter:6 of epoch:13: loss:0.12695117543789938\n",
      "iter:6 of epoch:14: loss:0.44299558600255934\n",
      "iter:6 of epoch:15: loss:0.2374132710684677\n",
      "iter:6 of epoch:16: loss:0.30628111493888177\n",
      "iter:6 of epoch:17: loss:0.181274756404395\n",
      "iter:6 of epoch:18: loss:0.3723629712110274\n",
      "iter:6 of epoch:19: loss:0.26352662561109064\n",
      "iter:6 of epoch:20: loss:0.7652398938569785\n",
      "iter:6 of epoch:21: loss:0.4974928986707597\n",
      "iter:6 of epoch:22: loss:0.5414724113413997\n",
      "iter:6 of epoch:23: loss:0.24114188194274971\n",
      "iter:6 of epoch:24: loss:0.41283668287932523\n",
      "iter:6 of epoch:25: loss:0.49057776815562965\n",
      "iter:6 of epoch:26: loss:0.9351271524769645\n",
      "iter:6 of epoch:27: loss:0.29828612701129675\n",
      "iter:6 of epoch:28: loss:0.19138575849650943\n",
      "iter:6 of epoch:29: loss:0.2972790297282227\n",
      "iter:6 of epoch:30: loss:0.21828453175327417\n",
      "iter:6 of epoch:31: loss:0.18514411236366937\n",
      "iter:6 of epoch:32: loss:0.24173127751021303\n",
      "iter:6 of epoch:33: loss:0.13806229622847496\n",
      "iter:6 of epoch:34: loss:0.22944483819098446\n",
      "iter:6 of epoch:35: loss:0.38925048504890597\n",
      "iter:6 of epoch:36: loss:0.4965414879091979\n",
      "iter:6 of epoch:37: loss:0.12835498877191504\n",
      "iter:6 of epoch:38: loss:0.7604099413324901\n",
      "iter:6 of epoch:39: loss:1.1215196282343696\n",
      "iter:6 of epoch:40: loss:0.06202413081739064\n",
      "iter:7 of epoch:0: loss:0.2329123006975943\n",
      "iter:7 of epoch:1: loss:0.473127721732404\n",
      "iter:7 of epoch:2: loss:0.3264324733895675\n",
      "iter:7 of epoch:3: loss:0.12375171707643218\n",
      "iter:7 of epoch:4: loss:0.15843360562118075\n",
      "iter:7 of epoch:5: loss:0.14454002620722037\n",
      "iter:7 of epoch:6: loss:0.3374839844479321\n",
      "iter:7 of epoch:7: loss:0.6024541433905152\n",
      "iter:7 of epoch:8: loss:0.35583394920195666\n",
      "iter:7 of epoch:9: loss:0.20714924698016696\n",
      "iter:7 of epoch:10: loss:0.6726261172988692\n",
      "iter:7 of epoch:11: loss:0.4309072731492246\n",
      "iter:7 of epoch:12: loss:0.13360358817955134\n",
      "iter:7 of epoch:13: loss:0.1493840263528564\n",
      "iter:7 of epoch:14: loss:0.2926828690360344\n",
      "iter:7 of epoch:15: loss:0.22498523639491075\n",
      "iter:7 of epoch:16: loss:0.2224636224170012\n",
      "iter:7 of epoch:17: loss:0.32779413349722\n",
      "iter:7 of epoch:18: loss:0.43880929096832527\n",
      "iter:7 of epoch:19: loss:0.8030170495872943\n",
      "iter:7 of epoch:20: loss:0.44330332373763903\n",
      "iter:7 of epoch:21: loss:0.1533717793109367\n",
      "iter:7 of epoch:22: loss:0.3583393316081757\n",
      "iter:7 of epoch:23: loss:0.528951348816945\n",
      "iter:7 of epoch:24: loss:0.38148209129939215\n",
      "iter:7 of epoch:25: loss:0.5996555927098479\n",
      "iter:7 of epoch:26: loss:1.0612837019607517\n",
      "iter:7 of epoch:27: loss:0.4342132455476152\n",
      "iter:7 of epoch:28: loss:0.26013062322180014\n",
      "iter:7 of epoch:29: loss:0.1593923128757559\n",
      "iter:7 of epoch:30: loss:0.41527691207404915\n",
      "iter:7 of epoch:31: loss:0.12588296150524045\n",
      "iter:7 of epoch:32: loss:0.3858172120157487\n",
      "iter:7 of epoch:33: loss:0.4292693747949382\n",
      "iter:7 of epoch:34: loss:0.132925151270824\n",
      "iter:7 of epoch:35: loss:0.12273543151017616\n",
      "iter:7 of epoch:36: loss:0.30188820174815556\n",
      "iter:7 of epoch:37: loss:0.4535055294177554\n",
      "iter:7 of epoch:38: loss:0.5166356116584853\n",
      "iter:7 of epoch:39: loss:0.08805450139383539\n",
      "iter:7 of epoch:40: loss:0.590452991398673\n",
      "iter:8 of epoch:0: loss:0.16169703419763776\n",
      "iter:8 of epoch:1: loss:0.053852940208580835\n",
      "iter:8 of epoch:2: loss:0.5532236681064404\n",
      "iter:8 of epoch:3: loss:0.2896601205056942\n",
      "iter:8 of epoch:4: loss:0.256770671257569\n",
      "iter:8 of epoch:5: loss:0.16175815419310627\n",
      "iter:8 of epoch:6: loss:0.611542581227243\n",
      "iter:8 of epoch:7: loss:0.2575096055426894\n",
      "iter:8 of epoch:8: loss:0.2875814919984726\n",
      "iter:8 of epoch:9: loss:0.17010434112728184\n",
      "iter:8 of epoch:10: loss:0.3892941856957267\n",
      "iter:8 of epoch:11: loss:0.11452243392725994\n",
      "iter:8 of epoch:12: loss:0.7803751318753138\n",
      "iter:8 of epoch:13: loss:0.12404352253200016\n",
      "iter:8 of epoch:14: loss:0.19191433053748266\n",
      "iter:8 of epoch:15: loss:0.19122461901256235\n",
      "iter:8 of epoch:16: loss:0.39108415031381594\n",
      "iter:8 of epoch:17: loss:0.13474984604142679\n",
      "iter:8 of epoch:18: loss:0.29546899084401634\n",
      "iter:8 of epoch:19: loss:0.10829879775625526\n",
      "iter:8 of epoch:20: loss:0.31812684319369827\n",
      "iter:8 of epoch:21: loss:0.6360774850775709\n",
      "iter:8 of epoch:22: loss:0.3801343338710428\n",
      "iter:8 of epoch:23: loss:0.39240811852492524\n",
      "iter:8 of epoch:24: loss:0.14285668075615582\n",
      "iter:8 of epoch:25: loss:0.16495572388230664\n",
      "iter:8 of epoch:26: loss:0.3611515729905383\n",
      "iter:8 of epoch:27: loss:0.4727541011771069\n",
      "iter:8 of epoch:28: loss:0.1317640171264793\n",
      "iter:8 of epoch:29: loss:1.087874577061276\n",
      "iter:8 of epoch:30: loss:0.1636882326280923\n",
      "iter:8 of epoch:31: loss:0.1968560642929353\n",
      "iter:8 of epoch:32: loss:0.16416105744693263\n",
      "iter:8 of epoch:33: loss:0.5792752078480659\n",
      "iter:8 of epoch:34: loss:0.5953648356282126\n",
      "iter:8 of epoch:35: loss:0.5163180806704699\n",
      "iter:8 of epoch:36: loss:0.24106722649486292\n",
      "iter:8 of epoch:37: loss:0.43304880275315527\n",
      "iter:8 of epoch:38: loss:0.159799568672281\n",
      "iter:8 of epoch:39: loss:0.29077442042407325\n",
      "iter:8 of epoch:40: loss:0.25169226257658206\n",
      "iter:9 of epoch:0: loss:0.5533634785849357\n",
      "iter:9 of epoch:1: loss:0.37420479654710537\n",
      "iter:9 of epoch:2: loss:0.43294672617998786\n",
      "iter:9 of epoch:3: loss:0.11282699603204771\n",
      "iter:9 of epoch:4: loss:0.7957776974056074\n",
      "iter:9 of epoch:5: loss:0.2579511526734959\n",
      "iter:9 of epoch:6: loss:0.38480308781796824\n",
      "iter:9 of epoch:7: loss:0.12939574184901012\n",
      "iter:9 of epoch:8: loss:0.28002551151966704\n",
      "iter:9 of epoch:9: loss:0.5188596075621221\n",
      "iter:9 of epoch:10: loss:0.4313337116819458\n",
      "iter:9 of epoch:11: loss:0.3808395743844871\n",
      "iter:9 of epoch:12: loss:0.5586019544368102\n",
      "iter:9 of epoch:13: loss:0.04813292988804794\n",
      "iter:9 of epoch:14: loss:0.21518037276001176\n",
      "iter:9 of epoch:15: loss:0.5515957940861534\n",
      "iter:9 of epoch:16: loss:0.2348487419608547\n",
      "iter:9 of epoch:17: loss:0.16497925010892442\n",
      "iter:9 of epoch:18: loss:0.2268552564572411\n",
      "iter:9 of epoch:19: loss:0.11647368831667167\n",
      "iter:9 of epoch:20: loss:0.19164455679562759\n",
      "iter:9 of epoch:21: loss:0.9387153705469545\n",
      "iter:9 of epoch:22: loss:0.20946804922821155\n",
      "iter:9 of epoch:23: loss:0.33877215121659876\n",
      "iter:9 of epoch:24: loss:0.4539437488306066\n",
      "iter:9 of epoch:25: loss:0.3281690606789504\n",
      "iter:9 of epoch:26: loss:0.3731869020735826\n",
      "iter:9 of epoch:27: loss:0.1399201843478184\n",
      "iter:9 of epoch:28: loss:0.14768044404388506\n",
      "iter:9 of epoch:29: loss:0.254754480560596\n",
      "iter:9 of epoch:30: loss:0.20908281873071394\n",
      "iter:9 of epoch:31: loss:0.21273939116551097\n",
      "iter:9 of epoch:32: loss:0.14289217737848628\n",
      "iter:9 of epoch:33: loss:0.09222447394810294\n",
      "iter:9 of epoch:34: loss:0.16446548462189506\n",
      "iter:9 of epoch:35: loss:0.12916306619441137\n",
      "iter:9 of epoch:36: loss:0.2942107430658977\n",
      "iter:9 of epoch:37: loss:0.14871806783563954\n",
      "iter:9 of epoch:38: loss:0.313377909417823\n",
      "iter:9 of epoch:39: loss:0.11716807394526459\n",
      "iter:9 of epoch:40: loss:0.1390323787942055\n",
      "iter:10 of epoch:0: loss:0.4028861116806066\n",
      "iter:10 of epoch:1: loss:0.19634709522779192\n",
      "iter:10 of epoch:2: loss:0.32945718535509616\n",
      "iter:10 of epoch:3: loss:0.36732229843305253\n",
      "iter:10 of epoch:4: loss:0.42243516530180336\n",
      "iter:10 of epoch:5: loss:0.17187427359164026\n",
      "iter:10 of epoch:6: loss:0.044954322361861976\n",
      "iter:10 of epoch:7: loss:0.2535081815501511\n",
      "iter:10 of epoch:8: loss:0.36833038819075853\n",
      "iter:10 of epoch:9: loss:0.08996810634483995\n",
      "iter:10 of epoch:10: loss:0.47553368718060546\n",
      "iter:10 of epoch:11: loss:0.140567006589899\n",
      "iter:10 of epoch:12: loss:0.3963176444184646\n",
      "iter:10 of epoch:13: loss:0.09806953544487436\n",
      "iter:10 of epoch:14: loss:0.5503952511591076\n",
      "iter:10 of epoch:15: loss:0.2720651500400558\n",
      "iter:10 of epoch:16: loss:0.22505467404251583\n",
      "iter:10 of epoch:17: loss:0.0980545989453214\n",
      "iter:10 of epoch:18: loss:0.2550735728925222\n",
      "iter:10 of epoch:19: loss:0.08022401815943524\n",
      "iter:10 of epoch:20: loss:0.09801638991263516\n",
      "iter:10 of epoch:21: loss:0.10926693651479161\n",
      "iter:10 of epoch:22: loss:0.1861793254865004\n",
      "iter:10 of epoch:23: loss:0.2588095325021845\n",
      "iter:10 of epoch:24: loss:0.19517219581314232\n",
      "iter:10 of epoch:25: loss:0.5644481272082573\n",
      "iter:10 of epoch:26: loss:0.20070189679998673\n",
      "iter:10 of epoch:27: loss:0.3247357443474511\n",
      "iter:10 of epoch:28: loss:0.48156334064260803\n",
      "iter:10 of epoch:29: loss:0.32650767363676797\n",
      "iter:10 of epoch:30: loss:0.10562138664617651\n",
      "iter:10 of epoch:31: loss:0.42881789222538724\n",
      "iter:10 of epoch:32: loss:0.24826886697478406\n",
      "iter:10 of epoch:33: loss:0.06877477927468836\n",
      "iter:10 of epoch:34: loss:0.3729736389288191\n",
      "iter:10 of epoch:35: loss:0.8150619512731188\n",
      "iter:10 of epoch:36: loss:0.15201883580535633\n",
      "iter:10 of epoch:37: loss:0.09926658258623808\n",
      "iter:10 of epoch:38: loss:0.30667229350705166\n",
      "iter:10 of epoch:39: loss:0.6065182382206984\n",
      "iter:10 of epoch:40: loss:0.019458442390402904\n",
      "iter:11 of epoch:0: loss:0.1139451223836728\n",
      "iter:11 of epoch:1: loss:0.3177407778042068\n",
      "iter:11 of epoch:2: loss:0.5492461138764281\n",
      "iter:11 of epoch:3: loss:0.17082223173884564\n",
      "iter:11 of epoch:4: loss:0.2425027243040065\n",
      "iter:11 of epoch:5: loss:0.04225782397016102\n",
      "iter:11 of epoch:6: loss:0.10133369380398441\n",
      "iter:11 of epoch:7: loss:0.2511050514869622\n",
      "iter:11 of epoch:8: loss:0.09441966591254024\n",
      "iter:11 of epoch:9: loss:0.25665035843627093\n",
      "iter:11 of epoch:10: loss:0.1556970255995626\n",
      "iter:11 of epoch:11: loss:0.13077993146467387\n",
      "iter:11 of epoch:12: loss:0.17063226282034683\n",
      "iter:11 of epoch:13: loss:0.5341526682986786\n",
      "iter:11 of epoch:14: loss:0.100765937348066\n",
      "iter:11 of epoch:15: loss:0.1153001826794979\n",
      "iter:11 of epoch:16: loss:0.46169773896934263\n",
      "iter:11 of epoch:17: loss:0.19594709098447005\n",
      "iter:11 of epoch:18: loss:0.5518044559451301\n",
      "iter:11 of epoch:19: loss:0.30105144257419625\n",
      "iter:11 of epoch:20: loss:0.0970024090261036\n",
      "iter:11 of epoch:21: loss:0.09660628806978437\n",
      "iter:11 of epoch:22: loss:0.1962028614890069\n",
      "iter:11 of epoch:23: loss:1.0248483423485146\n",
      "iter:11 of epoch:24: loss:0.4002730384977486\n",
      "iter:11 of epoch:25: loss:0.2196890790640932\n",
      "iter:11 of epoch:26: loss:0.2967687543441931\n",
      "iter:11 of epoch:27: loss:0.1499534191449484\n",
      "iter:11 of epoch:28: loss:0.09690716165049108\n",
      "iter:11 of epoch:29: loss:0.08981941198397202\n",
      "iter:11 of epoch:30: loss:0.3297520173656122\n",
      "iter:11 of epoch:31: loss:0.10175615680920805\n",
      "iter:11 of epoch:32: loss:0.22665405415614268\n",
      "iter:11 of epoch:33: loss:0.3819885116694405\n",
      "iter:11 of epoch:34: loss:0.1551871463767561\n",
      "iter:11 of epoch:35: loss:0.15272606337990102\n",
      "iter:11 of epoch:36: loss:0.49308262859866236\n",
      "iter:11 of epoch:37: loss:0.24862152330246529\n",
      "iter:11 of epoch:38: loss:0.31855721578216506\n",
      "iter:11 of epoch:39: loss:0.37936015200599527\n",
      "iter:11 of epoch:40: loss:0.3566095977063761\n",
      "iter:12 of epoch:0: loss:0.13315803396601994\n",
      "iter:12 of epoch:1: loss:0.07959651794654857\n",
      "iter:12 of epoch:2: loss:0.08048056855007824\n",
      "iter:12 of epoch:3: loss:0.12708914305378996\n",
      "iter:12 of epoch:4: loss:0.3335430469535417\n",
      "iter:12 of epoch:5: loss:0.24900169580016898\n",
      "iter:12 of epoch:6: loss:0.2893549452011818\n",
      "iter:12 of epoch:7: loss:0.41909885576167144\n",
      "iter:12 of epoch:8: loss:0.28664966132476727\n",
      "iter:12 of epoch:9: loss:0.2579886838764879\n",
      "iter:12 of epoch:10: loss:0.09127935961519233\n",
      "iter:12 of epoch:11: loss:0.2344936172194138\n",
      "iter:12 of epoch:12: loss:0.06015022251705392\n",
      "iter:12 of epoch:13: loss:0.30041596657855485\n",
      "iter:12 of epoch:14: loss:0.17791762327726432\n",
      "iter:12 of epoch:15: loss:0.0999085745768021\n",
      "iter:12 of epoch:16: loss:0.1279478863827263\n",
      "iter:12 of epoch:17: loss:0.18211353775860145\n",
      "iter:12 of epoch:18: loss:0.26625087301068795\n",
      "iter:12 of epoch:19: loss:0.3430894178331195\n",
      "iter:12 of epoch:20: loss:0.2566700829710776\n",
      "iter:12 of epoch:21: loss:0.16707153505363984\n",
      "iter:12 of epoch:22: loss:0.32708850933910527\n",
      "iter:12 of epoch:23: loss:0.17416031259583029\n",
      "iter:12 of epoch:24: loss:0.1742761030251861\n",
      "iter:12 of epoch:25: loss:0.07932552428456721\n",
      "iter:12 of epoch:26: loss:0.3819878914077579\n",
      "iter:12 of epoch:27: loss:0.2508566913580898\n",
      "iter:12 of epoch:28: loss:0.4794335934567419\n",
      "iter:12 of epoch:29: loss:0.13001678806229996\n",
      "iter:12 of epoch:30: loss:0.28387511985495895\n",
      "iter:12 of epoch:31: loss:0.5870725999292192\n",
      "iter:12 of epoch:32: loss:0.7850267137324628\n",
      "iter:12 of epoch:33: loss:0.29685590306908277\n",
      "iter:12 of epoch:34: loss:0.3869644299236841\n",
      "iter:12 of epoch:35: loss:0.12475633260864805\n",
      "iter:12 of epoch:36: loss:0.260235242925187\n",
      "iter:12 of epoch:37: loss:0.06532685750844866\n",
      "iter:12 of epoch:38: loss:0.2174007247296986\n",
      "iter:12 of epoch:39: loss:0.1331359256549465\n",
      "iter:12 of epoch:40: loss:0.25666445692623013\n",
      "iter:13 of epoch:0: loss:0.13956026280296574\n",
      "iter:13 of epoch:1: loss:0.07299496548874473\n",
      "iter:13 of epoch:2: loss:0.27500066413191004\n",
      "iter:13 of epoch:3: loss:0.3035187584231722\n",
      "iter:13 of epoch:4: loss:0.21052150128902997\n",
      "iter:13 of epoch:5: loss:0.4451982748775799\n",
      "iter:13 of epoch:6: loss:0.09321798452769431\n",
      "iter:13 of epoch:7: loss:0.1557151643922021\n",
      "iter:13 of epoch:8: loss:0.1614653227967583\n",
      "iter:13 of epoch:9: loss:0.06663238908521911\n",
      "iter:13 of epoch:10: loss:0.36423630361160564\n",
      "iter:13 of epoch:11: loss:0.6104397473333415\n",
      "iter:13 of epoch:12: loss:0.3073957749715286\n",
      "iter:13 of epoch:13: loss:0.09293450938085603\n",
      "iter:13 of epoch:14: loss:0.3136925340083201\n",
      "iter:13 of epoch:15: loss:0.3196661053932845\n",
      "iter:13 of epoch:16: loss:0.2834956267660073\n",
      "iter:13 of epoch:17: loss:0.0931154546398057\n",
      "iter:13 of epoch:18: loss:0.1187135066748097\n",
      "iter:13 of epoch:19: loss:0.06583085567262638\n",
      "iter:13 of epoch:20: loss:0.2031626300908333\n",
      "iter:13 of epoch:21: loss:0.05830397114873022\n",
      "iter:13 of epoch:22: loss:0.24343264051730013\n",
      "iter:13 of epoch:23: loss:0.12736890771167608\n",
      "iter:13 of epoch:24: loss:0.7271626366242417\n",
      "iter:13 of epoch:25: loss:0.1256915664799057\n",
      "iter:13 of epoch:26: loss:0.5030662997027531\n",
      "iter:13 of epoch:27: loss:0.38623636710938436\n",
      "iter:13 of epoch:28: loss:0.128315495910806\n",
      "iter:13 of epoch:29: loss:0.10126405594933403\n",
      "iter:13 of epoch:30: loss:0.23198797448166536\n",
      "iter:13 of epoch:31: loss:0.2517402043218572\n",
      "iter:13 of epoch:32: loss:0.07854415939099066\n",
      "iter:13 of epoch:33: loss:0.16580273901205142\n",
      "iter:13 of epoch:34: loss:0.23367696578222436\n",
      "iter:13 of epoch:35: loss:0.38384717945296265\n",
      "iter:13 of epoch:36: loss:0.2482531753989195\n",
      "iter:13 of epoch:37: loss:0.03458708010838597\n",
      "iter:13 of epoch:38: loss:0.05646550826939524\n",
      "iter:13 of epoch:39: loss:0.2820593422812391\n",
      "iter:13 of epoch:40: loss:0.4917858023465491\n",
      "iter:14 of epoch:0: loss:0.09854409853843288\n",
      "iter:14 of epoch:1: loss:0.2825126199208706\n",
      "iter:14 of epoch:2: loss:0.5420812948668059\n",
      "iter:14 of epoch:3: loss:0.17740201040457396\n",
      "iter:14 of epoch:4: loss:0.08973525651708367\n",
      "iter:14 of epoch:5: loss:0.13277786504130284\n",
      "iter:14 of epoch:6: loss:0.16176400107348674\n",
      "iter:14 of epoch:7: loss:0.12434465724229553\n",
      "iter:14 of epoch:8: loss:0.07513992124269363\n",
      "iter:14 of epoch:9: loss:0.08656451216058092\n",
      "iter:14 of epoch:10: loss:0.20909156615068342\n",
      "iter:14 of epoch:11: loss:0.4562261335885626\n",
      "iter:14 of epoch:12: loss:0.11963573205643234\n",
      "iter:14 of epoch:13: loss:0.2359144951496881\n",
      "iter:14 of epoch:14: loss:0.3510061578764118\n",
      "iter:14 of epoch:15: loss:0.5342519746936221\n",
      "iter:14 of epoch:16: loss:0.280360661859644\n",
      "iter:14 of epoch:17: loss:0.31522564587357055\n",
      "iter:14 of epoch:18: loss:0.05940449598899718\n",
      "iter:14 of epoch:19: loss:0.2351674091887094\n",
      "iter:14 of epoch:20: loss:0.15825617249918505\n",
      "iter:14 of epoch:21: loss:0.1622011877465271\n",
      "iter:14 of epoch:22: loss:0.15810806489808543\n",
      "iter:14 of epoch:23: loss:0.17876290669618197\n",
      "iter:14 of epoch:24: loss:0.18121142383960653\n",
      "iter:14 of epoch:25: loss:0.28793675474746916\n",
      "iter:14 of epoch:26: loss:0.05167192788238277\n",
      "iter:14 of epoch:27: loss:0.1662489486522099\n",
      "iter:14 of epoch:28: loss:0.28691552936934117\n",
      "iter:14 of epoch:29: loss:0.11155948309991584\n",
      "iter:14 of epoch:30: loss:0.11875254695238217\n",
      "iter:14 of epoch:31: loss:0.23092449593564143\n",
      "iter:14 of epoch:32: loss:0.15211920135219884\n",
      "iter:14 of epoch:33: loss:0.053518120258550614\n",
      "iter:14 of epoch:34: loss:0.8177029571375641\n",
      "iter:14 of epoch:35: loss:0.2126962732925281\n",
      "iter:14 of epoch:36: loss:0.07354570976136451\n",
      "iter:14 of epoch:37: loss:0.20794178899003532\n",
      "iter:14 of epoch:38: loss:0.08825854710222328\n",
      "iter:14 of epoch:39: loss:0.220145859185621\n",
      "iter:14 of epoch:40: loss:0.6882483169184348\n",
      "iter:15 of epoch:0: loss:0.19146445548295024\n",
      "iter:15 of epoch:1: loss:0.06804883394815862\n",
      "iter:15 of epoch:2: loss:0.3934295783259729\n",
      "iter:15 of epoch:3: loss:0.17111319467272257\n",
      "iter:15 of epoch:4: loss:0.057061006231500934\n",
      "iter:15 of epoch:5: loss:0.0802401037135659\n",
      "iter:15 of epoch:6: loss:0.15768964107626654\n",
      "iter:15 of epoch:7: loss:0.11600366346598497\n",
      "iter:15 of epoch:8: loss:0.1027576132625347\n",
      "iter:15 of epoch:9: loss:0.31351084604890683\n",
      "iter:15 of epoch:10: loss:0.0789141654679032\n",
      "iter:15 of epoch:11: loss:0.2110466319570053\n",
      "iter:15 of epoch:12: loss:0.20874072569303248\n",
      "iter:15 of epoch:13: loss:0.19292143208916446\n",
      "iter:15 of epoch:14: loss:0.36087776421788076\n",
      "iter:15 of epoch:15: loss:0.2995581387627126\n",
      "iter:15 of epoch:16: loss:0.4012997524493855\n",
      "iter:15 of epoch:17: loss:0.08090869948108766\n",
      "iter:15 of epoch:18: loss:0.3696955793814071\n",
      "iter:15 of epoch:19: loss:0.24065677575486322\n",
      "iter:15 of epoch:20: loss:0.05075131534870736\n",
      "iter:15 of epoch:21: loss:0.08849039941023559\n",
      "iter:15 of epoch:22: loss:0.10507210276238266\n",
      "iter:15 of epoch:23: loss:0.2203188293114675\n",
      "iter:15 of epoch:24: loss:0.2068061420551291\n",
      "iter:15 of epoch:25: loss:0.17046117634634794\n",
      "iter:15 of epoch:26: loss:0.1313880707965216\n",
      "iter:15 of epoch:27: loss:0.0796188124120531\n",
      "iter:15 of epoch:28: loss:0.5773862879269759\n",
      "iter:15 of epoch:29: loss:0.17246657438076093\n",
      "iter:15 of epoch:30: loss:0.08971243352276262\n",
      "iter:15 of epoch:31: loss:0.22960983514467057\n",
      "iter:15 of epoch:32: loss:0.08973295029752024\n",
      "iter:15 of epoch:33: loss:0.7173841770774437\n",
      "iter:15 of epoch:34: loss:0.23173862394567202\n",
      "iter:15 of epoch:35: loss:0.08169269390041467\n",
      "iter:15 of epoch:36: loss:0.2531318165310253\n",
      "iter:15 of epoch:37: loss:0.10183326466347686\n",
      "iter:15 of epoch:38: loss:0.3489230361703764\n",
      "iter:15 of epoch:39: loss:0.06436253993239084\n",
      "iter:15 of epoch:40: loss:0.4826526687494789\n",
      "iter:16 of epoch:0: loss:0.2905555415496357\n",
      "iter:16 of epoch:1: loss:0.0834553022892158\n",
      "iter:16 of epoch:2: loss:0.17931750116899284\n",
      "iter:16 of epoch:3: loss:0.06271575628226216\n",
      "iter:16 of epoch:4: loss:0.15792810288716763\n",
      "iter:16 of epoch:5: loss:0.1331215636831523\n",
      "iter:16 of epoch:6: loss:0.16371282144518218\n",
      "iter:16 of epoch:7: loss:0.13412861812988747\n",
      "iter:16 of epoch:8: loss:0.2998273858427323\n",
      "iter:16 of epoch:9: loss:0.5744389262026676\n",
      "iter:16 of epoch:10: loss:0.09954245542954528\n",
      "iter:16 of epoch:11: loss:0.19958124288986404\n",
      "iter:16 of epoch:12: loss:0.324801018097315\n",
      "iter:16 of epoch:13: loss:0.05154517482984763\n",
      "iter:16 of epoch:14: loss:0.20501016108617193\n",
      "iter:16 of epoch:15: loss:0.18615638023697897\n",
      "iter:16 of epoch:16: loss:0.1539477028229955\n",
      "iter:16 of epoch:17: loss:0.09680192829376762\n",
      "iter:16 of epoch:18: loss:0.056002890467993005\n",
      "iter:16 of epoch:19: loss:0.0660039486495857\n",
      "iter:16 of epoch:20: loss:0.2545899473626967\n",
      "iter:16 of epoch:21: loss:0.5923052809465508\n",
      "iter:16 of epoch:22: loss:0.329676719107192\n",
      "iter:16 of epoch:23: loss:0.19665995561908473\n",
      "iter:16 of epoch:24: loss:0.0547454473186184\n",
      "iter:16 of epoch:25: loss:0.08386548408274956\n",
      "iter:16 of epoch:26: loss:0.705713171780754\n",
      "iter:16 of epoch:27: loss:0.07572108213919712\n",
      "iter:16 of epoch:28: loss:0.189962859353145\n",
      "iter:16 of epoch:29: loss:0.05328171260867608\n",
      "iter:16 of epoch:30: loss:0.09006520736995902\n",
      "iter:16 of epoch:31: loss:0.21375554598841223\n",
      "iter:16 of epoch:32: loss:0.2856192165337681\n",
      "iter:16 of epoch:33: loss:0.26910906802927176\n",
      "iter:16 of epoch:34: loss:0.13457076504176643\n",
      "iter:16 of epoch:35: loss:0.1111498377209901\n",
      "iter:16 of epoch:36: loss:0.3116322555252754\n",
      "iter:16 of epoch:37: loss:0.16216558122781582\n",
      "iter:16 of epoch:38: loss:0.0722159617526642\n",
      "iter:16 of epoch:39: loss:0.0988496223588026\n",
      "iter:16 of epoch:40: loss:0.22758708870044284\n",
      "iter:17 of epoch:0: loss:0.0588030584918357\n",
      "iter:17 of epoch:1: loss:0.7064945433239671\n",
      "iter:17 of epoch:2: loss:0.2138868655509624\n",
      "iter:17 of epoch:3: loss:0.06643553152078648\n",
      "iter:17 of epoch:4: loss:0.08365984460313895\n",
      "iter:17 of epoch:5: loss:0.2566564316298608\n",
      "iter:17 of epoch:6: loss:0.4118970014640987\n",
      "iter:17 of epoch:7: loss:0.3603673906145778\n",
      "iter:17 of epoch:8: loss:0.20720028988845254\n",
      "iter:17 of epoch:9: loss:0.24997357083522592\n",
      "iter:17 of epoch:10: loss:0.341444173134422\n",
      "iter:17 of epoch:11: loss:0.19955274618747879\n",
      "iter:17 of epoch:12: loss:0.19158147725275776\n",
      "iter:17 of epoch:13: loss:0.22009413786870713\n",
      "iter:17 of epoch:14: loss:0.20020154488616826\n",
      "iter:17 of epoch:15: loss:0.2799143557574327\n",
      "iter:17 of epoch:16: loss:0.48223996001453806\n",
      "iter:17 of epoch:17: loss:0.1863216291430804\n",
      "iter:17 of epoch:18: loss:0.11264729547169541\n",
      "iter:17 of epoch:19: loss:0.13414754642073162\n",
      "iter:17 of epoch:20: loss:0.14804183117782557\n",
      "iter:17 of epoch:21: loss:0.19374701079190368\n",
      "iter:17 of epoch:22: loss:0.1941757574097965\n",
      "iter:17 of epoch:23: loss:0.046263197682293596\n",
      "iter:17 of epoch:24: loss:0.21278178758154004\n",
      "iter:17 of epoch:25: loss:0.2145633917567419\n",
      "iter:17 of epoch:26: loss:0.13099223323311865\n",
      "iter:17 of epoch:27: loss:0.08776985126049823\n",
      "iter:17 of epoch:28: loss:0.07534623023533918\n",
      "iter:17 of epoch:29: loss:0.0511880771703498\n",
      "iter:17 of epoch:30: loss:0.21731856192236557\n",
      "iter:17 of epoch:31: loss:0.09782506108500892\n",
      "iter:17 of epoch:32: loss:0.07701648608521913\n",
      "iter:17 of epoch:33: loss:0.05480735461149692\n",
      "iter:17 of epoch:34: loss:0.3407879959917593\n",
      "iter:17 of epoch:35: loss:0.16989365919932115\n",
      "iter:17 of epoch:36: loss:0.08288131802613788\n",
      "iter:17 of epoch:37: loss:0.03055258280654184\n",
      "iter:17 of epoch:38: loss:0.07032291395696857\n",
      "iter:17 of epoch:39: loss:0.07827509920605609\n",
      "iter:17 of epoch:40: loss:0.022246416092555107\n",
      "iter:18 of epoch:0: loss:0.18122623116950465\n",
      "iter:18 of epoch:1: loss:0.20109200802738858\n",
      "iter:18 of epoch:2: loss:0.04189203457637485\n",
      "iter:18 of epoch:3: loss:0.04634922152551157\n",
      "iter:18 of epoch:4: loss:0.1324138278817118\n",
      "iter:18 of epoch:5: loss:0.4105749471872631\n",
      "iter:18 of epoch:6: loss:0.2051888287468416\n",
      "iter:18 of epoch:7: loss:0.07956204034584476\n",
      "iter:18 of epoch:8: loss:0.05283462446639331\n",
      "iter:18 of epoch:9: loss:0.07031326034190255\n",
      "iter:18 of epoch:10: loss:0.24583389474434428\n",
      "iter:18 of epoch:11: loss:0.18608138027062396\n",
      "iter:18 of epoch:12: loss:0.203943308273967\n",
      "iter:18 of epoch:13: loss:0.2734340315160444\n",
      "iter:18 of epoch:14: loss:0.2021852736180898\n",
      "iter:18 of epoch:15: loss:0.11986861150915915\n",
      "iter:18 of epoch:16: loss:0.1275298460973555\n",
      "iter:18 of epoch:17: loss:0.2703240391727835\n",
      "iter:18 of epoch:18: loss:0.03407001581651738\n",
      "iter:18 of epoch:19: loss:0.3213368077984661\n",
      "iter:18 of epoch:20: loss:0.11500796026295959\n",
      "iter:18 of epoch:21: loss:0.22087320946970354\n",
      "iter:18 of epoch:22: loss:0.06044288639366067\n",
      "iter:18 of epoch:23: loss:0.20171286823142237\n",
      "iter:18 of epoch:24: loss:0.14441061016247556\n",
      "iter:18 of epoch:25: loss:0.17894002857661223\n",
      "iter:18 of epoch:26: loss:0.15584861659377328\n",
      "iter:18 of epoch:27: loss:0.1714412743158052\n",
      "iter:18 of epoch:28: loss:0.09925561312820556\n",
      "iter:18 of epoch:29: loss:0.11087382944849382\n",
      "iter:18 of epoch:30: loss:0.16265014552833534\n",
      "iter:18 of epoch:31: loss:0.11439924759552962\n",
      "iter:18 of epoch:32: loss:0.2603734192172985\n",
      "iter:18 of epoch:33: loss:0.04718083909924205\n",
      "iter:18 of epoch:34: loss:0.2873564019440769\n",
      "iter:18 of epoch:35: loss:0.2216815817623654\n",
      "iter:18 of epoch:36: loss:0.20890562778609184\n",
      "iter:18 of epoch:37: loss:0.6595264486740211\n",
      "iter:18 of epoch:38: loss:0.1430811204696983\n",
      "iter:18 of epoch:39: loss:0.20510996558896072\n",
      "iter:18 of epoch:40: loss:0.15567701759636937\n",
      "iter:19 of epoch:0: loss:0.08189546117390582\n",
      "iter:19 of epoch:1: loss:0.6364735227060143\n",
      "iter:19 of epoch:2: loss:0.17697015210742553\n",
      "iter:19 of epoch:3: loss:0.13721473124294928\n",
      "iter:19 of epoch:4: loss:0.16029658780832526\n",
      "iter:19 of epoch:5: loss:0.5991286903634472\n",
      "iter:19 of epoch:6: loss:0.19101275888122915\n",
      "iter:19 of epoch:7: loss:0.23896847089366266\n",
      "iter:19 of epoch:8: loss:0.1475405562734198\n",
      "iter:19 of epoch:9: loss:0.12785483849353838\n",
      "iter:19 of epoch:10: loss:0.4106923451668124\n",
      "iter:19 of epoch:11: loss:0.15731237060090342\n",
      "iter:19 of epoch:12: loss:0.10105286333433812\n",
      "iter:19 of epoch:13: loss:0.2663262600387585\n",
      "iter:19 of epoch:14: loss:0.1341828664008887\n",
      "iter:19 of epoch:15: loss:0.251459304764282\n",
      "iter:19 of epoch:16: loss:0.051861338974378654\n",
      "iter:19 of epoch:17: loss:0.035458538645879\n",
      "iter:19 of epoch:18: loss:0.12094434726227617\n",
      "iter:19 of epoch:19: loss:0.10358569015033174\n",
      "iter:19 of epoch:20: loss:0.05563542581866891\n",
      "iter:19 of epoch:21: loss:0.06803391111450753\n",
      "iter:19 of epoch:22: loss:0.0730454407724587\n",
      "iter:19 of epoch:23: loss:0.10234711640993414\n",
      "iter:19 of epoch:24: loss:0.24451676643028852\n",
      "iter:19 of epoch:25: loss:0.28454722566522406\n",
      "iter:19 of epoch:26: loss:0.21752920808886228\n",
      "iter:19 of epoch:27: loss:0.03512817710636305\n",
      "iter:19 of epoch:28: loss:0.10194962237749157\n",
      "iter:19 of epoch:29: loss:0.03510895576746644\n",
      "iter:19 of epoch:30: loss:0.07554286723774913\n",
      "iter:19 of epoch:31: loss:0.2861192441156512\n",
      "iter:19 of epoch:32: loss:0.1379735882983991\n",
      "iter:19 of epoch:33: loss:0.05565714124679141\n",
      "iter:19 of epoch:34: loss:0.08583239076262685\n",
      "iter:19 of epoch:35: loss:0.25819185672777234\n",
      "iter:19 of epoch:36: loss:0.29504045009140295\n",
      "iter:19 of epoch:37: loss:0.09078172483714028\n",
      "iter:19 of epoch:38: loss:0.1571897067374863\n",
      "iter:19 of epoch:39: loss:0.15720228240212805\n",
      "iter:19 of epoch:40: loss:0.018789576853361023\n",
      "iter:20 of epoch:0: loss:0.26719627411319846\n",
      "iter:20 of epoch:1: loss:0.06614496677916185\n",
      "iter:20 of epoch:2: loss:0.30256895420898244\n",
      "iter:20 of epoch:3: loss:0.10536781369904406\n",
      "iter:20 of epoch:4: loss:0.2914165878387567\n",
      "iter:20 of epoch:5: loss:0.11156100556549979\n",
      "iter:20 of epoch:6: loss:0.09805121719622359\n",
      "iter:20 of epoch:7: loss:0.22819130102589152\n",
      "iter:20 of epoch:8: loss:0.21641675007063105\n",
      "iter:20 of epoch:9: loss:0.10638389132447254\n",
      "iter:20 of epoch:10: loss:0.17812942375772733\n",
      "iter:20 of epoch:11: loss:0.06023563475338632\n",
      "iter:20 of epoch:12: loss:0.22518321303323513\n",
      "iter:20 of epoch:13: loss:0.06462451887238566\n",
      "iter:20 of epoch:14: loss:0.06482608371712403\n",
      "iter:20 of epoch:15: loss:0.045403850737235206\n",
      "iter:20 of epoch:16: loss:0.2963119211282218\n",
      "iter:20 of epoch:17: loss:0.2270937866700463\n",
      "iter:20 of epoch:18: loss:0.06929612841812548\n",
      "iter:20 of epoch:19: loss:0.20377214369547705\n",
      "iter:20 of epoch:20: loss:0.08917694661760907\n",
      "iter:20 of epoch:21: loss:0.10964213773753348\n",
      "iter:20 of epoch:22: loss:0.5562364079153018\n",
      "iter:20 of epoch:23: loss:0.030772369312714848\n",
      "iter:20 of epoch:24: loss:0.06974969811181243\n",
      "iter:20 of epoch:25: loss:0.09458414868774803\n",
      "iter:20 of epoch:26: loss:0.06066252327578312\n",
      "iter:20 of epoch:27: loss:0.35069325951828473\n",
      "iter:20 of epoch:28: loss:0.17566828600552242\n",
      "iter:20 of epoch:29: loss:0.04449773723022095\n",
      "iter:20 of epoch:30: loss:0.1469044585508975\n",
      "iter:20 of epoch:31: loss:0.1542973675666519\n",
      "iter:20 of epoch:32: loss:0.09337045406602054\n",
      "iter:20 of epoch:33: loss:0.20106939241874486\n",
      "iter:20 of epoch:34: loss:0.061983039362813484\n",
      "iter:20 of epoch:35: loss:0.06316591831323133\n",
      "iter:20 of epoch:36: loss:0.39063407879629564\n",
      "iter:20 of epoch:37: loss:0.10411220697846804\n",
      "iter:20 of epoch:38: loss:0.30738881410337926\n",
      "iter:20 of epoch:39: loss:0.23274981274555456\n",
      "iter:20 of epoch:40: loss:0.3009733639430636\n",
      "iter:21 of epoch:0: loss:0.2474818860286784\n",
      "iter:21 of epoch:1: loss:0.05179399274271623\n",
      "iter:21 of epoch:2: loss:0.09377153333819303\n",
      "iter:21 of epoch:3: loss:0.24081974983754506\n",
      "iter:21 of epoch:4: loss:0.0953795155091821\n",
      "iter:21 of epoch:5: loss:0.14031443173014163\n",
      "iter:21 of epoch:6: loss:0.2898049350085602\n",
      "iter:21 of epoch:7: loss:0.06631973948487335\n",
      "iter:21 of epoch:8: loss:0.062488911638242106\n",
      "iter:21 of epoch:9: loss:0.16143392158094594\n",
      "iter:21 of epoch:10: loss:0.12006395016313172\n",
      "iter:21 of epoch:11: loss:0.1557619964600524\n",
      "iter:21 of epoch:12: loss:0.14955372582917564\n",
      "iter:21 of epoch:13: loss:0.07104862070213895\n",
      "iter:21 of epoch:14: loss:0.11362156893452417\n",
      "iter:21 of epoch:15: loss:0.16812460440067858\n",
      "iter:21 of epoch:16: loss:0.016162104613931954\n",
      "iter:21 of epoch:17: loss:0.17037815712309706\n",
      "iter:21 of epoch:18: loss:0.12005929752306592\n",
      "iter:21 of epoch:19: loss:0.15745433800911615\n",
      "iter:21 of epoch:20: loss:0.08685811957403483\n",
      "iter:21 of epoch:21: loss:0.14991643104374014\n",
      "iter:21 of epoch:22: loss:0.258569807478504\n",
      "iter:21 of epoch:23: loss:0.040068092957806986\n",
      "iter:21 of epoch:24: loss:0.19537089935362958\n",
      "iter:21 of epoch:25: loss:0.10929222689252734\n",
      "iter:21 of epoch:26: loss:0.08367446550738722\n",
      "iter:21 of epoch:27: loss:0.16078592581798243\n",
      "iter:21 of epoch:28: loss:0.2764410481953322\n",
      "iter:21 of epoch:29: loss:0.11400353275772475\n",
      "iter:21 of epoch:30: loss:0.19306539262539157\n",
      "iter:21 of epoch:31: loss:0.055194850756856914\n",
      "iter:21 of epoch:32: loss:0.16543597707660876\n",
      "iter:21 of epoch:33: loss:0.15678503791910067\n",
      "iter:21 of epoch:34: loss:0.6902965053493674\n",
      "iter:21 of epoch:35: loss:0.14392386614398153\n",
      "iter:21 of epoch:36: loss:0.23555776250807528\n",
      "iter:21 of epoch:37: loss:0.18500863408999063\n",
      "iter:21 of epoch:38: loss:0.18862649342627477\n",
      "iter:21 of epoch:39: loss:0.24185556133641853\n",
      "iter:21 of epoch:40: loss:0.024878842471401368\n",
      "iter:22 of epoch:0: loss:0.11384277876262269\n",
      "iter:22 of epoch:1: loss:0.15069055490786532\n",
      "iter:22 of epoch:2: loss:0.0637107376439378\n",
      "iter:22 of epoch:3: loss:0.0352938888667701\n",
      "iter:22 of epoch:4: loss:0.10087446028154656\n",
      "iter:22 of epoch:5: loss:0.17562983225849324\n",
      "iter:22 of epoch:6: loss:0.13504045143732185\n",
      "iter:22 of epoch:7: loss:0.1513101262025166\n",
      "iter:22 of epoch:8: loss:0.16437470103310917\n",
      "iter:22 of epoch:9: loss:0.14625211023128157\n",
      "iter:22 of epoch:10: loss:0.5693681877618992\n",
      "iter:22 of epoch:11: loss:0.18476587699451053\n",
      "iter:22 of epoch:12: loss:0.17462704410879734\n",
      "iter:22 of epoch:13: loss:0.13428025568764806\n",
      "iter:22 of epoch:14: loss:0.042711873570832136\n",
      "iter:22 of epoch:15: loss:0.05581270608836546\n",
      "iter:22 of epoch:16: loss:0.2567672706886664\n",
      "iter:22 of epoch:17: loss:0.18055117904215043\n",
      "iter:22 of epoch:18: loss:0.32228485098171183\n",
      "iter:22 of epoch:19: loss:0.06933137171250137\n",
      "iter:22 of epoch:20: loss:0.033248386013123576\n",
      "iter:22 of epoch:21: loss:0.10281891306758531\n",
      "iter:22 of epoch:22: loss:0.15188683983111012\n",
      "iter:22 of epoch:23: loss:0.06691293925936186\n",
      "iter:22 of epoch:24: loss:0.04743965462995564\n",
      "iter:22 of epoch:25: loss:0.3133920530625689\n",
      "iter:22 of epoch:26: loss:0.07238892077491103\n",
      "iter:22 of epoch:27: loss:0.30564519007467356\n",
      "iter:22 of epoch:28: loss:0.03727647043066604\n",
      "iter:22 of epoch:29: loss:0.035931781030281315\n",
      "iter:22 of epoch:30: loss:0.05773531528196059\n",
      "iter:22 of epoch:31: loss:0.2174174890921102\n",
      "iter:22 of epoch:32: loss:0.06542675527960831\n",
      "iter:22 of epoch:33: loss:0.2571512763758763\n",
      "iter:22 of epoch:34: loss:0.23767880804787192\n",
      "iter:22 of epoch:35: loss:0.14832172172010666\n",
      "iter:22 of epoch:36: loss:0.24824647126459692\n",
      "iter:22 of epoch:37: loss:0.10897953153377397\n",
      "iter:22 of epoch:38: loss:0.15266094379457884\n",
      "iter:22 of epoch:39: loss:0.1756280293921535\n",
      "iter:22 of epoch:40: loss:0.36144032874793913\n",
      "iter:23 of epoch:0: loss:0.035683997020278024\n",
      "iter:23 of epoch:1: loss:0.09520524589066985\n",
      "iter:23 of epoch:2: loss:0.1860007288235866\n",
      "iter:23 of epoch:3: loss:0.10422870088239475\n",
      "iter:23 of epoch:4: loss:0.2192461457808037\n",
      "iter:23 of epoch:5: loss:0.255578762999834\n",
      "iter:23 of epoch:6: loss:0.11566888347933957\n",
      "iter:23 of epoch:7: loss:0.12312589924959001\n",
      "iter:23 of epoch:8: loss:0.20415353515630516\n",
      "iter:23 of epoch:9: loss:0.1756262271331634\n",
      "iter:23 of epoch:10: loss:0.04394398597121557\n",
      "iter:23 of epoch:11: loss:0.059065459454711165\n",
      "iter:23 of epoch:12: loss:0.5404532046313191\n",
      "iter:23 of epoch:13: loss:0.39869290556171244\n",
      "iter:23 of epoch:14: loss:0.0705788199968094\n",
      "iter:23 of epoch:15: loss:0.1203982619779114\n",
      "iter:23 of epoch:16: loss:0.2147932013710228\n",
      "iter:23 of epoch:17: loss:0.21172017752306577\n",
      "iter:23 of epoch:18: loss:0.04767669302524017\n",
      "iter:23 of epoch:19: loss:0.07301223217405217\n",
      "iter:23 of epoch:20: loss:0.0628239548275879\n",
      "iter:23 of epoch:21: loss:0.11162684061963743\n",
      "iter:23 of epoch:22: loss:0.15867215102995283\n",
      "iter:23 of epoch:23: loss:0.19409503804740647\n",
      "iter:23 of epoch:24: loss:0.13957341689978034\n",
      "iter:23 of epoch:25: loss:0.11792543742158035\n",
      "iter:23 of epoch:26: loss:0.07264864696559373\n",
      "iter:23 of epoch:27: loss:0.14289914781066332\n",
      "iter:23 of epoch:28: loss:0.07191878840821969\n",
      "iter:23 of epoch:29: loss:0.13307091919129208\n",
      "iter:23 of epoch:30: loss:0.2255481871610851\n",
      "iter:23 of epoch:31: loss:0.11667021945817846\n",
      "iter:23 of epoch:32: loss:0.08892434723189005\n",
      "iter:23 of epoch:33: loss:0.12026345852176071\n",
      "iter:23 of epoch:34: loss:0.05962720551747834\n",
      "iter:23 of epoch:35: loss:0.1062473388269766\n",
      "iter:23 of epoch:36: loss:0.24385921297547813\n",
      "iter:23 of epoch:37: loss:0.2137103511760831\n",
      "iter:23 of epoch:38: loss:0.12281662277960584\n",
      "iter:23 of epoch:39: loss:0.05061553045676093\n",
      "iter:23 of epoch:40: loss:0.36279251923474104\n",
      "iter:24 of epoch:0: loss:0.34622244183729556\n",
      "iter:24 of epoch:1: loss:0.1879622061601515\n",
      "iter:24 of epoch:2: loss:0.22195118690607565\n",
      "iter:24 of epoch:3: loss:0.03369887071117026\n",
      "iter:24 of epoch:4: loss:0.08159163313115751\n",
      "iter:24 of epoch:5: loss:0.05475674333966829\n",
      "iter:24 of epoch:6: loss:0.107754961460207\n",
      "iter:24 of epoch:7: loss:0.3270448818656277\n",
      "iter:24 of epoch:8: loss:0.03469201966184149\n",
      "iter:24 of epoch:9: loss:0.22502352636043824\n",
      "iter:24 of epoch:10: loss:0.5472637728146831\n",
      "iter:24 of epoch:11: loss:0.22300638754099636\n",
      "iter:24 of epoch:12: loss:0.05007139630366965\n",
      "iter:24 of epoch:13: loss:0.10992682650429877\n",
      "iter:24 of epoch:14: loss:0.1566119345269247\n",
      "iter:24 of epoch:15: loss:0.23541265983552268\n",
      "iter:24 of epoch:16: loss:0.06822336952222059\n",
      "iter:24 of epoch:17: loss:0.13035384284335455\n",
      "iter:24 of epoch:18: loss:0.04941488932416172\n",
      "iter:24 of epoch:19: loss:0.07280218944098775\n",
      "iter:24 of epoch:20: loss:0.05506600263972784\n",
      "iter:24 of epoch:21: loss:0.0634764756311236\n",
      "iter:24 of epoch:22: loss:0.06932269481703815\n",
      "iter:24 of epoch:23: loss:0.052821065545882596\n",
      "iter:24 of epoch:24: loss:0.19277827836290926\n",
      "iter:24 of epoch:25: loss:0.16704408201467022\n",
      "iter:24 of epoch:26: loss:0.06714731582438022\n",
      "iter:24 of epoch:27: loss:0.0383958572956596\n",
      "iter:24 of epoch:28: loss:0.05024079360635695\n",
      "iter:24 of epoch:29: loss:0.247792237183585\n",
      "iter:24 of epoch:30: loss:0.16012543977823823\n",
      "iter:24 of epoch:31: loss:0.03698996306387077\n",
      "iter:24 of epoch:32: loss:0.1880957570544258\n",
      "iter:24 of epoch:33: loss:0.29852410876545293\n",
      "iter:24 of epoch:34: loss:0.11318234216085474\n",
      "iter:24 of epoch:35: loss:0.09937457902825675\n",
      "iter:24 of epoch:36: loss:0.040848367874533796\n",
      "iter:24 of epoch:37: loss:0.13899459921599025\n",
      "iter:24 of epoch:38: loss:0.20249772748414122\n",
      "iter:24 of epoch:39: loss:0.04514834278936837\n",
      "iter:24 of epoch:40: loss:0.5183849955506504\n",
      "iter:25 of epoch:0: loss:0.2102511273965276\n",
      "iter:25 of epoch:1: loss:0.061944882501088414\n",
      "iter:25 of epoch:2: loss:0.16553017623608127\n",
      "iter:25 of epoch:3: loss:0.6601380479047505\n",
      "iter:25 of epoch:4: loss:0.15646875930895365\n",
      "iter:25 of epoch:5: loss:0.18759631906751692\n",
      "iter:25 of epoch:6: loss:0.05719395026102385\n",
      "iter:25 of epoch:7: loss:0.1295515335571672\n",
      "iter:25 of epoch:8: loss:0.17270981674765876\n",
      "iter:25 of epoch:9: loss:0.057222135564128696\n",
      "iter:25 of epoch:10: loss:0.13913796549663143\n",
      "iter:25 of epoch:11: loss:0.07917574822325364\n",
      "iter:25 of epoch:12: loss:0.2004507478592866\n",
      "iter:25 of epoch:13: loss:0.1496228216797936\n",
      "iter:25 of epoch:14: loss:0.12438967363491256\n",
      "iter:25 of epoch:15: loss:0.2738161200333719\n",
      "iter:25 of epoch:16: loss:0.08277040027876678\n",
      "iter:25 of epoch:17: loss:0.09259585331246893\n",
      "iter:25 of epoch:18: loss:0.11927836921787786\n",
      "iter:25 of epoch:19: loss:0.0745573910954837\n",
      "iter:25 of epoch:20: loss:0.13183540820532896\n",
      "iter:25 of epoch:21: loss:0.17222567577575038\n",
      "iter:25 of epoch:22: loss:0.2566523689917203\n",
      "iter:25 of epoch:23: loss:0.024366195104143267\n",
      "iter:25 of epoch:24: loss:0.06821201205130004\n",
      "iter:25 of epoch:25: loss:0.021754065466257334\n",
      "iter:25 of epoch:26: loss:0.026978152065380297\n",
      "iter:25 of epoch:27: loss:0.18044411785978884\n",
      "iter:25 of epoch:28: loss:0.17249128940699204\n",
      "iter:25 of epoch:29: loss:0.11368740473352856\n",
      "iter:25 of epoch:30: loss:0.3977598421930081\n",
      "iter:25 of epoch:31: loss:0.05582733412539513\n",
      "iter:25 of epoch:32: loss:0.10824566819628663\n",
      "iter:25 of epoch:33: loss:0.06575873859127629\n",
      "iter:25 of epoch:34: loss:0.05253745214565538\n",
      "iter:25 of epoch:35: loss:0.04146203873376787\n",
      "iter:25 of epoch:36: loss:0.19547037423877436\n",
      "iter:25 of epoch:37: loss:0.05789541119360557\n",
      "iter:25 of epoch:38: loss:0.06847608601523161\n",
      "iter:25 of epoch:39: loss:0.2140474764318397\n",
      "iter:25 of epoch:40: loss:0.0494562244346652\n",
      "iter:26 of epoch:0: loss:0.10017604561566311\n",
      "iter:26 of epoch:1: loss:0.29709393201255463\n",
      "iter:26 of epoch:2: loss:0.20862882855560078\n",
      "iter:26 of epoch:3: loss:0.2589974935422219\n",
      "iter:26 of epoch:4: loss:0.023777978310892744\n",
      "iter:26 of epoch:5: loss:0.14641419254853721\n",
      "iter:26 of epoch:6: loss:0.05839193367817898\n",
      "iter:26 of epoch:7: loss:0.0767355348294319\n",
      "iter:26 of epoch:8: loss:0.08465481271900446\n",
      "iter:26 of epoch:9: loss:0.34302207730644885\n",
      "iter:26 of epoch:10: loss:0.07666851423550333\n",
      "iter:26 of epoch:11: loss:0.21166519053106106\n",
      "iter:26 of epoch:12: loss:0.31491163643716424\n",
      "iter:26 of epoch:13: loss:0.07921832851693891\n",
      "iter:26 of epoch:14: loss:0.16797466341806486\n",
      "iter:26 of epoch:15: loss:0.11714866201424257\n",
      "iter:26 of epoch:16: loss:0.14119093049135667\n",
      "iter:26 of epoch:17: loss:0.2019479868641298\n",
      "iter:26 of epoch:18: loss:0.06591918304408764\n",
      "iter:26 of epoch:19: loss:0.0844620770903416\n",
      "iter:26 of epoch:20: loss:0.020851221638399448\n",
      "iter:26 of epoch:21: loss:0.1097382856539082\n",
      "iter:26 of epoch:22: loss:0.17790432877806212\n",
      "iter:26 of epoch:23: loss:0.057864712803696074\n",
      "iter:26 of epoch:24: loss:0.04820306981545204\n",
      "iter:26 of epoch:25: loss:0.043169252929145446\n",
      "iter:26 of epoch:26: loss:0.16717171208237497\n",
      "iter:26 of epoch:27: loss:0.2326994604116767\n",
      "iter:26 of epoch:28: loss:0.13086098666499463\n",
      "iter:26 of epoch:29: loss:0.07919645997209115\n",
      "iter:26 of epoch:30: loss:0.10713593865335272\n",
      "iter:26 of epoch:31: loss:0.4190766288761057\n",
      "iter:26 of epoch:32: loss:0.055996113812766826\n",
      "iter:26 of epoch:33: loss:0.09456181869720329\n",
      "iter:26 of epoch:34: loss:0.1244784835437801\n",
      "iter:26 of epoch:35: loss:0.11606575932691476\n",
      "iter:26 of epoch:36: loss:0.06712550935236895\n",
      "iter:26 of epoch:37: loss:0.09704542774636873\n",
      "iter:26 of epoch:38: loss:0.09071757520050625\n",
      "iter:26 of epoch:39: loss:0.14464338682068786\n",
      "iter:26 of epoch:40: loss:0.02898738166498474\n",
      "iter:27 of epoch:0: loss:0.054607837988121896\n",
      "iter:27 of epoch:1: loss:0.08597260440651025\n",
      "iter:27 of epoch:2: loss:0.10966715143286287\n",
      "iter:27 of epoch:3: loss:0.19480608071053837\n",
      "iter:27 of epoch:4: loss:0.09263079732375634\n",
      "iter:27 of epoch:5: loss:0.0497510096006901\n",
      "iter:27 of epoch:6: loss:0.5360737009047307\n",
      "iter:27 of epoch:7: loss:0.0535511019412511\n",
      "iter:27 of epoch:8: loss:0.10014883842572238\n",
      "iter:27 of epoch:9: loss:0.06787489752006012\n",
      "iter:27 of epoch:10: loss:0.05685283322953506\n",
      "iter:27 of epoch:11: loss:0.17174307424938232\n",
      "iter:27 of epoch:12: loss:0.04409266844310523\n",
      "iter:27 of epoch:13: loss:0.1746766136193258\n",
      "iter:27 of epoch:14: loss:0.1285778199456741\n",
      "iter:27 of epoch:15: loss:0.0859606022978995\n",
      "iter:27 of epoch:16: loss:0.1267825961477311\n",
      "iter:27 of epoch:17: loss:0.07040990431251153\n",
      "iter:27 of epoch:18: loss:0.0537697533429332\n",
      "iter:27 of epoch:19: loss:0.3421725079143462\n",
      "iter:27 of epoch:20: loss:0.17207371171353833\n",
      "iter:27 of epoch:21: loss:0.13089085329308608\n",
      "iter:27 of epoch:22: loss:0.029158025702917423\n",
      "iter:27 of epoch:23: loss:0.09936185876280774\n",
      "iter:27 of epoch:24: loss:0.11952614023903826\n",
      "iter:27 of epoch:25: loss:0.29721998144124356\n",
      "iter:27 of epoch:26: loss:0.07817002521885043\n",
      "iter:27 of epoch:27: loss:0.1755819082267349\n",
      "iter:27 of epoch:28: loss:0.13364580709981516\n",
      "iter:27 of epoch:29: loss:0.05848277825595462\n",
      "iter:27 of epoch:30: loss:0.1001394603794469\n",
      "iter:27 of epoch:31: loss:0.11225364307921822\n",
      "iter:27 of epoch:32: loss:0.23962260331249507\n",
      "iter:27 of epoch:33: loss:0.059506790004324996\n",
      "iter:27 of epoch:34: loss:0.2551759474351465\n",
      "iter:27 of epoch:35: loss:0.07057428546368302\n",
      "iter:27 of epoch:36: loss:0.13316002183217795\n",
      "iter:27 of epoch:37: loss:0.1829362825225424\n",
      "iter:27 of epoch:38: loss:0.0454641351705067\n",
      "iter:27 of epoch:39: loss:0.1467272769429029\n",
      "iter:27 of epoch:40: loss:0.15415502897835895\n",
      "iter:28 of epoch:0: loss:0.04890016949517401\n",
      "iter:28 of epoch:1: loss:0.17474039528264385\n",
      "iter:28 of epoch:2: loss:0.040831956972999336\n",
      "iter:28 of epoch:3: loss:0.03602100799766354\n",
      "iter:28 of epoch:4: loss:0.07730958378746314\n",
      "iter:28 of epoch:5: loss:0.09209767464749133\n",
      "iter:28 of epoch:6: loss:0.08613224741516082\n",
      "iter:28 of epoch:7: loss:0.15884019686623324\n",
      "iter:28 of epoch:8: loss:0.17022154037078172\n",
      "iter:28 of epoch:9: loss:0.11174665980034995\n",
      "iter:28 of epoch:10: loss:0.041632357066829016\n",
      "iter:28 of epoch:11: loss:0.1221160520361299\n",
      "iter:28 of epoch:12: loss:0.09148635498224007\n",
      "iter:28 of epoch:13: loss:0.05062395812923752\n",
      "iter:28 of epoch:14: loss:0.07902260178542786\n",
      "iter:28 of epoch:15: loss:0.03919665243217168\n",
      "iter:28 of epoch:16: loss:0.11957239954967631\n",
      "iter:28 of epoch:17: loss:0.11655932467643244\n",
      "iter:28 of epoch:18: loss:0.13319327796767527\n",
      "iter:28 of epoch:19: loss:0.3132493246397293\n",
      "iter:28 of epoch:20: loss:0.17341328076593282\n",
      "iter:28 of epoch:21: loss:0.07868918425888288\n",
      "iter:28 of epoch:22: loss:0.1815206792523574\n",
      "iter:28 of epoch:23: loss:0.19646383650620514\n",
      "iter:28 of epoch:24: loss:0.23738386244901113\n",
      "iter:28 of epoch:25: loss:0.10030622882030812\n",
      "iter:28 of epoch:26: loss:0.13255294590673108\n",
      "iter:28 of epoch:27: loss:0.1254989342638521\n",
      "iter:28 of epoch:28: loss:0.1938714787680676\n",
      "iter:28 of epoch:29: loss:0.11383287395151842\n",
      "iter:28 of epoch:30: loss:0.06894243093684471\n",
      "iter:28 of epoch:31: loss:0.159257268779686\n",
      "iter:28 of epoch:32: loss:0.46110347767995447\n",
      "iter:28 of epoch:33: loss:0.12347534912507334\n",
      "iter:28 of epoch:34: loss:0.10443611214821982\n",
      "iter:28 of epoch:35: loss:0.24159443214197526\n",
      "iter:28 of epoch:36: loss:0.1676785611636051\n",
      "iter:28 of epoch:37: loss:0.07597553525035278\n",
      "iter:28 of epoch:38: loss:0.0485681851046959\n",
      "iter:28 of epoch:39: loss:0.045491952674867234\n",
      "iter:28 of epoch:40: loss:0.01551268466611169\n",
      "iter:29 of epoch:0: loss:0.03333559045731181\n",
      "iter:29 of epoch:1: loss:0.029020274887443985\n",
      "iter:29 of epoch:2: loss:0.16058668129919626\n",
      "iter:29 of epoch:3: loss:0.043419588165371545\n",
      "iter:29 of epoch:4: loss:0.04478425880915289\n",
      "iter:29 of epoch:5: loss:0.18458677573746746\n",
      "iter:29 of epoch:6: loss:0.1988114879117811\n",
      "iter:29 of epoch:7: loss:0.07028918625245711\n",
      "iter:29 of epoch:8: loss:0.07562481907017357\n",
      "iter:29 of epoch:9: loss:0.1496996725835335\n",
      "iter:29 of epoch:10: loss:0.1037793971009682\n",
      "iter:29 of epoch:11: loss:0.22548592158775055\n",
      "iter:29 of epoch:12: loss:0.14154945632780908\n",
      "iter:29 of epoch:13: loss:0.15577033957861403\n",
      "iter:29 of epoch:14: loss:0.23606422284899473\n",
      "iter:29 of epoch:15: loss:0.23961085496800782\n",
      "iter:29 of epoch:16: loss:0.14765286911192535\n",
      "iter:29 of epoch:17: loss:0.08192073910883252\n",
      "iter:29 of epoch:18: loss:0.10381574523389299\n",
      "iter:29 of epoch:19: loss:0.18237940992466273\n",
      "iter:29 of epoch:20: loss:0.41464977829537697\n",
      "iter:29 of epoch:21: loss:0.0976126001730881\n",
      "iter:29 of epoch:22: loss:0.09972844900523804\n",
      "iter:29 of epoch:23: loss:0.11498488977070694\n",
      "iter:29 of epoch:24: loss:0.05472814805595132\n",
      "iter:29 of epoch:25: loss:0.06899067537518748\n",
      "iter:29 of epoch:26: loss:0.07945943532040259\n",
      "iter:29 of epoch:27: loss:0.09411568354135666\n",
      "iter:29 of epoch:28: loss:0.23633029673376732\n",
      "iter:29 of epoch:29: loss:0.032378181231568465\n",
      "iter:29 of epoch:30: loss:0.22574581214582584\n",
      "iter:29 of epoch:31: loss:0.13026119365676975\n",
      "iter:29 of epoch:32: loss:0.2315343380907937\n",
      "iter:29 of epoch:33: loss:0.02967782567551689\n",
      "iter:29 of epoch:34: loss:0.03433812180553149\n",
      "iter:29 of epoch:35: loss:0.1044551925314657\n",
      "iter:29 of epoch:36: loss:0.06094162833310749\n",
      "iter:29 of epoch:37: loss:0.03522193480617458\n",
      "iter:29 of epoch:38: loss:0.15818564454872802\n",
      "iter:29 of epoch:39: loss:0.07787340686191721\n",
      "iter:29 of epoch:40: loss:0.027314846556651264\n",
      "iter:30 of epoch:0: loss:0.09607549677916953\n",
      "iter:30 of epoch:1: loss:0.26397969798679066\n",
      "iter:30 of epoch:2: loss:0.08933802880841604\n",
      "iter:30 of epoch:3: loss:0.12573525088100354\n",
      "iter:30 of epoch:4: loss:0.14488732778177188\n",
      "iter:30 of epoch:5: loss:0.04397026215056479\n",
      "iter:30 of epoch:6: loss:0.09960109726925305\n",
      "iter:30 of epoch:7: loss:0.38743800904662884\n",
      "iter:30 of epoch:8: loss:0.15928234610700662\n",
      "iter:30 of epoch:9: loss:0.06951055769899284\n",
      "iter:30 of epoch:10: loss:0.14086827285262615\n",
      "iter:30 of epoch:11: loss:0.1197606034451855\n",
      "iter:30 of epoch:12: loss:0.054298860576103516\n",
      "iter:30 of epoch:13: loss:0.13203510386326114\n",
      "iter:30 of epoch:14: loss:0.08805237669211806\n",
      "iter:30 of epoch:15: loss:0.05577477995065113\n",
      "iter:30 of epoch:16: loss:0.30341603326676625\n",
      "iter:30 of epoch:17: loss:0.06718820213485041\n",
      "iter:30 of epoch:18: loss:0.060275776146274905\n",
      "iter:30 of epoch:19: loss:0.39259452094725295\n",
      "iter:30 of epoch:20: loss:0.08459823879887814\n",
      "iter:30 of epoch:21: loss:0.08643599955347707\n",
      "iter:30 of epoch:22: loss:0.030352434130158406\n",
      "iter:30 of epoch:23: loss:0.0986974015790798\n",
      "iter:30 of epoch:24: loss:0.09293244683276462\n",
      "iter:30 of epoch:25: loss:0.03870229126996478\n",
      "iter:30 of epoch:26: loss:0.28098435755402473\n",
      "iter:30 of epoch:27: loss:0.06788636168546572\n",
      "iter:30 of epoch:28: loss:0.08971205534556008\n",
      "iter:30 of epoch:29: loss:0.21657110213585962\n",
      "iter:30 of epoch:30: loss:0.027605003412639793\n",
      "iter:30 of epoch:31: loss:0.08522479337488789\n",
      "iter:30 of epoch:32: loss:0.08592988505699721\n",
      "iter:30 of epoch:33: loss:0.08056014973742591\n",
      "iter:30 of epoch:34: loss:0.06605142712639415\n",
      "iter:30 of epoch:35: loss:0.212694148812994\n",
      "iter:30 of epoch:36: loss:0.0593100177312013\n",
      "iter:30 of epoch:37: loss:0.09082632955080554\n",
      "iter:30 of epoch:38: loss:0.03960287375875511\n",
      "iter:30 of epoch:39: loss:0.09844982176250719\n",
      "iter:30 of epoch:40: loss:0.10747586992302663\n",
      "iter:31 of epoch:0: loss:0.2477562706144551\n",
      "iter:31 of epoch:1: loss:0.07718223897830673\n",
      "iter:31 of epoch:2: loss:0.09898393723585755\n",
      "iter:31 of epoch:3: loss:0.028392404981949348\n",
      "iter:31 of epoch:4: loss:0.03631319066081632\n",
      "iter:31 of epoch:5: loss:0.10600816841337864\n",
      "iter:31 of epoch:6: loss:0.3971987942692649\n",
      "iter:31 of epoch:7: loss:0.06296315200492689\n",
      "iter:31 of epoch:8: loss:0.176874622031785\n",
      "iter:31 of epoch:9: loss:0.0997098384417749\n",
      "iter:31 of epoch:10: loss:0.04725491196305548\n",
      "iter:31 of epoch:11: loss:0.05156754600965657\n",
      "iter:31 of epoch:12: loss:0.054909381148489235\n",
      "iter:31 of epoch:13: loss:0.08317234872812704\n",
      "iter:31 of epoch:14: loss:0.06898666804691371\n",
      "iter:31 of epoch:15: loss:0.09287871942461745\n",
      "iter:31 of epoch:16: loss:0.1355845516192308\n",
      "iter:31 of epoch:17: loss:0.18375746319554856\n",
      "iter:31 of epoch:18: loss:0.2100443311321601\n",
      "iter:31 of epoch:19: loss:0.08436673104438983\n",
      "iter:31 of epoch:20: loss:0.2716703932771927\n",
      "iter:31 of epoch:21: loss:0.1628778555595078\n",
      "iter:31 of epoch:22: loss:0.09655948972637912\n",
      "iter:31 of epoch:23: loss:0.15452285756705214\n",
      "iter:31 of epoch:24: loss:0.07631050147076034\n",
      "iter:31 of epoch:25: loss:0.09160164800642169\n",
      "iter:31 of epoch:26: loss:0.16716168931325714\n",
      "iter:31 of epoch:27: loss:0.07409724404955195\n",
      "iter:31 of epoch:28: loss:0.3069277910189615\n",
      "iter:31 of epoch:29: loss:0.08699909360564936\n",
      "iter:31 of epoch:30: loss:0.11999736433264893\n",
      "iter:31 of epoch:31: loss:0.021735820004125307\n",
      "iter:31 of epoch:32: loss:0.038123302755887864\n",
      "iter:31 of epoch:33: loss:0.06837236334202411\n",
      "iter:31 of epoch:34: loss:0.086835252151647\n",
      "iter:31 of epoch:35: loss:0.02944755070294327\n",
      "iter:31 of epoch:36: loss:0.13830580236074053\n",
      "iter:31 of epoch:37: loss:0.08649076947837511\n",
      "iter:31 of epoch:38: loss:0.036129497825028924\n",
      "iter:31 of epoch:39: loss:0.20620962803468762\n",
      "iter:31 of epoch:40: loss:0.18349740315094704\n",
      "iter:32 of epoch:0: loss:0.18512164589477434\n",
      "iter:32 of epoch:1: loss:0.06617664266073398\n",
      "iter:32 of epoch:2: loss:0.055879507108868444\n",
      "iter:32 of epoch:3: loss:0.08878099371425698\n",
      "iter:32 of epoch:4: loss:0.19420885911115793\n",
      "iter:32 of epoch:5: loss:0.03569703623955414\n",
      "iter:32 of epoch:6: loss:0.4539540390595934\n",
      "iter:32 of epoch:7: loss:0.13548559110010844\n",
      "iter:32 of epoch:8: loss:0.08212122579711581\n",
      "iter:32 of epoch:9: loss:0.10223938287408282\n",
      "iter:32 of epoch:10: loss:0.12166951013329512\n",
      "iter:32 of epoch:11: loss:0.04815384816537771\n",
      "iter:32 of epoch:12: loss:0.04757302147058233\n",
      "iter:32 of epoch:13: loss:0.04003764430527259\n",
      "iter:32 of epoch:14: loss:0.13013919309042749\n",
      "iter:32 of epoch:15: loss:0.1692005792163635\n",
      "iter:32 of epoch:16: loss:0.3016844642021244\n",
      "iter:32 of epoch:17: loss:0.05452577361857475\n",
      "iter:32 of epoch:18: loss:0.033929745433104896\n",
      "iter:32 of epoch:19: loss:0.1384507366480286\n",
      "iter:32 of epoch:20: loss:0.10023300458369071\n",
      "iter:32 of epoch:21: loss:0.05755283916819155\n",
      "iter:32 of epoch:22: loss:0.12467915700287846\n",
      "iter:32 of epoch:23: loss:0.06925407019117066\n",
      "iter:32 of epoch:24: loss:0.07901728003436971\n",
      "iter:32 of epoch:25: loss:0.1604467089250814\n",
      "iter:32 of epoch:26: loss:0.08339352263679124\n",
      "iter:32 of epoch:27: loss:0.08257004112535996\n",
      "iter:32 of epoch:28: loss:0.30456774000944986\n",
      "iter:32 of epoch:29: loss:0.039670702510077496\n",
      "iter:32 of epoch:30: loss:0.02443050493114304\n",
      "iter:32 of epoch:31: loss:0.1061159980991087\n",
      "iter:32 of epoch:32: loss:0.07007760903144525\n",
      "iter:32 of epoch:33: loss:0.045757746391732815\n",
      "iter:32 of epoch:34: loss:0.1998315529734769\n",
      "iter:32 of epoch:35: loss:0.1512142659337841\n",
      "iter:32 of epoch:36: loss:0.14398202284006226\n",
      "iter:32 of epoch:37: loss:0.08332590087214724\n",
      "iter:32 of epoch:38: loss:0.05752342165172206\n",
      "iter:32 of epoch:39: loss:0.055129348647311824\n",
      "iter:32 of epoch:40: loss:0.20543302923284665\n",
      "iter:33 of epoch:0: loss:0.09503082736715365\n",
      "iter:33 of epoch:1: loss:0.05372753002991502\n",
      "iter:33 of epoch:2: loss:0.10659692639336649\n",
      "iter:33 of epoch:3: loss:0.14586809953557575\n",
      "iter:33 of epoch:4: loss:0.13437881109802105\n",
      "iter:33 of epoch:5: loss:0.08043306426161732\n",
      "iter:33 of epoch:6: loss:0.08134819083123138\n",
      "iter:33 of epoch:7: loss:0.06322342426052201\n",
      "iter:33 of epoch:8: loss:0.06662293549448212\n",
      "iter:33 of epoch:9: loss:0.03529492331921297\n",
      "iter:33 of epoch:10: loss:0.13898934581611763\n",
      "iter:33 of epoch:11: loss:0.2070837493038768\n",
      "iter:33 of epoch:12: loss:0.15966365321655923\n",
      "iter:33 of epoch:13: loss:0.11609036353666294\n",
      "iter:33 of epoch:14: loss:0.04892077325691242\n",
      "iter:33 of epoch:15: loss:0.047681442663993635\n",
      "iter:33 of epoch:16: loss:0.08880676074444308\n",
      "iter:33 of epoch:17: loss:0.04215632583130813\n",
      "iter:33 of epoch:18: loss:0.09446819247729651\n",
      "iter:33 of epoch:19: loss:0.07167810713450999\n",
      "iter:33 of epoch:20: loss:0.09840189488783095\n",
      "iter:33 of epoch:21: loss:0.20682445373788796\n",
      "iter:33 of epoch:22: loss:0.25600834866779937\n",
      "iter:33 of epoch:23: loss:0.04912304325194076\n",
      "iter:33 of epoch:24: loss:0.047225759841390366\n",
      "iter:33 of epoch:25: loss:0.16425279971697984\n",
      "iter:33 of epoch:26: loss:0.061545214513715765\n",
      "iter:33 of epoch:27: loss:0.2540743510683797\n",
      "iter:33 of epoch:28: loss:0.5251860138315247\n",
      "iter:33 of epoch:29: loss:0.10611524970865042\n",
      "iter:33 of epoch:30: loss:0.060459462547621245\n",
      "iter:33 of epoch:31: loss:0.2426119376640603\n",
      "iter:33 of epoch:32: loss:0.1331179571823188\n",
      "iter:33 of epoch:33: loss:0.05480494172263042\n",
      "iter:33 of epoch:34: loss:0.0393213136129764\n",
      "iter:33 of epoch:35: loss:0.044439757959294515\n",
      "iter:33 of epoch:36: loss:0.04442918890756046\n",
      "iter:33 of epoch:37: loss:0.05960485920990157\n",
      "iter:33 of epoch:38: loss:0.07283977346288412\n",
      "iter:33 of epoch:39: loss:0.08244904790363669\n",
      "iter:33 of epoch:40: loss:0.012346225821711809\n",
      "iter:34 of epoch:0: loss:0.1657705113365934\n",
      "iter:34 of epoch:1: loss:0.044489878196335916\n",
      "iter:34 of epoch:2: loss:0.1005061272137864\n",
      "iter:34 of epoch:3: loss:0.026642946228514437\n",
      "iter:34 of epoch:4: loss:0.04133217264446913\n",
      "iter:34 of epoch:5: loss:0.168042074186115\n",
      "iter:34 of epoch:6: loss:0.08980152395471115\n",
      "iter:34 of epoch:7: loss:0.10567434044740802\n",
      "iter:34 of epoch:8: loss:0.08714482238120189\n",
      "iter:34 of epoch:9: loss:0.05259018204859951\n",
      "iter:34 of epoch:10: loss:0.18836443618529455\n",
      "iter:34 of epoch:11: loss:0.14425881519403222\n",
      "iter:34 of epoch:12: loss:0.15334357788288644\n",
      "iter:34 of epoch:13: loss:0.07865179256329956\n",
      "iter:34 of epoch:14: loss:0.5274250862117753\n",
      "iter:34 of epoch:15: loss:0.03405230783525114\n",
      "iter:34 of epoch:16: loss:0.08267312578939157\n",
      "iter:34 of epoch:17: loss:0.034059258115986125\n",
      "iter:34 of epoch:18: loss:0.08714090215729778\n",
      "iter:34 of epoch:19: loss:0.12155543135491512\n",
      "iter:34 of epoch:20: loss:0.043093226621910896\n",
      "iter:34 of epoch:21: loss:0.05740588210173379\n",
      "iter:34 of epoch:22: loss:0.06060685046413563\n",
      "iter:34 of epoch:23: loss:0.13308028925222512\n",
      "iter:34 of epoch:24: loss:0.07983464809760896\n",
      "iter:34 of epoch:25: loss:0.11130722052396529\n",
      "iter:34 of epoch:26: loss:0.11198353420736054\n",
      "iter:34 of epoch:27: loss:0.1688045471526661\n",
      "iter:34 of epoch:28: loss:0.07609495445931971\n",
      "iter:34 of epoch:29: loss:0.06804978836175804\n",
      "iter:34 of epoch:30: loss:0.28097016987208334\n",
      "iter:34 of epoch:31: loss:0.14729749859211833\n",
      "iter:34 of epoch:32: loss:0.06284278372334354\n",
      "iter:34 of epoch:33: loss:0.07794490664248767\n",
      "iter:34 of epoch:34: loss:0.10529438649859406\n",
      "iter:34 of epoch:35: loss:0.08759499626259522\n",
      "iter:34 of epoch:36: loss:0.04299632931961665\n",
      "iter:34 of epoch:37: loss:0.07700393931564234\n",
      "iter:34 of epoch:38: loss:0.06377920199454114\n",
      "iter:34 of epoch:39: loss:0.1126346686153833\n",
      "iter:34 of epoch:40: loss:0.18910876287482659\n",
      "iter:35 of epoch:0: loss:0.14009774826225857\n",
      "iter:35 of epoch:1: loss:0.08414704956695038\n",
      "iter:35 of epoch:2: loss:0.25269282978039825\n",
      "iter:35 of epoch:3: loss:0.3620607332565454\n",
      "iter:35 of epoch:4: loss:0.07678617626807781\n",
      "iter:35 of epoch:5: loss:0.15262675113511442\n",
      "iter:35 of epoch:6: loss:0.053642611569447995\n",
      "iter:35 of epoch:7: loss:0.07645370943342143\n",
      "iter:35 of epoch:8: loss:0.13702356113719044\n",
      "iter:35 of epoch:9: loss:0.07240324596422745\n",
      "iter:35 of epoch:10: loss:0.0610930919647925\n",
      "iter:35 of epoch:11: loss:0.03851406630802647\n",
      "iter:35 of epoch:12: loss:0.03188977806334087\n",
      "iter:35 of epoch:13: loss:0.11899940662442639\n",
      "iter:35 of epoch:14: loss:0.01761928560861716\n",
      "iter:35 of epoch:15: loss:0.17258025883628741\n",
      "iter:35 of epoch:16: loss:0.24805125681972\n",
      "iter:35 of epoch:17: loss:0.038728178750672436\n",
      "iter:35 of epoch:18: loss:0.06192305860546952\n",
      "iter:35 of epoch:19: loss:0.1319734854550161\n",
      "iter:35 of epoch:20: loss:0.08132677589066775\n",
      "iter:35 of epoch:21: loss:0.011319674662904181\n",
      "iter:35 of epoch:22: loss:0.04061023651500487\n",
      "iter:35 of epoch:23: loss:0.058801806317823314\n",
      "iter:35 of epoch:24: loss:0.2342825695303913\n",
      "iter:35 of epoch:25: loss:0.04953893882031335\n",
      "iter:35 of epoch:26: loss:0.09431537766935912\n",
      "iter:35 of epoch:27: loss:0.10848780343698264\n",
      "iter:35 of epoch:28: loss:0.07402914365507432\n",
      "iter:35 of epoch:29: loss:0.11906884961605016\n",
      "iter:35 of epoch:30: loss:0.18786412090770305\n",
      "iter:35 of epoch:31: loss:0.08483068732806306\n",
      "iter:35 of epoch:32: loss:0.0292124344488272\n",
      "iter:35 of epoch:33: loss:0.15510471959303257\n",
      "iter:35 of epoch:34: loss:0.11077568133051016\n",
      "iter:35 of epoch:35: loss:0.07976820512945373\n",
      "iter:35 of epoch:36: loss:0.1010694246594697\n",
      "iter:35 of epoch:37: loss:0.11239022902821802\n",
      "iter:35 of epoch:38: loss:0.10957801737954447\n",
      "iter:35 of epoch:39: loss:0.04656721799726764\n",
      "iter:35 of epoch:40: loss:0.11850467710101809\n",
      "iter:36 of epoch:0: loss:0.0480432860353873\n",
      "iter:36 of epoch:1: loss:0.06092761990955475\n",
      "iter:36 of epoch:2: loss:0.29836272874987446\n",
      "iter:36 of epoch:3: loss:0.12320299056041073\n",
      "iter:36 of epoch:4: loss:0.09363675815625636\n",
      "iter:36 of epoch:5: loss:0.1307269066893406\n",
      "iter:36 of epoch:6: loss:0.03013480183606073\n",
      "iter:36 of epoch:7: loss:0.03776439215420385\n",
      "iter:36 of epoch:8: loss:0.0809148589951083\n",
      "iter:36 of epoch:9: loss:0.1772306269513365\n",
      "iter:36 of epoch:10: loss:0.05115907785068038\n",
      "iter:36 of epoch:11: loss:0.08788906638648811\n",
      "iter:36 of epoch:12: loss:0.13067844645430982\n",
      "iter:36 of epoch:13: loss:0.02510041877815124\n",
      "iter:36 of epoch:14: loss:0.1382137026350771\n",
      "iter:36 of epoch:15: loss:0.02866338280474845\n",
      "iter:36 of epoch:16: loss:0.16612763553230975\n",
      "iter:36 of epoch:17: loss:0.07350884095733112\n",
      "iter:36 of epoch:18: loss:0.05130430141744926\n",
      "iter:36 of epoch:19: loss:0.050285379485592085\n",
      "iter:36 of epoch:20: loss:0.08093728851471935\n",
      "iter:36 of epoch:21: loss:0.15230333924366848\n",
      "iter:36 of epoch:22: loss:0.0470427920646838\n",
      "iter:36 of epoch:23: loss:0.17691566549782262\n",
      "iter:36 of epoch:24: loss:0.34433872646147556\n",
      "iter:36 of epoch:25: loss:0.3422780184621722\n",
      "iter:36 of epoch:26: loss:0.12107266873534088\n",
      "iter:36 of epoch:27: loss:0.04271481550391335\n",
      "iter:36 of epoch:28: loss:0.19962912887361633\n",
      "iter:36 of epoch:29: loss:0.04072955582386339\n",
      "iter:36 of epoch:30: loss:0.0646588496438545\n",
      "iter:36 of epoch:31: loss:0.06126615231442691\n",
      "iter:36 of epoch:32: loss:0.13820234456137023\n",
      "iter:36 of epoch:33: loss:0.11842537635159309\n",
      "iter:36 of epoch:34: loss:0.04028788205780613\n",
      "iter:36 of epoch:35: loss:0.018559438279732464\n",
      "iter:36 of epoch:36: loss:0.04087098172157554\n",
      "iter:36 of epoch:37: loss:0.0301341930954999\n",
      "iter:36 of epoch:38: loss:0.1191684840288908\n",
      "iter:36 of epoch:39: loss:0.07836323866986986\n",
      "iter:36 of epoch:40: loss:0.06440050698099316\n",
      "iter:37 of epoch:0: loss:0.07121555037595488\n",
      "iter:37 of epoch:1: loss:0.1183099357086684\n",
      "iter:37 of epoch:2: loss:0.05316787448467105\n",
      "iter:37 of epoch:3: loss:0.04417530368292074\n",
      "iter:37 of epoch:4: loss:0.026023582021373743\n",
      "iter:37 of epoch:5: loss:0.1085721742880923\n",
      "iter:37 of epoch:6: loss:0.06110313562279318\n",
      "iter:37 of epoch:7: loss:0.2356415880633409\n",
      "iter:37 of epoch:8: loss:0.10213656151057307\n",
      "iter:37 of epoch:9: loss:0.17243342280468382\n",
      "iter:37 of epoch:10: loss:0.07494769543255678\n",
      "iter:37 of epoch:11: loss:0.051119218808294606\n",
      "iter:37 of epoch:12: loss:0.3395474213639109\n",
      "iter:37 of epoch:13: loss:0.03614941568072\n",
      "iter:37 of epoch:14: loss:0.1022778916521454\n",
      "iter:37 of epoch:15: loss:0.3136194785620747\n",
      "iter:37 of epoch:16: loss:0.07352983464655452\n",
      "iter:37 of epoch:17: loss:0.0682654608151534\n",
      "iter:37 of epoch:18: loss:0.06468144738682415\n",
      "iter:37 of epoch:19: loss:0.11042954492444779\n",
      "iter:37 of epoch:20: loss:0.18476718266314496\n",
      "iter:37 of epoch:21: loss:0.09020083707017154\n",
      "iter:37 of epoch:22: loss:0.076596798462423\n",
      "iter:37 of epoch:23: loss:0.02749944225451168\n",
      "iter:37 of epoch:24: loss:0.12155938222310761\n",
      "iter:37 of epoch:25: loss:0.030294893935914263\n",
      "iter:37 of epoch:26: loss:0.07884350770075514\n",
      "iter:37 of epoch:27: loss:0.13729080851677672\n",
      "iter:37 of epoch:28: loss:0.06261671061906823\n",
      "iter:37 of epoch:29: loss:0.09820100429751634\n",
      "iter:37 of epoch:30: loss:0.08018703174988333\n",
      "iter:37 of epoch:31: loss:0.04877409968582893\n",
      "iter:37 of epoch:32: loss:0.07523201634759881\n",
      "iter:37 of epoch:33: loss:0.04633478333821871\n",
      "iter:37 of epoch:34: loss:0.11012641490829685\n",
      "iter:37 of epoch:35: loss:0.08838069605371253\n",
      "iter:37 of epoch:36: loss:0.1028791691913958\n",
      "iter:37 of epoch:37: loss:0.044426692151264754\n",
      "iter:37 of epoch:38: loss:0.12501112386559163\n",
      "iter:37 of epoch:39: loss:0.18483722773672034\n",
      "iter:37 of epoch:40: loss:0.07735885537789974\n",
      "iter:38 of epoch:0: loss:0.03678483937503777\n",
      "iter:38 of epoch:1: loss:0.08570560941925982\n",
      "iter:38 of epoch:2: loss:0.1056510849812958\n",
      "iter:38 of epoch:3: loss:0.09233450792241402\n",
      "iter:38 of epoch:4: loss:0.025037173051465793\n",
      "iter:38 of epoch:5: loss:0.03153890489931851\n",
      "iter:38 of epoch:6: loss:0.15402159904637502\n",
      "iter:38 of epoch:7: loss:0.04208562136299048\n",
      "iter:38 of epoch:8: loss:0.28324755654920847\n",
      "iter:38 of epoch:9: loss:0.23814702847144656\n",
      "iter:38 of epoch:10: loss:0.0741449693761003\n",
      "iter:38 of epoch:11: loss:0.1380607304890522\n",
      "iter:38 of epoch:12: loss:0.06522282585514916\n",
      "iter:38 of epoch:13: loss:0.11245013515110161\n",
      "iter:38 of epoch:14: loss:0.14178233316132466\n",
      "iter:38 of epoch:15: loss:0.05183192687887479\n",
      "iter:38 of epoch:16: loss:0.03966063475745221\n",
      "iter:38 of epoch:17: loss:0.05032740371617702\n",
      "iter:38 of epoch:18: loss:0.04468018416114372\n",
      "iter:38 of epoch:19: loss:0.0596867238806944\n",
      "iter:38 of epoch:20: loss:0.08889314392819596\n",
      "iter:38 of epoch:21: loss:0.031356561128710374\n",
      "iter:38 of epoch:22: loss:0.02540244566320553\n",
      "iter:38 of epoch:23: loss:0.16795153231746401\n",
      "iter:38 of epoch:24: loss:0.04128974952178014\n",
      "iter:38 of epoch:25: loss:0.04716070248044344\n",
      "iter:38 of epoch:26: loss:0.034988225675685616\n",
      "iter:38 of epoch:27: loss:0.08053357080160148\n",
      "iter:38 of epoch:28: loss:0.36471461435994496\n",
      "iter:38 of epoch:29: loss:0.20797795246867504\n",
      "iter:38 of epoch:30: loss:0.12354108293290315\n",
      "iter:38 of epoch:31: loss:0.022089545438336307\n",
      "iter:38 of epoch:32: loss:0.08280452142846252\n",
      "iter:38 of epoch:33: loss:0.049404132900349305\n",
      "iter:38 of epoch:34: loss:0.1697081520064229\n",
      "iter:38 of epoch:35: loss:0.09609178064183185\n",
      "iter:38 of epoch:36: loss:0.03023592738990667\n",
      "iter:38 of epoch:37: loss:0.2428112780591066\n",
      "iter:38 of epoch:38: loss:0.07947147558181183\n",
      "iter:38 of epoch:39: loss:0.0663663861743573\n",
      "iter:38 of epoch:40: loss:0.15498920359743723\n",
      "iter:39 of epoch:0: loss:0.2678840013524576\n",
      "iter:39 of epoch:1: loss:0.23970695669123393\n",
      "iter:39 of epoch:2: loss:0.1734078710692427\n",
      "iter:39 of epoch:3: loss:0.07666820994909787\n",
      "iter:39 of epoch:4: loss:0.03693260145249261\n",
      "iter:39 of epoch:5: loss:0.05336801469946262\n",
      "iter:39 of epoch:6: loss:0.06467041812530924\n",
      "iter:39 of epoch:7: loss:0.10559919770347936\n",
      "iter:39 of epoch:8: loss:0.08735310266957373\n",
      "iter:39 of epoch:9: loss:0.09851420997997928\n",
      "iter:39 of epoch:10: loss:0.041887723891763526\n",
      "iter:39 of epoch:11: loss:0.012707448812271843\n",
      "iter:39 of epoch:12: loss:0.05704730017627798\n",
      "iter:39 of epoch:13: loss:0.10477499923076605\n",
      "iter:39 of epoch:14: loss:0.04121926397362193\n",
      "iter:39 of epoch:15: loss:0.08297758918972731\n",
      "iter:39 of epoch:16: loss:0.07916344638979642\n",
      "iter:39 of epoch:17: loss:0.11719904289802455\n",
      "iter:39 of epoch:18: loss:0.11944037408587622\n",
      "iter:39 of epoch:19: loss:0.03937495551648918\n",
      "iter:39 of epoch:20: loss:0.06320934815815253\n",
      "iter:39 of epoch:21: loss:0.2521558994064228\n",
      "iter:39 of epoch:22: loss:0.07023170045811417\n",
      "iter:39 of epoch:23: loss:0.06953162681979616\n",
      "iter:39 of epoch:24: loss:0.32591799721097053\n",
      "iter:39 of epoch:25: loss:0.03684198624496117\n",
      "iter:39 of epoch:26: loss:0.05476788783440798\n",
      "iter:39 of epoch:27: loss:0.11618411654710739\n",
      "iter:39 of epoch:28: loss:0.09297834603859112\n",
      "iter:39 of epoch:29: loss:0.03925157201964721\n",
      "iter:39 of epoch:30: loss:0.04312599983720368\n",
      "iter:39 of epoch:31: loss:0.15287199436854795\n",
      "iter:39 of epoch:32: loss:0.08725926599400778\n",
      "iter:39 of epoch:33: loss:0.020835570527690656\n",
      "iter:39 of epoch:34: loss:0.057982533789041835\n",
      "iter:39 of epoch:35: loss:0.1737426711663767\n",
      "iter:39 of epoch:36: loss:0.06750744850378122\n",
      "iter:39 of epoch:37: loss:0.03239186342814282\n",
      "iter:39 of epoch:38: loss:0.06970214091440832\n",
      "iter:39 of epoch:39: loss:0.149702494259052\n",
      "iter:39 of epoch:40: loss:0.049837022437747576\n",
      "iter:40 of epoch:0: loss:0.2048186065178676\n",
      "iter:40 of epoch:1: loss:0.060123874174215816\n",
      "iter:40 of epoch:2: loss:0.08294024727567303\n",
      "iter:40 of epoch:3: loss:0.11101553239282362\n",
      "iter:40 of epoch:4: loss:0.06408542019046254\n",
      "iter:40 of epoch:5: loss:0.07214929790541662\n",
      "iter:40 of epoch:6: loss:0.05317038023338001\n",
      "iter:40 of epoch:7: loss:0.11869467259738756\n",
      "iter:40 of epoch:8: loss:0.058310387902155036\n",
      "iter:40 of epoch:9: loss:0.04705098824271244\n",
      "iter:40 of epoch:10: loss:0.055938876625492365\n",
      "iter:40 of epoch:11: loss:0.13309522978611835\n",
      "iter:40 of epoch:12: loss:0.1855562086778127\n",
      "iter:40 of epoch:13: loss:0.4106949969307527\n",
      "iter:40 of epoch:14: loss:0.03839435930286776\n",
      "iter:40 of epoch:15: loss:0.10092766457518274\n",
      "iter:40 of epoch:16: loss:0.05581360616622162\n",
      "iter:40 of epoch:17: loss:0.10818313254085787\n",
      "iter:40 of epoch:18: loss:0.041025293385284836\n",
      "iter:40 of epoch:19: loss:0.1604980661951914\n",
      "iter:40 of epoch:20: loss:0.11755590339134844\n",
      "iter:40 of epoch:21: loss:0.055857816086945976\n",
      "iter:40 of epoch:22: loss:0.03866983485100546\n",
      "iter:40 of epoch:23: loss:0.061036710606984936\n",
      "iter:40 of epoch:24: loss:0.09545153568885381\n",
      "iter:40 of epoch:25: loss:0.07869489024506508\n",
      "iter:40 of epoch:26: loss:0.12998675988241806\n",
      "iter:40 of epoch:27: loss:0.05293787051349434\n",
      "iter:40 of epoch:28: loss:0.025536905086666062\n",
      "iter:40 of epoch:29: loss:0.06454754316158094\n",
      "iter:40 of epoch:30: loss:0.0384534579996148\n",
      "iter:40 of epoch:31: loss:0.0592025855901259\n",
      "iter:40 of epoch:32: loss:0.02517568846376738\n",
      "iter:40 of epoch:33: loss:0.10829570223443827\n",
      "iter:40 of epoch:34: loss:0.020566006709437246\n",
      "iter:40 of epoch:35: loss:0.10879102334765267\n",
      "iter:40 of epoch:36: loss:0.13222564149166247\n",
      "iter:40 of epoch:37: loss:0.038582762703032625\n",
      "iter:40 of epoch:38: loss:0.07928911711890986\n",
      "iter:40 of epoch:39: loss:0.11382004839836331\n",
      "iter:40 of epoch:40: loss:0.5021059772748975\n",
      "iter:41 of epoch:0: loss:0.08655674933488441\n",
      "iter:41 of epoch:1: loss:0.11520183886227386\n",
      "iter:41 of epoch:2: loss:0.045370396974618976\n",
      "iter:41 of epoch:3: loss:0.07927059966980386\n",
      "iter:41 of epoch:4: loss:0.06465706977121735\n",
      "iter:41 of epoch:5: loss:0.03781811552090434\n",
      "iter:41 of epoch:6: loss:0.11022483764092321\n",
      "iter:41 of epoch:7: loss:0.09895519092231028\n",
      "iter:41 of epoch:8: loss:0.040013369757020925\n",
      "iter:41 of epoch:9: loss:0.029862801779023194\n",
      "iter:41 of epoch:10: loss:0.09108154934913135\n",
      "iter:41 of epoch:11: loss:0.06849657060680668\n",
      "iter:41 of epoch:12: loss:0.05723732881976403\n",
      "iter:41 of epoch:13: loss:0.09062334690405666\n",
      "iter:41 of epoch:14: loss:0.08201114874890185\n",
      "iter:41 of epoch:15: loss:0.1171595937929875\n",
      "iter:41 of epoch:16: loss:0.06912338406945308\n",
      "iter:41 of epoch:17: loss:0.05350343748864288\n",
      "iter:41 of epoch:18: loss:0.08413858543574267\n",
      "iter:41 of epoch:19: loss:0.04129000339272809\n",
      "iter:41 of epoch:20: loss:0.055853617119718355\n",
      "iter:41 of epoch:21: loss:0.15517986378269077\n",
      "iter:41 of epoch:22: loss:0.10159961224540046\n",
      "iter:41 of epoch:23: loss:0.03699931996110303\n",
      "iter:41 of epoch:24: loss:0.30295592254594206\n",
      "iter:41 of epoch:25: loss:0.03567062396915016\n",
      "iter:41 of epoch:26: loss:0.1326748300050808\n",
      "iter:41 of epoch:27: loss:0.22395486081833077\n",
      "iter:41 of epoch:28: loss:0.11790603890334406\n",
      "iter:41 of epoch:29: loss:0.13558413632390895\n",
      "iter:41 of epoch:30: loss:0.20235631004768967\n",
      "iter:41 of epoch:31: loss:0.03008394495468155\n",
      "iter:41 of epoch:32: loss:0.15761465888637075\n",
      "iter:41 of epoch:33: loss:0.029584092144158098\n",
      "iter:41 of epoch:34: loss:0.11031298194898606\n",
      "iter:41 of epoch:35: loss:0.04133380020662842\n",
      "iter:41 of epoch:36: loss:0.046562819589184665\n",
      "iter:41 of epoch:37: loss:0.03633681660463145\n",
      "iter:41 of epoch:38: loss:0.21934546915547132\n",
      "iter:41 of epoch:39: loss:0.05924291926181193\n",
      "iter:41 of epoch:40: loss:0.09430159978162404\n",
      "iter:42 of epoch:0: loss:0.0498967897659179\n",
      "iter:42 of epoch:1: loss:0.027049059943147834\n",
      "iter:42 of epoch:2: loss:0.03216387909799055\n",
      "iter:42 of epoch:3: loss:0.14991281485574015\n",
      "iter:42 of epoch:4: loss:0.05128661663699592\n",
      "iter:42 of epoch:5: loss:0.066897709824666\n",
      "iter:42 of epoch:6: loss:0.1778944909045236\n",
      "iter:42 of epoch:7: loss:0.21595448819658966\n",
      "iter:42 of epoch:8: loss:0.1879989118271792\n",
      "iter:42 of epoch:9: loss:0.0778474750435746\n",
      "iter:42 of epoch:10: loss:0.07752344598071789\n",
      "iter:42 of epoch:11: loss:0.03610763551286957\n",
      "iter:42 of epoch:12: loss:0.04461546204243878\n",
      "iter:42 of epoch:13: loss:0.03290879038352717\n",
      "iter:42 of epoch:14: loss:0.08182207773155778\n",
      "iter:42 of epoch:15: loss:0.03178258265916509\n",
      "iter:42 of epoch:16: loss:0.08266965049669481\n",
      "iter:42 of epoch:17: loss:0.36738996319939743\n",
      "iter:42 of epoch:18: loss:0.07755136586760594\n",
      "iter:42 of epoch:19: loss:0.03351953371416286\n",
      "iter:42 of epoch:20: loss:0.0668058518834121\n",
      "iter:42 of epoch:21: loss:0.056656458446593574\n",
      "iter:42 of epoch:22: loss:0.024717667168170882\n",
      "iter:42 of epoch:23: loss:0.059317461898024934\n",
      "iter:42 of epoch:24: loss:0.17750913976818794\n",
      "iter:42 of epoch:25: loss:0.11064013281012183\n",
      "iter:42 of epoch:26: loss:0.06283315239439832\n",
      "iter:42 of epoch:27: loss:0.1938408720206522\n",
      "iter:42 of epoch:28: loss:0.06582773231778447\n",
      "iter:42 of epoch:29: loss:0.06609816170039991\n",
      "iter:42 of epoch:30: loss:0.13914846346580148\n",
      "iter:42 of epoch:31: loss:0.051118257709166316\n",
      "iter:42 of epoch:32: loss:0.07009842758396219\n",
      "iter:42 of epoch:33: loss:0.09265829336072305\n",
      "iter:42 of epoch:34: loss:0.09216856678396705\n",
      "iter:42 of epoch:35: loss:0.07000278428836204\n",
      "iter:42 of epoch:36: loss:0.022613211989725858\n",
      "iter:42 of epoch:37: loss:0.04620974931878688\n",
      "iter:42 of epoch:38: loss:0.1453684392075696\n",
      "iter:42 of epoch:39: loss:0.12683502283114495\n",
      "iter:42 of epoch:40: loss:0.025150868572160992\n",
      "iter:43 of epoch:0: loss:0.07113850058228145\n",
      "iter:43 of epoch:1: loss:0.07336239373444725\n",
      "iter:43 of epoch:2: loss:0.0739907229227924\n",
      "iter:43 of epoch:3: loss:0.04908584839331143\n",
      "iter:43 of epoch:4: loss:0.0971307579826268\n",
      "iter:43 of epoch:5: loss:0.18194424914616794\n",
      "iter:43 of epoch:6: loss:0.05299867358729775\n",
      "iter:43 of epoch:7: loss:0.04115447476432628\n",
      "iter:43 of epoch:8: loss:0.08790815623773234\n",
      "iter:43 of epoch:9: loss:0.1314151621574946\n",
      "iter:43 of epoch:10: loss:0.047960867001981995\n",
      "iter:43 of epoch:11: loss:0.05654628766418668\n",
      "iter:43 of epoch:12: loss:0.09528385433882894\n",
      "iter:43 of epoch:13: loss:0.09385384454533645\n",
      "iter:43 of epoch:14: loss:0.10845149463144255\n",
      "iter:43 of epoch:15: loss:0.05386183946845903\n",
      "iter:43 of epoch:16: loss:0.06489585675115651\n",
      "iter:43 of epoch:17: loss:0.03739616521971901\n",
      "iter:43 of epoch:18: loss:0.05731701515276386\n",
      "iter:43 of epoch:19: loss:0.3397924171756964\n",
      "iter:43 of epoch:20: loss:0.13609540469232842\n",
      "iter:43 of epoch:21: loss:0.01988000996864913\n",
      "iter:43 of epoch:22: loss:0.14191130509081223\n",
      "iter:43 of epoch:23: loss:0.046705923016791225\n",
      "iter:43 of epoch:24: loss:0.09826053176816738\n",
      "iter:43 of epoch:25: loss:0.0584873030241123\n",
      "iter:43 of epoch:26: loss:0.04612047261176432\n",
      "iter:43 of epoch:27: loss:0.30089947748468204\n",
      "iter:43 of epoch:28: loss:0.04469983906455931\n",
      "iter:43 of epoch:29: loss:0.04731611809878603\n",
      "iter:43 of epoch:30: loss:0.03988193001235813\n",
      "iter:43 of epoch:31: loss:0.08865814172370266\n",
      "iter:43 of epoch:32: loss:0.21061858086509982\n",
      "iter:43 of epoch:33: loss:0.0988802146437685\n",
      "iter:43 of epoch:34: loss:0.032114724054667934\n",
      "iter:43 of epoch:35: loss:0.032144517246224844\n",
      "iter:43 of epoch:36: loss:0.06099261061921555\n",
      "iter:43 of epoch:37: loss:0.0711355535398935\n",
      "iter:43 of epoch:38: loss:0.07866284945767356\n",
      "iter:43 of epoch:39: loss:0.0848888924555843\n",
      "iter:43 of epoch:40: loss:0.060172331907128285\n",
      "iter:44 of epoch:0: loss:0.05401458785870141\n",
      "iter:44 of epoch:1: loss:0.09646858009577328\n",
      "iter:44 of epoch:2: loss:0.070222450284217\n",
      "iter:44 of epoch:3: loss:0.19802427082085464\n",
      "iter:44 of epoch:4: loss:0.03821902268122041\n",
      "iter:44 of epoch:5: loss:0.10386150458047907\n",
      "iter:44 of epoch:6: loss:0.1757261865198395\n",
      "iter:44 of epoch:7: loss:0.02896546458144189\n",
      "iter:44 of epoch:8: loss:0.09681192632440275\n",
      "iter:44 of epoch:9: loss:0.035014539755087934\n",
      "iter:44 of epoch:10: loss:0.22327541630010922\n",
      "iter:44 of epoch:11: loss:0.15582590367623178\n",
      "iter:44 of epoch:12: loss:0.04668611776866767\n",
      "iter:44 of epoch:13: loss:0.15475793467918017\n",
      "iter:44 of epoch:14: loss:0.03839995724782396\n",
      "iter:44 of epoch:15: loss:0.04663229766534925\n",
      "iter:44 of epoch:16: loss:0.04690227182918387\n",
      "iter:44 of epoch:17: loss:0.04743945579230049\n",
      "iter:44 of epoch:18: loss:0.05167336480967334\n",
      "iter:44 of epoch:19: loss:0.05044105621099485\n",
      "iter:44 of epoch:20: loss:0.07602546372220163\n",
      "iter:44 of epoch:21: loss:0.03315855630090427\n",
      "iter:44 of epoch:22: loss:0.42301632786891086\n",
      "iter:44 of epoch:23: loss:0.15511892043267822\n",
      "iter:44 of epoch:24: loss:0.06731264504433135\n",
      "iter:44 of epoch:25: loss:0.13154841222098299\n",
      "iter:44 of epoch:26: loss:0.023519697194253952\n",
      "iter:44 of epoch:27: loss:0.015043847261435131\n",
      "iter:44 of epoch:28: loss:0.022663633515935713\n",
      "iter:44 of epoch:29: loss:0.05584102492507124\n",
      "iter:44 of epoch:30: loss:0.07740218625093456\n",
      "iter:44 of epoch:31: loss:0.06392481404518774\n",
      "iter:44 of epoch:32: loss:0.07314980122221085\n",
      "iter:44 of epoch:33: loss:0.07729870689441862\n",
      "iter:44 of epoch:34: loss:0.0587384640525791\n",
      "iter:44 of epoch:35: loss:0.1416706568544687\n",
      "iter:44 of epoch:36: loss:0.08280293367527061\n",
      "iter:44 of epoch:37: loss:0.10588862287371723\n",
      "iter:44 of epoch:38: loss:0.018664323851162814\n",
      "iter:44 of epoch:39: loss:0.030890765617684814\n",
      "iter:44 of epoch:40: loss:0.029382996002422883\n",
      "iter:45 of epoch:0: loss:0.3131999882930049\n",
      "iter:45 of epoch:1: loss:0.1066570345356315\n",
      "iter:45 of epoch:2: loss:0.16146367820523852\n",
      "iter:45 of epoch:3: loss:0.05008803376127123\n",
      "iter:45 of epoch:4: loss:0.08206226326961533\n",
      "iter:45 of epoch:5: loss:0.02067612828558437\n",
      "iter:45 of epoch:6: loss:0.038961625473279825\n",
      "iter:45 of epoch:7: loss:0.0780457547797952\n",
      "iter:45 of epoch:8: loss:0.02856251531158669\n",
      "iter:45 of epoch:9: loss:0.02540154095458392\n",
      "iter:45 of epoch:10: loss:0.03735501557180031\n",
      "iter:45 of epoch:11: loss:0.05810644226083206\n",
      "iter:45 of epoch:12: loss:0.08612490402667972\n",
      "iter:45 of epoch:13: loss:0.3285717397767985\n",
      "iter:45 of epoch:14: loss:0.07646885817018396\n",
      "iter:45 of epoch:15: loss:0.032753691989006836\n",
      "iter:45 of epoch:16: loss:0.059722258912466686\n",
      "iter:45 of epoch:17: loss:0.2508571799246686\n",
      "iter:45 of epoch:18: loss:0.05506723414903257\n",
      "iter:45 of epoch:19: loss:0.0735949307876364\n",
      "iter:45 of epoch:20: loss:0.14746725036117456\n",
      "iter:45 of epoch:21: loss:0.013794862612122575\n",
      "iter:45 of epoch:22: loss:0.043287728219040415\n",
      "iter:45 of epoch:23: loss:0.03354442841476071\n",
      "iter:45 of epoch:24: loss:0.07948925069667448\n",
      "iter:45 of epoch:25: loss:0.04621788704272648\n",
      "iter:45 of epoch:26: loss:0.12941590566222458\n",
      "iter:45 of epoch:27: loss:0.027490279634118358\n",
      "iter:45 of epoch:28: loss:0.13925402177954654\n",
      "iter:45 of epoch:29: loss:0.05021149301847536\n",
      "iter:45 of epoch:30: loss:0.07456270827201268\n",
      "iter:45 of epoch:31: loss:0.0679650153865743\n",
      "iter:45 of epoch:32: loss:0.09586751930032503\n",
      "iter:45 of epoch:33: loss:0.019345686195555534\n",
      "iter:45 of epoch:34: loss:0.051402610003164165\n",
      "iter:45 of epoch:35: loss:0.03331839834773438\n",
      "iter:45 of epoch:36: loss:0.053955052557284985\n",
      "iter:45 of epoch:37: loss:0.19862545369989718\n",
      "iter:45 of epoch:38: loss:0.0832046081614034\n",
      "iter:45 of epoch:39: loss:0.08077030619041628\n",
      "iter:45 of epoch:40: loss:0.019678870488887685\n",
      "iter:46 of epoch:0: loss:0.05280889673906734\n",
      "iter:46 of epoch:1: loss:0.06592687047708927\n",
      "iter:46 of epoch:2: loss:0.04985731514665533\n",
      "iter:46 of epoch:3: loss:0.08162051264733394\n",
      "iter:46 of epoch:4: loss:0.0900849879189172\n",
      "iter:46 of epoch:5: loss:0.042099973570099786\n",
      "iter:46 of epoch:6: loss:0.03980614161343547\n",
      "iter:46 of epoch:7: loss:0.02208746309652669\n",
      "iter:46 of epoch:8: loss:0.04185959203144917\n",
      "iter:46 of epoch:9: loss:0.057826866601400476\n",
      "iter:46 of epoch:10: loss:0.029653154903386963\n",
      "iter:46 of epoch:11: loss:0.10799305799743883\n",
      "iter:46 of epoch:12: loss:0.36330208049145696\n",
      "iter:46 of epoch:13: loss:0.03670938583976553\n",
      "iter:46 of epoch:14: loss:0.16677268720986815\n",
      "iter:46 of epoch:15: loss:0.10920144830097249\n",
      "iter:46 of epoch:16: loss:0.04908627413861361\n",
      "iter:46 of epoch:17: loss:0.07496396953705264\n",
      "iter:46 of epoch:18: loss:0.12874052959241694\n",
      "iter:46 of epoch:19: loss:0.05994336224557047\n",
      "iter:46 of epoch:20: loss:0.06240291571532965\n",
      "iter:46 of epoch:21: loss:0.0409558481746373\n",
      "iter:46 of epoch:22: loss:0.08107006148824633\n",
      "iter:46 of epoch:23: loss:0.06401609830420082\n",
      "iter:46 of epoch:24: loss:0.0674280433314884\n",
      "iter:46 of epoch:25: loss:0.054311997369914676\n",
      "iter:46 of epoch:26: loss:0.07601923275031372\n",
      "iter:46 of epoch:27: loss:0.08463133431456202\n",
      "iter:46 of epoch:28: loss:0.06075146974600258\n",
      "iter:46 of epoch:29: loss:0.05391801292898172\n",
      "iter:46 of epoch:30: loss:0.14751254966756872\n",
      "iter:46 of epoch:31: loss:0.03590765768650308\n",
      "iter:46 of epoch:32: loss:0.057067482144214435\n",
      "iter:46 of epoch:33: loss:0.2376855420391281\n",
      "iter:46 of epoch:34: loss:0.05588467250812053\n",
      "iter:46 of epoch:35: loss:0.02702412681832102\n",
      "iter:46 of epoch:36: loss:0.04664812947568847\n",
      "iter:46 of epoch:37: loss:0.11245250411362813\n",
      "iter:46 of epoch:38: loss:0.1857298100337555\n",
      "iter:46 of epoch:39: loss:0.13852652254039022\n",
      "iter:46 of epoch:40: loss:0.042024003390465726\n",
      "iter:47 of epoch:0: loss:0.06897380900381064\n",
      "iter:47 of epoch:1: loss:0.061182851662684\n",
      "iter:47 of epoch:2: loss:0.10487495319084876\n",
      "iter:47 of epoch:3: loss:0.10652960529512963\n",
      "iter:47 of epoch:4: loss:0.17891118598104833\n",
      "iter:47 of epoch:5: loss:0.033094431244423926\n",
      "iter:47 of epoch:6: loss:0.15041045378839252\n",
      "iter:47 of epoch:7: loss:0.07493868536813228\n",
      "iter:47 of epoch:8: loss:0.19962330808418702\n",
      "iter:47 of epoch:9: loss:0.0201269993087798\n",
      "iter:47 of epoch:10: loss:0.20005635212686732\n",
      "iter:47 of epoch:11: loss:0.09608319120162247\n",
      "iter:47 of epoch:12: loss:0.2702917747698065\n",
      "iter:47 of epoch:13: loss:0.07104807281374889\n",
      "iter:47 of epoch:14: loss:0.04221049211714588\n",
      "iter:47 of epoch:15: loss:0.021016598955050463\n",
      "iter:47 of epoch:16: loss:0.20916925440589224\n",
      "iter:47 of epoch:17: loss:0.022442975953375403\n",
      "iter:47 of epoch:18: loss:0.07154772651688342\n",
      "iter:47 of epoch:19: loss:0.034769461471993134\n",
      "iter:47 of epoch:20: loss:0.07572251221912542\n",
      "iter:47 of epoch:21: loss:0.022442552320908683\n",
      "iter:47 of epoch:22: loss:0.042192003507806414\n",
      "iter:47 of epoch:23: loss:0.026388180599153856\n",
      "iter:47 of epoch:24: loss:0.13567458561326187\n",
      "iter:47 of epoch:25: loss:0.041165901974836105\n",
      "iter:47 of epoch:26: loss:0.07667365848118823\n",
      "iter:47 of epoch:27: loss:0.03409951768208794\n",
      "iter:47 of epoch:28: loss:0.13248763823175444\n",
      "iter:47 of epoch:29: loss:0.025249095111012206\n",
      "iter:47 of epoch:30: loss:0.0851757442891248\n",
      "iter:47 of epoch:31: loss:0.038905782222178845\n",
      "iter:47 of epoch:32: loss:0.1322858362783535\n",
      "iter:47 of epoch:33: loss:0.04825386245958021\n",
      "iter:47 of epoch:34: loss:0.11256577834749035\n",
      "iter:47 of epoch:35: loss:0.046529533245130886\n",
      "iter:47 of epoch:36: loss:0.031233416965729628\n",
      "iter:47 of epoch:37: loss:0.04624071382842281\n",
      "iter:47 of epoch:38: loss:0.08279542378746733\n",
      "iter:47 of epoch:39: loss:0.0370337835904922\n",
      "iter:47 of epoch:40: loss:0.0176420459397707\n",
      "iter:48 of epoch:0: loss:0.03813030314908371\n",
      "iter:48 of epoch:1: loss:0.1390650421985129\n",
      "iter:48 of epoch:2: loss:0.03573916353007659\n",
      "iter:48 of epoch:3: loss:0.062454361879534\n",
      "iter:48 of epoch:4: loss:0.30435966781485596\n",
      "iter:48 of epoch:5: loss:0.033806937068990094\n",
      "iter:48 of epoch:6: loss:0.16361482458604412\n",
      "iter:48 of epoch:7: loss:0.040013074852702724\n",
      "iter:48 of epoch:8: loss:0.05821165954282448\n",
      "iter:48 of epoch:9: loss:0.05752053643940104\n",
      "iter:48 of epoch:10: loss:0.04651316801080373\n",
      "iter:48 of epoch:11: loss:0.2937861790966482\n",
      "iter:48 of epoch:12: loss:0.011853840879595942\n",
      "iter:48 of epoch:13: loss:0.07236056796915113\n",
      "iter:48 of epoch:14: loss:0.04165004293511445\n",
      "iter:48 of epoch:15: loss:0.1414647721483492\n",
      "iter:48 of epoch:16: loss:0.01758356864096334\n",
      "iter:48 of epoch:17: loss:0.05080435722846266\n",
      "iter:48 of epoch:18: loss:0.04382641077297297\n",
      "iter:48 of epoch:19: loss:0.029896149506312964\n",
      "iter:48 of epoch:20: loss:0.14600124170720818\n",
      "iter:48 of epoch:21: loss:0.07880303268009894\n",
      "iter:48 of epoch:22: loss:0.04716508339938898\n",
      "iter:48 of epoch:23: loss:0.06747311676656645\n",
      "iter:48 of epoch:24: loss:0.0354519394327635\n",
      "iter:48 of epoch:25: loss:0.03751116107433982\n",
      "iter:48 of epoch:26: loss:0.03403649536512756\n",
      "iter:48 of epoch:27: loss:0.074698952013396\n",
      "iter:48 of epoch:28: loss:0.1159463141455331\n",
      "iter:48 of epoch:29: loss:0.13533504242676558\n",
      "iter:48 of epoch:30: loss:0.05829163382403676\n",
      "iter:48 of epoch:31: loss:0.06995952942942026\n",
      "iter:48 of epoch:32: loss:0.23228146008449757\n",
      "iter:48 of epoch:33: loss:0.03191106844329536\n",
      "iter:48 of epoch:34: loss:0.030616434885132205\n",
      "iter:48 of epoch:35: loss:0.13790165173868818\n",
      "iter:48 of epoch:36: loss:0.02149778467468345\n",
      "iter:48 of epoch:37: loss:0.06643698225597736\n",
      "iter:48 of epoch:38: loss:0.1115135428466943\n",
      "iter:48 of epoch:39: loss:0.02824974942878189\n",
      "iter:48 of epoch:40: loss:0.023790108212707634\n",
      "iter:49 of epoch:0: loss:0.16994922522592634\n",
      "iter:49 of epoch:1: loss:0.1572294722237033\n",
      "iter:49 of epoch:2: loss:0.03837791809859806\n",
      "iter:49 of epoch:3: loss:0.23386804234251035\n",
      "iter:49 of epoch:4: loss:0.049792566047943444\n",
      "iter:49 of epoch:5: loss:0.04894191976173811\n",
      "iter:49 of epoch:6: loss:0.044637661611286884\n",
      "iter:49 of epoch:7: loss:0.12896943559378407\n",
      "iter:49 of epoch:8: loss:0.05051447032321331\n",
      "iter:49 of epoch:9: loss:0.037944155092202295\n",
      "iter:49 of epoch:10: loss:0.07479621178189713\n",
      "iter:49 of epoch:11: loss:0.0634049765023152\n",
      "iter:49 of epoch:12: loss:0.04774355480932192\n",
      "iter:49 of epoch:13: loss:0.05556162723780406\n",
      "iter:49 of epoch:14: loss:0.0186893899873459\n",
      "iter:49 of epoch:15: loss:0.08577405999392554\n",
      "iter:49 of epoch:16: loss:0.07476495020471234\n",
      "iter:49 of epoch:17: loss:0.028542406154503104\n",
      "iter:49 of epoch:18: loss:0.030524526969353753\n",
      "iter:49 of epoch:19: loss:0.049450652650009766\n",
      "iter:49 of epoch:20: loss:0.03841957750992009\n",
      "iter:49 of epoch:21: loss:0.01944123536040394\n",
      "iter:49 of epoch:22: loss:0.055657233247098224\n",
      "iter:49 of epoch:23: loss:0.1820351559761687\n",
      "iter:49 of epoch:24: loss:0.03683944365513255\n",
      "iter:49 of epoch:25: loss:0.1446690264075386\n",
      "iter:49 of epoch:26: loss:0.13128035823917122\n",
      "iter:49 of epoch:27: loss:0.09611546517950867\n",
      "iter:49 of epoch:28: loss:0.022273160690637287\n",
      "iter:49 of epoch:29: loss:0.06234088950211145\n",
      "iter:49 of epoch:30: loss:0.05160902241639058\n",
      "iter:49 of epoch:31: loss:0.027763490392081612\n",
      "iter:49 of epoch:32: loss:0.03664584700000822\n",
      "iter:49 of epoch:33: loss:0.2965087808820033\n",
      "iter:49 of epoch:34: loss:0.1024681993169809\n",
      "iter:49 of epoch:35: loss:0.15034803487614423\n",
      "iter:49 of epoch:36: loss:0.03216469236906826\n",
      "iter:49 of epoch:37: loss:0.03331077625169932\n",
      "iter:49 of epoch:38: loss:0.06218281679638464\n",
      "iter:49 of epoch:39: loss:0.1255350713493334\n",
      "iter:49 of epoch:40: loss:0.018991150205874625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (1)导入数据集\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "'''\n",
    "data = np.fromfile('data/data95203/housing.data',dtype=np.float,sep=' ')\n",
    "#data是一维的数组，0-13是一个样本\n",
    "data = data.reshape(data.shape[0]//14,14)\n",
    "#这里的data是(506,14)，即506个样本\n",
    "\n",
    "# (2)数据集划分\n",
    "ratio = 0.8\n",
    "offset = int(0.8*data.shape[0])\n",
    "train_data = data[0:offset]\n",
    "\n",
    "# (3)使用train_data的数据对所有数据进行归一化处理\n",
    "maximums,minimums,avgs = train_data.max(axis=0),train_data.min(axis=0),train_data.sum(axis=0)/train_data.shape[0]\n",
    "for i in range(14):\n",
    "    data[:,i] = (data[:,i]-minimums[i])/(maximums[i]-minimums[i])\n",
    "'''\n",
    "#========================================================================================================================\n",
    "# (4)封装成load_data函数\n",
    "def load_data():\n",
    "    feature_num = 14 #13个自变量拟合一个因变量\n",
    "    data = np.fromfile('data/data95203/housing.data',dtype=np.float,sep=' ')\n",
    "    data = data.reshape(data.shape[0]//feature_num,feature_num)\n",
    "    ratio = 0.8\n",
    "    offset = int(0.8*data.shape[0])\n",
    "    train_data = data[0:offset]\n",
    "    maximums,minimums,avgs = train_data.max(axis=0),train_data.min(axis=0),train_data.sum(axis=0)/train_data.shape[0]\n",
    "    for i in range(14):\n",
    "        data[:,i] = (data[:,i]-minimums[i])/(maximums[i]-minimums[i])\n",
    "    train_data = data[:offset]\n",
    "    test_data = data[offset:]\n",
    "    return train_data,test_data\n",
    "#=======================================================================================================================\n",
    "\n",
    "# (5)模型设计\n",
    "class LinearNetwork(object):\n",
    "    def __init__(self,num_of_weights):\n",
    "        #随机初始化权重\n",
    "        np.random.seed(2022)\n",
    "        self.w = np.random.randn(num_of_weights,1)\n",
    "        self.b = 0\n",
    "    def forward(self,x):\n",
    "        z = np.dot(x,self.w) + self.b\n",
    "        return z\n",
    "    def loss(self,z,y):#残差平方损失函数\n",
    "        #使用平方误差而不是绝对值有两个好处\n",
    "            #1,曲线的最低点是可导的\n",
    "            #2,越接近最低点，坡度逐渐放缓，有助于通过当前的梯度值判断接近最低点的程度\n",
    "        error = z.squeeze() - y.squeeze()\n",
    "        cost = np.sum(error*error)/404\n",
    "\n",
    "        return cost\n",
    "    def gradient(self,x,y):\n",
    "        z = self.forward(x)\n",
    "        gradient_w_tmp = np.dot((z.squeeze()-y),x)\n",
    "        #print(gradient_w_tmp.shape)\n",
    "        gradient_w = gradient_w_tmp/404\n",
    "        gradient_b = np.mean(z-y)\n",
    "        return gradient_w,gradient_b\n",
    "    def update(self,gradient_w,gradient_b,lr):\n",
    "        self.w = self.w.squeeze() - lr*gradient_w.squeeze()\n",
    "        self.b = self.b - lr*gradient_b\n",
    "\n",
    "    def train(self,x,y,iterations=100,lr=0.01):\n",
    "        losses = []\n",
    "        for i in range(iterations):\n",
    "            z = self.forward(x)\n",
    "            L = self.loss(z,y)\n",
    "            #print(z.shape,L.shape)\n",
    "\n",
    "            gradient_w,gradient_b = self.gradient(x,y)\n",
    "            \n",
    "            self.update(gradient_w,gradient_b,lr=0.01)\n",
    "            losses.append(L)\n",
    "            if (i+1)%10 == 0:\n",
    "                print('iter:{} loss{}'.format(i,L))\n",
    "        return losses\n",
    "\n",
    "# (7)开始训练,训练iterations轮，即遍历数据集iterations次\n",
    "'''\n",
    "train_data,test_data = load_data()\n",
    "x = train_data[:,:-1]\n",
    "y = train_data[:,-1]\n",
    "\n",
    "net = LinearNetwork(13)\n",
    "num_iterations = 2000\n",
    "losses = net.train(x,y,iterations=num_iterations,lr=0.01)\n",
    "\n",
    "plot_x = np.arange(num_iterations)\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x,plot_y)\n",
    "plt.show()       \n",
    "'''\n",
    "# (8) SGD(使用mini-batch方法)\n",
    "#在上述程序中，每次损失函数和梯度计算都是基于数据集中的全部数据，但是对于大容量样本的数据集，每次参数更新都使用全部数据会使得计算效率非常低。\n",
    "#mini_batch:每次迭代抽取出的一批数据称为一个mini-batch\n",
    "def load_data_SGD():\n",
    "\n",
    "    batch_size = 10\n",
    "\n",
    "    train_data,test_data = load_data()\n",
    "    np.random.shuffle(train_data)\n",
    "\n",
    "    mini_batches = [train_data[k:k+batch_size] for k in range(0,len(train_data),batch_size)]\n",
    "    return mini_batches\n",
    "\n",
    "class LinearNetwork_SGD(object):\n",
    "    def __init__(self,num_of_weights):\n",
    "        #随机初始化权重\n",
    "        np.random.seed(2022)\n",
    "        self.w = np.random.randn(num_of_weights,1)\n",
    "        self.b = 0\n",
    "    def forward(self,x):\n",
    "        z = np.dot(x,self.w) + self.b\n",
    "        return z\n",
    "    def loss(self,z,y):#残差平方损失函数\n",
    "        #使用平方误差而不是绝对值有两个好处\n",
    "            #1,曲线的最低点是可导的\n",
    "            #2,越接近最低点，坡度逐渐放缓，有助于通过当前的梯度值判断接近最低点的程度\n",
    "        error = z.squeeze() - y.squeeze()\n",
    "        cost = np.sum(error*error)/error.shape[0]\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def gradient(self,x,y):\n",
    "        z = self.forward(x)\n",
    "        gradient_w_tmp = np.dot((z.squeeze()-y),x)\n",
    "        #print(gradient_w_tmp.shape)\n",
    "        gradient_w = gradient_w_tmp/x.shape[0]\n",
    "        gradient_b = np.mean(z-y)\n",
    "        return gradient_w,gradient_b\n",
    "    def update(self,gradient_w,gradient_b,lr):\n",
    "        self.w = self.w.squeeze() - lr*gradient_w.squeeze()\n",
    "        self.b = self.b - lr*gradient_b\n",
    "\n",
    "    def train(self,num_epochs,batch_size,lr):\n",
    "        n = 404\n",
    "        losses = []\n",
    "        for epoch in range(num_epochs):\n",
    "            mini_batches = load_data_SGD()\n",
    "            for iter,mini_batch in enumerate(mini_batches):\n",
    "                x = mini_batch[:,:-1]\n",
    "                y = mini_batch[:,-1]\n",
    "                z = self.forward(x)\n",
    "                loss = self.loss(z,y)\n",
    "                gradient_w,gradient_b = self.gradient(x,y)\n",
    "                self.update(gradient_w,gradient_b,lr=0.01)\n",
    "                losses.append(loss)\n",
    "                print('iter:{} of epoch:{}: loss:{}'.format(epoch,iter,loss))\n",
    "        return losses\n",
    "    \n",
    "net = LinearNetwork_SGD(13)\n",
    "losses = net.train(50,10,0.01)#40*50=2000\n",
    "plot_x = np.arange(len(losses))\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x,plot_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结:\n",
    "### 在一个epoch下，迭代iter个mini_batch，算是遍历了一次数据集。\n",
    "### iter = len(train_data) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度学习框架\n",
    "\n",
    "Paddle框架在2016年正式开源\n",
    "\n",
    "\n",
    "|思考过程|工作内容|个性化部分|通用部分(平台框架负责)|\n",
    "|--|--|--|--|\n",
    "|Step1: 模型设计|假设一种网络|设计网络结构|网络模块的实现(Layer,Variable)|\n",
    "||设计评价函数|指定评价函数|Loss函数实现|\n",
    "||寻找优化寻解方法|指定优化器|优化算法实现|\n",
    "|Step2：准备数据集|准备训练数据|提供数据格式、位置、模型接受数据方式|为模型批量送入数据|\n",
    "|Step3: 训练配置|训练配置|单机和多机配置|单机到多机的转换(transpile to run)|\n",
    "|Step4: 应用部署|部署应用和测试环境|确定保存和加载模型的环节点|save_inference_model,load_inference_model|\n",
    "|Step5: 模型评估|评估模型效果|指定评估指标|模型实现，Visual DL|\n",
    "|Step6：基本过程|全流程串起来|主程序| |\n",
    "\n",
    "- 飞桨集成 深度学习核心框架、基础模型库、端到端开发套件、工具组件、服务平台 于一体\n",
    "\n",
    "![title](https://img2.baidu.com/it/u=68423699,3683953266&fm=253&fmt=auto&app=138&f=PNG?w=1071&h=500) \n",
    "\n",
    "\n",
    "| **框架工具** |简介|\n",
    "|--|--|\n",
    "|Paddle Inference|飞桨原生推理库,用于服务器端模型部署|\n",
    "|Paddle Serving|飞桨服务化部署框架，用于云端服务器部署框架，可以将模型作为单独的Wed服务|\n",
    "|Paddle Lite|飞桨轻量化推理模型，用于移动端部署。**部署到EdgeBoard就是用此方法**|\n",
    "|Paddle.js|使用JS部署模型，在浏览器、小程序等环境部署模型|\n",
    "|Paddle Slim|模型压缩工具|\n",
    "|X2Paddle|飞桨模型转化工具，将其他框架模型转化为Paddle模型|\n",
    "|AutoDL|飞桨自动搜索网络结构、超参数的工具|\n",
    "|VisualDL|飞桨可视化分析工具|\n",
    "|PaddleFL|飞桨联邦学习框架，让用户运用外部伙伴的服务器资源训练，但不泄露业务数据|\n",
    "|PaddleX|飞桨全流程开发工具|\n",
    "\n",
    "- Paddle Hub的使用最简单，模型库的可定制性最强，覆盖场景最简单\n",
    "|Paddle Hub|预训练模型库、迁移学习组件。是进行FOC(模型验证的首选工具)| \n",
    "|--|--|\n",
    "\n",
    "|**开发套件**|针对具体的应用场景提供的全套研发工具|\n",
    "|--|--|\n",
    "|PaddleClas|飞桨图像分类套件|\n",
    "|PaddleDetection|飞桨目标检测套件|\n",
    "|PaddleSeg|飞桨图像分割套件|\n",
    "|PLSC|海量分类套件|\n",
    "|Parakeet|飞桨语音合成套件|\n",
    "\n",
    "> 移动端\n",
    ">> ARM-CPU、Mali-CPU、Adreno-GPU、Metal GPU、FPGA、华为NPU\n",
    "\n",
    "> 系统\n",
    ">> Linux、MAC、Windows、Android、ios\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2,使用飞桨重写波士顿房价预测\n",
    "> step1 数据处理  \n",
    " step2 模型设计    \n",
    " step3 训练配置  \n",
    " step4 训练过程  \n",
    " step5 模型保存\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-09T01:44:09.076892Z",
     "iopub.status.busy": "2022-01-09T01:44:09.076174Z",
     "iopub.status.idle": "2022-01-09T01:44:10.334552Z",
     "shell.execute_reply": "2022-01-09T01:44:10.333353Z",
     "shell.execute_reply.started": "2022-01-09T01:44:09.076861Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "import paddle.fluid.dygraph as dygraph\n",
    "from paddle.fluid.dygraph import Linear\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# 1,数据处理\n",
    "def load_data_SGD():\n",
    "\n",
    "    batch_size = 10\n",
    "\n",
    "    train_data,test_data = load_data()\n",
    "    np.random.shuffle(train_data)\n",
    "\n",
    "    mini_batches = [train_data[k:k+batch_size] for k in range(0,len(train_data),batch_size)]\n",
    "    return mini_batches\n",
    "\n",
    "# 2,模型设计\n",
    "class Regressor(fluid.dygraph.Layer):\n",
    "    def __init__(self):\n",
    "        super(Regressor,self).__init__()\n",
    "        self.fc = Linear(input_dim=13,output_dim=1,act=None)\n",
    "    def forward(self,x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "# 3,训练配置\n",
    "# 训练配置包括四步: 指定运行的机器资源 ——> 声明模型实例 ——> 加载训练和测试数据 ——> 设置优化算法和学习率\n",
    "with fluid.dygraph.guard():\n",
    "#当with后面的代码块全部被执行完之后，将调用前面返回对象的 __exit__()方法\n",
    "    model = Regressor()\n",
    "    model.train() #开启训练模式\n",
    "    train_data,test_data = load_data()\n",
    "    opt = fluid.optimizer.SGD(learning_rate=0.01,parameter_list=model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型实例有两种形态:model.train()和model.eval()即训练和测试，训练正向反向都进行，测试只进行正向传播。且部分算子，如Drop out，BN 在这两种模式下执行方式不同。\n",
    "- 模型实例化，读取数据，定义优化器 全部都在动态图上下文环境中进行\n",
    "\n",
    "```python\n",
    "'''\n",
    "numpy底部使用C语言编写，运行非常快\n",
    "'''\n",
    "x_tenosr = dygraph.to_variable(x_numpy) #np转tensor\n",
    "x_numpy = x_tensor.numpy() #tensor转numpy\n",
    "```\n",
    "##### ndarray切片\n",
    "```python\n",
    "a = np.arange(30)\n",
    "b = a[4:7]\n",
    "b[0] = 0\n",
    "#此时a[4]也变成了0\n",
    "'''\n",
    "数组切片产生的新数组还是会指向原数组的内存区域，视图上任何的修改也会改变原数组\n",
    "'''\n",
    "a = np.arange(30)\n",
    "b = np.copy(a[4:7])\n",
    "b[0] = 0\n",
    "```\n",
    "##### np.random()\n",
    "```python\n",
    "np.random.seed(2022)\n",
    "#生成符合均匀分布的数据\n",
    "a = np.random.uniform(low=-1.0,high=1.0,size=(2,2))\n",
    "#生成符合标准正态分布的数据\n",
    "a = np.random.randn(3,3)\n",
    "#随机打乱\n",
    "np.random.shuffle(a)]\n",
    "#随机选取元素\n",
    "b = np.random.choice(a,size=5)\n",
    "```\n",
    "\n",
    "- 空格和回车都属于空白字符，即sep = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-09T01:44:10.336177Z",
     "iopub.status.busy": "2022-01-09T01:44:10.335837Z",
     "iopub.status.idle": "2022-01-09T01:44:13.299091Z",
     "shell.execute_reply": "2022-01-09T01:44:13.297960Z",
     "shell.execute_reply.started": "2022-01-09T01:44:10.336146Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0 of epoch:0: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [1.63380551])\n",
      "iter:10 of epoch:0: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.39020351])\n",
      "iter:20 of epoch:0: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.19055939])\n",
      "iter:30 of epoch:0: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.14655244])\n",
      "iter:40 of epoch:0: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.43570739])\n",
      "iter:0 of epoch:1: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.14061514])\n",
      "iter:10 of epoch:1: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.14477594])\n",
      "iter:20 of epoch:1: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.12468851])\n",
      "iter:30 of epoch:1: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08732101])\n",
      "iter:40 of epoch:1: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.11454653])\n",
      "iter:0 of epoch:2: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.12681404])\n",
      "iter:10 of epoch:2: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.15077144])\n",
      "iter:20 of epoch:2: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.15796894])\n",
      "iter:30 of epoch:2: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.11695134])\n",
      "iter:40 of epoch:2: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.25943711])\n",
      "iter:0 of epoch:3: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.20815282])\n",
      "iter:10 of epoch:3: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08521003])\n",
      "iter:20 of epoch:3: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.10046854])\n",
      "iter:30 of epoch:3: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05480967])\n",
      "iter:40 of epoch:3: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04061206])\n",
      "iter:0 of epoch:4: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07227321])\n",
      "iter:10 of epoch:4: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.10618376])\n",
      "iter:20 of epoch:4: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.13522269])\n",
      "iter:30 of epoch:4: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06546989])\n",
      "iter:40 of epoch:4: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06183465])\n",
      "iter:0 of epoch:5: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06761892])\n",
      "iter:10 of epoch:5: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.10209254])\n",
      "iter:20 of epoch:5: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05935880])\n",
      "iter:30 of epoch:5: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.10611027])\n",
      "iter:40 of epoch:5: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.19250357])\n",
      "iter:0 of epoch:6: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07276597])\n",
      "iter:10 of epoch:6: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08963875])\n",
      "iter:20 of epoch:6: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08658004])\n",
      "iter:30 of epoch:6: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.10223637])\n",
      "iter:40 of epoch:6: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.11844830])\n",
      "iter:0 of epoch:7: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05302821])\n",
      "iter:10 of epoch:7: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06322239])\n",
      "iter:20 of epoch:7: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07369843])\n",
      "iter:30 of epoch:7: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08697839])\n",
      "iter:40 of epoch:7: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.11704053])\n",
      "iter:0 of epoch:8: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03915300])\n",
      "iter:10 of epoch:8: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04272766])\n",
      "iter:20 of epoch:8: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04155793])\n",
      "iter:30 of epoch:8: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.13166869])\n",
      "iter:40 of epoch:8: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01106207])\n",
      "iter:0 of epoch:9: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07237095])\n",
      "iter:10 of epoch:9: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06654102])\n",
      "iter:20 of epoch:9: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07689479])\n",
      "iter:30 of epoch:9: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04812139])\n",
      "iter:40 of epoch:9: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05899581])\n",
      "iter:0 of epoch:10: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07926694])\n",
      "iter:10 of epoch:10: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04590520])\n",
      "iter:20 of epoch:10: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06843109])\n",
      "iter:30 of epoch:10: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04418834])\n",
      "iter:40 of epoch:10: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03791033])\n",
      "iter:0 of epoch:11: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05734974])\n",
      "iter:10 of epoch:11: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06110458])\n",
      "iter:20 of epoch:11: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03919851])\n",
      "iter:30 of epoch:11: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04016181])\n",
      "iter:40 of epoch:11: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.14300314])\n",
      "iter:0 of epoch:12: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03991600])\n",
      "iter:10 of epoch:12: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07854019])\n",
      "iter:20 of epoch:12: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.13296121])\n",
      "iter:30 of epoch:12: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.09235016])\n",
      "iter:40 of epoch:12: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07388595])\n",
      "iter:0 of epoch:13: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04014702])\n",
      "iter:10 of epoch:13: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06314402])\n",
      "iter:20 of epoch:13: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05544432])\n",
      "iter:30 of epoch:13: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02966726])\n",
      "iter:40 of epoch:13: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05218414])\n",
      "iter:0 of epoch:14: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03254775])\n",
      "iter:10 of epoch:14: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04113538])\n",
      "iter:20 of epoch:14: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05807275])\n",
      "iter:30 of epoch:14: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02754525])\n",
      "iter:40 of epoch:14: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02440415])\n",
      "iter:0 of epoch:15: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.12140551])\n",
      "iter:10 of epoch:15: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07993874])\n",
      "iter:20 of epoch:15: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02644852])\n",
      "iter:30 of epoch:15: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04209729])\n",
      "iter:40 of epoch:15: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08966990])\n",
      "iter:0 of epoch:16: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04376204])\n",
      "iter:10 of epoch:16: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07017486])\n",
      "iter:20 of epoch:16: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06356666])\n",
      "iter:30 of epoch:16: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03818645])\n",
      "iter:40 of epoch:16: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.13949198])\n",
      "iter:0 of epoch:17: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07520915])\n",
      "iter:10 of epoch:17: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06013320])\n",
      "iter:20 of epoch:17: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.10416544])\n",
      "iter:30 of epoch:17: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01712498])\n",
      "iter:40 of epoch:17: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.00644648])\n",
      "iter:0 of epoch:18: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.10137332])\n",
      "iter:10 of epoch:18: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06274650])\n",
      "iter:20 of epoch:18: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04742734])\n",
      "iter:30 of epoch:18: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.10558204])\n",
      "iter:40 of epoch:18: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01658664])\n",
      "iter:0 of epoch:19: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07352450])\n",
      "iter:10 of epoch:19: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07312693])\n",
      "iter:20 of epoch:19: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02492299])\n",
      "iter:30 of epoch:19: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.09477223])\n",
      "iter:40 of epoch:19: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02012893])\n",
      "iter:0 of epoch:20: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03955164])\n",
      "iter:10 of epoch:20: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05986277])\n",
      "iter:20 of epoch:20: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05895419])\n",
      "iter:30 of epoch:20: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05309528])\n",
      "iter:40 of epoch:20: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.00676093])\n",
      "iter:0 of epoch:21: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05650258])\n",
      "iter:10 of epoch:21: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02115447])\n",
      "iter:20 of epoch:21: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05375316])\n",
      "iter:30 of epoch:21: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05119287])\n",
      "iter:40 of epoch:21: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02545832])\n",
      "iter:0 of epoch:22: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05320583])\n",
      "iter:10 of epoch:22: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.09192239])\n",
      "iter:20 of epoch:22: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02397267])\n",
      "iter:30 of epoch:22: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04847612])\n",
      "iter:40 of epoch:22: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02857965])\n",
      "iter:0 of epoch:23: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05378420])\n",
      "iter:10 of epoch:23: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03123168])\n",
      "iter:20 of epoch:23: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06956920])\n",
      "iter:30 of epoch:23: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05780530])\n",
      "iter:40 of epoch:23: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08764263])\n",
      "iter:0 of epoch:24: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01865563])\n",
      "iter:10 of epoch:24: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01721127])\n",
      "iter:20 of epoch:24: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04975538])\n",
      "iter:30 of epoch:24: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06447214])\n",
      "iter:40 of epoch:24: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02241880])\n",
      "iter:0 of epoch:25: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05085234])\n",
      "iter:10 of epoch:25: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05463061])\n",
      "iter:20 of epoch:25: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05849585])\n",
      "iter:30 of epoch:25: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08149962])\n",
      "iter:40 of epoch:25: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03100258])\n",
      "iter:0 of epoch:26: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02691861])\n",
      "iter:10 of epoch:26: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01879255])\n",
      "iter:20 of epoch:26: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03089161])\n",
      "iter:30 of epoch:26: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06043412])\n",
      "iter:40 of epoch:26: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.11593290])\n",
      "iter:0 of epoch:27: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05368845])\n",
      "iter:10 of epoch:27: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01352010])\n",
      "iter:20 of epoch:27: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06386550])\n",
      "iter:30 of epoch:27: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06916282])\n",
      "iter:40 of epoch:27: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03699293])\n",
      "iter:0 of epoch:28: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04545297])\n",
      "iter:10 of epoch:28: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04942027])\n",
      "iter:20 of epoch:28: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04288057])\n",
      "iter:30 of epoch:28: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05207721])\n",
      "iter:40 of epoch:28: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07506622])\n",
      "iter:0 of epoch:29: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05413927])\n",
      "iter:10 of epoch:29: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03671140])\n",
      "iter:20 of epoch:29: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04024636])\n",
      "iter:30 of epoch:29: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02632923])\n",
      "iter:40 of epoch:29: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02977747])\n",
      "iter:0 of epoch:30: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05404119])\n",
      "iter:10 of epoch:30: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03134662])\n",
      "iter:20 of epoch:30: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03044385])\n",
      "iter:30 of epoch:30: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.09390391])\n",
      "iter:40 of epoch:30: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01971523])\n",
      "iter:0 of epoch:31: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04014525])\n",
      "iter:10 of epoch:31: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05547461])\n",
      "iter:20 of epoch:31: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04837176])\n",
      "iter:30 of epoch:31: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03045499])\n",
      "iter:40 of epoch:31: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.00523425])\n",
      "iter:0 of epoch:32: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05168068])\n",
      "iter:10 of epoch:32: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06944611])\n",
      "iter:20 of epoch:32: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05451468])\n",
      "iter:30 of epoch:32: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01979559])\n",
      "iter:40 of epoch:32: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02197617])\n",
      "iter:0 of epoch:33: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06094367])\n",
      "iter:10 of epoch:33: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04197887])\n",
      "iter:20 of epoch:33: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05342622])\n",
      "iter:30 of epoch:33: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08500509])\n",
      "iter:40 of epoch:33: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02190307])\n",
      "iter:0 of epoch:34: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07893214])\n",
      "iter:10 of epoch:34: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02529821])\n",
      "iter:20 of epoch:34: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02633811])\n",
      "iter:30 of epoch:34: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08980115])\n",
      "iter:40 of epoch:34: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01982576])\n",
      "iter:0 of epoch:35: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01721510])\n",
      "iter:10 of epoch:35: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01524691])\n",
      "iter:20 of epoch:35: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03637692])\n",
      "iter:30 of epoch:35: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06928935])\n",
      "iter:40 of epoch:35: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02857176])\n",
      "iter:0 of epoch:36: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.10732698])\n",
      "iter:10 of epoch:36: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08731288])\n",
      "iter:20 of epoch:36: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06124408])\n",
      "iter:30 of epoch:36: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04110907])\n",
      "iter:40 of epoch:36: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06271549])\n",
      "iter:0 of epoch:37: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03035627])\n",
      "iter:10 of epoch:37: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01367471])\n",
      "iter:20 of epoch:37: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07273096])\n",
      "iter:30 of epoch:37: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05218215])\n",
      "iter:40 of epoch:37: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02487474])\n",
      "iter:0 of epoch:38: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07417392])\n",
      "iter:10 of epoch:38: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03139887])\n",
      "iter:20 of epoch:38: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02132836])\n",
      "iter:30 of epoch:38: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03853517])\n",
      "iter:40 of epoch:38: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.00485259])\n",
      "iter:0 of epoch:39: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03093584])\n",
      "iter:10 of epoch:39: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06330665])\n",
      "iter:20 of epoch:39: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02678319])\n",
      "iter:30 of epoch:39: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06018263])\n",
      "iter:40 of epoch:39: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.16738465])\n",
      "iter:0 of epoch:40: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02907608])\n",
      "iter:10 of epoch:40: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01336472])\n",
      "iter:20 of epoch:40: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05779507])\n",
      "iter:30 of epoch:40: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03367760])\n",
      "iter:40 of epoch:40: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.09968150])\n",
      "iter:0 of epoch:41: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03458226])\n",
      "iter:10 of epoch:41: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04891926])\n",
      "iter:20 of epoch:41: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08742718])\n",
      "iter:30 of epoch:41: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07216045])\n",
      "iter:40 of epoch:41: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06211363])\n",
      "iter:0 of epoch:42: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06857383])\n",
      "iter:10 of epoch:42: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02721443])\n",
      "iter:20 of epoch:42: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02043377])\n",
      "iter:30 of epoch:42: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02607883])\n",
      "iter:40 of epoch:42: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03177760])\n",
      "iter:0 of epoch:43: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04480005])\n",
      "iter:10 of epoch:43: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01119130])\n",
      "iter:20 of epoch:43: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05736536])\n",
      "iter:30 of epoch:43: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04821337])\n",
      "iter:40 of epoch:43: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02469990])\n",
      "iter:0 of epoch:44: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01282385])\n",
      "iter:10 of epoch:44: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08321032])\n",
      "iter:20 of epoch:44: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07254986])\n",
      "iter:30 of epoch:44: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04520146])\n",
      "iter:40 of epoch:44: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07382008])\n",
      "iter:0 of epoch:45: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02571542])\n",
      "iter:10 of epoch:45: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05305893])\n",
      "iter:20 of epoch:45: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07806136])\n",
      "iter:30 of epoch:45: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04761418])\n",
      "iter:40 of epoch:45: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05751448])\n",
      "iter:0 of epoch:46: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05448651])\n",
      "iter:10 of epoch:46: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01724779])\n",
      "iter:20 of epoch:46: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06653744])\n",
      "iter:30 of epoch:46: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05706406])\n",
      "iter:40 of epoch:46: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02511979])\n",
      "iter:0 of epoch:47: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03670941])\n",
      "iter:10 of epoch:47: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01621209])\n",
      "iter:20 of epoch:47: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02565864])\n",
      "iter:30 of epoch:47: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01406910])\n",
      "iter:40 of epoch:47: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01935430])\n",
      "iter:0 of epoch:48: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03295007])\n",
      "iter:10 of epoch:48: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02464882])\n",
      "iter:20 of epoch:48: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06497427])\n",
      "iter:30 of epoch:48: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05997868])\n",
      "iter:40 of epoch:48: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01558626])\n",
      "iter:0 of epoch:49: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05154794])\n",
      "iter:10 of epoch:49: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02991389])\n",
      "iter:20 of epoch:49: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01779128])\n",
      "iter:30 of epoch:49: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05003522])\n",
      "iter:40 of epoch:49: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01267754])\n",
      "iter:0 of epoch:50: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05106393])\n",
      "iter:10 of epoch:50: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04919224])\n",
      "iter:20 of epoch:50: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05540570])\n",
      "iter:30 of epoch:50: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.00736290])\n",
      "iter:40 of epoch:50: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.00116468])\n",
      "iter:0 of epoch:51: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02859631])\n",
      "iter:10 of epoch:51: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03558341])\n",
      "iter:20 of epoch:51: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02173917])\n",
      "iter:30 of epoch:51: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02736852])\n",
      "iter:40 of epoch:51: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03309105])\n",
      "iter:0 of epoch:52: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02179521])\n",
      "iter:10 of epoch:52: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03395461])\n",
      "iter:20 of epoch:52: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06185508])\n",
      "iter:30 of epoch:52: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07521977])\n",
      "iter:40 of epoch:52: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03355588])\n",
      "iter:0 of epoch:53: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01277881])\n",
      "iter:10 of epoch:53: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02017400])\n",
      "iter:20 of epoch:53: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01253044])\n",
      "iter:30 of epoch:53: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02508738])\n",
      "iter:40 of epoch:53: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01400023])\n",
      "iter:0 of epoch:54: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03200517])\n",
      "iter:10 of epoch:54: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06217665])\n",
      "iter:20 of epoch:54: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05180132])\n",
      "iter:30 of epoch:54: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03516475])\n",
      "iter:40 of epoch:54: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05829805])\n",
      "iter:0 of epoch:55: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01543873])\n",
      "iter:10 of epoch:55: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01421259])\n",
      "iter:20 of epoch:55: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02610455])\n",
      "iter:30 of epoch:55: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02959596])\n",
      "iter:40 of epoch:55: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.00788049])\n",
      "iter:0 of epoch:56: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04562148])\n",
      "iter:10 of epoch:56: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04696507])\n",
      "iter:20 of epoch:56: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06228177])\n",
      "iter:30 of epoch:56: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03259045])\n",
      "iter:40 of epoch:56: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01648169])\n",
      "iter:0 of epoch:57: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05416009])\n",
      "iter:10 of epoch:57: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06301911])\n",
      "iter:20 of epoch:57: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08582919])\n",
      "iter:30 of epoch:57: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01774799])\n",
      "iter:40 of epoch:57: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.10019408])\n",
      "iter:0 of epoch:58: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08268337])\n",
      "iter:10 of epoch:58: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03344910])\n",
      "iter:20 of epoch:58: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06438300])\n",
      "iter:30 of epoch:58: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02681023])\n",
      "iter:40 of epoch:58: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05267978])\n",
      "iter:0 of epoch:59: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05891493])\n",
      "iter:10 of epoch:59: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03738212])\n",
      "iter:20 of epoch:59: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03834600])\n",
      "iter:30 of epoch:59: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01774739])\n",
      "iter:40 of epoch:59: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05151717])\n",
      "iter:0 of epoch:60: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08660552])\n",
      "iter:10 of epoch:60: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06726542])\n",
      "iter:20 of epoch:60: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01875992])\n",
      "iter:30 of epoch:60: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05751739])\n",
      "iter:40 of epoch:60: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04577853])\n",
      "iter:0 of epoch:61: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04769249])\n",
      "iter:10 of epoch:61: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03313119])\n",
      "iter:20 of epoch:61: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06008039])\n",
      "iter:30 of epoch:61: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04809930])\n",
      "iter:40 of epoch:61: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06438313])\n",
      "iter:0 of epoch:62: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07826935])\n",
      "iter:10 of epoch:62: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02419173])\n",
      "iter:20 of epoch:62: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01426272])\n",
      "iter:30 of epoch:62: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01593045])\n",
      "iter:40 of epoch:62: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01334822])\n",
      "iter:0 of epoch:63: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06847083])\n",
      "iter:10 of epoch:63: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01551130])\n",
      "iter:20 of epoch:63: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03961619])\n",
      "iter:30 of epoch:63: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06168512])\n",
      "iter:40 of epoch:63: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02136718])\n",
      "iter:0 of epoch:64: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01481453])\n",
      "iter:10 of epoch:64: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03628097])\n",
      "iter:20 of epoch:64: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05316959])\n",
      "iter:30 of epoch:64: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02177857])\n",
      "iter:40 of epoch:64: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01448810])\n",
      "iter:0 of epoch:65: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01656639])\n",
      "iter:10 of epoch:65: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02168204])\n",
      "iter:20 of epoch:65: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07328739])\n",
      "iter:30 of epoch:65: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.00825984])\n",
      "iter:40 of epoch:65: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02452143])\n",
      "iter:0 of epoch:66: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.00678826])\n",
      "iter:10 of epoch:66: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05129122])\n",
      "iter:20 of epoch:66: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03710618])\n",
      "iter:30 of epoch:66: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.00567292])\n",
      "iter:40 of epoch:66: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.10880314])\n",
      "iter:0 of epoch:67: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08334501])\n",
      "iter:10 of epoch:67: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05615097])\n",
      "iter:20 of epoch:67: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07624142])\n",
      "iter:30 of epoch:67: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06004217])\n",
      "iter:40 of epoch:67: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02448742])\n",
      "iter:0 of epoch:68: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03481904])\n",
      "iter:10 of epoch:68: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01550141])\n",
      "iter:20 of epoch:68: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06100620])\n",
      "iter:30 of epoch:68: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06140568])\n",
      "iter:40 of epoch:68: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.00450676])\n",
      "iter:0 of epoch:69: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02407611])\n",
      "iter:10 of epoch:69: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03981780])\n",
      "iter:20 of epoch:69: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03836481])\n",
      "iter:30 of epoch:69: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06825936])\n",
      "iter:40 of epoch:69: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04108211])\n",
      "iter:0 of epoch:70: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04391497])\n",
      "iter:10 of epoch:70: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01115001])\n",
      "iter:20 of epoch:70: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08770727])\n",
      "iter:30 of epoch:70: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06628441])\n",
      "iter:40 of epoch:70: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07094721])\n",
      "iter:0 of epoch:71: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05930872])\n",
      "iter:10 of epoch:71: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03299193])\n",
      "iter:20 of epoch:71: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04154640])\n",
      "iter:30 of epoch:71: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02382444])\n",
      "iter:40 of epoch:71: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02987046])\n",
      "iter:0 of epoch:72: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05304350])\n",
      "iter:10 of epoch:72: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02721916])\n",
      "iter:20 of epoch:72: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06999707])\n",
      "iter:30 of epoch:72: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02558490])\n",
      "iter:40 of epoch:72: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01912485])\n",
      "iter:0 of epoch:73: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01612753])\n",
      "iter:10 of epoch:73: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04855497])\n",
      "iter:20 of epoch:73: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03620135])\n",
      "iter:30 of epoch:73: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04122001])\n",
      "iter:40 of epoch:73: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.09186571])\n",
      "iter:0 of epoch:74: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04367039])\n",
      "iter:10 of epoch:74: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03714442])\n",
      "iter:20 of epoch:74: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03466936])\n",
      "iter:30 of epoch:74: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02290276])\n",
      "iter:40 of epoch:74: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.10001995])\n",
      "iter:0 of epoch:75: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08163891])\n",
      "iter:10 of epoch:75: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.00820595])\n",
      "iter:20 of epoch:75: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04031203])\n",
      "iter:30 of epoch:75: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07723761])\n",
      "iter:40 of epoch:75: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07284644])\n",
      "iter:0 of epoch:76: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06742164])\n",
      "iter:10 of epoch:76: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02135879])\n",
      "iter:20 of epoch:76: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02266428])\n",
      "iter:30 of epoch:76: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07626771])\n",
      "iter:40 of epoch:76: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01899697])\n",
      "iter:0 of epoch:77: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03662598])\n",
      "iter:10 of epoch:77: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07445488])\n",
      "iter:20 of epoch:77: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06001013])\n",
      "iter:30 of epoch:77: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.10877661])\n",
      "iter:40 of epoch:77: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02446287])\n",
      "iter:0 of epoch:78: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04102537])\n",
      "iter:10 of epoch:78: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04885876])\n",
      "iter:20 of epoch:78: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01314900])\n",
      "iter:30 of epoch:78: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05573656])\n",
      "iter:40 of epoch:78: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04335671])\n",
      "iter:0 of epoch:79: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04380099])\n",
      "iter:10 of epoch:79: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05445433])\n",
      "iter:20 of epoch:79: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05531865])\n",
      "iter:30 of epoch:79: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05578290])\n",
      "iter:40 of epoch:79: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01193696])\n",
      "iter:0 of epoch:80: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01333940])\n",
      "iter:10 of epoch:80: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03043948])\n",
      "iter:20 of epoch:80: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08539959])\n",
      "iter:30 of epoch:80: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02396190])\n",
      "iter:40 of epoch:80: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05057801])\n",
      "iter:0 of epoch:81: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05707268])\n",
      "iter:10 of epoch:81: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.08321973])\n",
      "iter:20 of epoch:81: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02161815])\n",
      "iter:30 of epoch:81: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07085078])\n",
      "iter:40 of epoch:81: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01387173])\n",
      "iter:0 of epoch:82: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04584615])\n",
      "iter:10 of epoch:82: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05455381])\n",
      "iter:20 of epoch:82: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05031957])\n",
      "iter:30 of epoch:82: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01605820])\n",
      "iter:40 of epoch:82: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01681926])\n",
      "iter:0 of epoch:83: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05582361])\n",
      "iter:10 of epoch:83: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07239024])\n",
      "iter:20 of epoch:83: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04482464])\n",
      "iter:30 of epoch:83: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05058176])\n",
      "iter:40 of epoch:83: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01003189])\n",
      "iter:0 of epoch:84: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03700130])\n",
      "iter:10 of epoch:84: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01565352])\n",
      "iter:20 of epoch:84: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02724396])\n",
      "iter:30 of epoch:84: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.00783443])\n",
      "iter:40 of epoch:84: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02394262])\n",
      "iter:0 of epoch:85: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01288670])\n",
      "iter:10 of epoch:85: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02116369])\n",
      "iter:20 of epoch:85: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01485712])\n",
      "iter:30 of epoch:85: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06525814])\n",
      "iter:40 of epoch:85: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04459779])\n",
      "iter:0 of epoch:86: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02577104])\n",
      "iter:10 of epoch:86: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01140579])\n",
      "iter:20 of epoch:86: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.10966332])\n",
      "iter:30 of epoch:86: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05452584])\n",
      "iter:40 of epoch:86: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.11393158])\n",
      "iter:0 of epoch:87: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01830251])\n",
      "iter:10 of epoch:87: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.05503792])\n",
      "iter:20 of epoch:87: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.09512530])\n",
      "iter:30 of epoch:87: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04517751])\n",
      "iter:40 of epoch:87: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02649346])\n",
      "iter:0 of epoch:88: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01318374])\n",
      "iter:10 of epoch:88: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04057701])\n",
      "iter:20 of epoch:88: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02599069])\n",
      "iter:30 of epoch:88: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04229384])\n",
      "iter:40 of epoch:88: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03544561])\n",
      "iter:0 of epoch:89: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04400324])\n",
      "iter:10 of epoch:89: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.07678121])\n",
      "iter:20 of epoch:89: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01546060])\n",
      "iter:30 of epoch:89: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06822934])\n",
      "iter:40 of epoch:89: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01077778])\n",
      "iter:0 of epoch:90: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01495578])\n",
      "iter:10 of epoch:90: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06329182])\n",
      "iter:20 of epoch:90: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.09056557])\n",
      "iter:30 of epoch:90: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04248708])\n",
      "iter:40 of epoch:90: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01413045])\n",
      "iter:0 of epoch:91: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03569803])\n",
      "iter:10 of epoch:91: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02980488])\n",
      "iter:20 of epoch:91: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.02384821])\n",
      "iter:30 of epoch:91: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.03358457])\n",
      "iter:40 of epoch:91: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.04714214])\n",
      "iter:0 of epoch:92: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.01500739])\n",
      "iter:10 of epoch:92: loss:Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [0.06018696])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3670/392113343.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iter:{} of epoch:{}: loss:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mavg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# 保存模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-220>\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, startup_program, parameter_list, no_grad_set)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/base.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_switch_tracer_mode_guard_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, startup_program, parameter_list, no_grad_set)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m         optimize_ops = self.apply_optimize(\n\u001b[0;32m--> 951\u001b[0;31m             loss, startup_program=startup_program, params_grads=params_grads)\n\u001b[0m\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moptimize_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_grads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/optimizer.py\u001b[0m in \u001b[0;36mapply_optimize\u001b[0;34m(self, loss, startup_program, params_grads)\u001b[0m\n\u001b[1;32m    856\u001b[0m                 params_grads = append_regularization_ops(params_grads,\n\u001b[1;32m    857\u001b[0m                                                          self.regularization)\n\u001b[0;32m--> 858\u001b[0;31m                 \u001b[0moptimize_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_optimization_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0mprogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/optimizer.py\u001b[0m in \u001b[0;36m_create_optimization_pass\u001b[0;34m(self, parameters_and_grads)\u001b[0m\n\u001b[1;32m    670\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam_and_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_optimize_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_block\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_and_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mparam_and_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters_and_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-222>\u001b[0m in \u001b[0;36m_append_optimize_op\u001b[0;34m(self, block, param_and_grad)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/base.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_switch_tracer_mode_guard_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/optimizer.py\u001b[0m in \u001b[0;36m_append_optimize_op\u001b[0;34m(self, block, param_and_grad)\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_dygraph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m             core.ops.sgd(param_and_grad[0], lr, param_and_grad[1],\n\u001b[0;32m-> 1032\u001b[0;31m                          param_and_grad[0])\n\u001b[0m\u001b[1;32m   1033\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4,训练过程\n",
    "with dygraph.guard(fluid.CPUPlace()):\n",
    "    EPOCH_NUM = 2000\n",
    "    BATCH_SIZE = 10\n",
    "    for epoch in range(EPOCH_NUM):\n",
    "        mini_batches = load_data_SGD()\n",
    "\n",
    "        for iter,mini_batch in enumerate(mini_batches):\n",
    "            x = np.array(mini_batch[:,:-1]).astype('float32')\n",
    "            y = np.array(mini_batch[:,-1]).astype('float32')\n",
    "\n",
    "            x_p_tensor = dygraph.to_variable(x)\n",
    "            y_p_tensor = dygraph.to_variable(y)\n",
    "\n",
    "            predicts = model.forward(x_p_tensor)\n",
    "            loss = fluid.layers.square_error_cost(predicts,label=y_p_tensor)\n",
    "            avg_loss = fluid.layers.mean(loss)\n",
    "            if iter % 10 == 0:\n",
    "                print('iter:{} of epoch:{}: loss:{}'.format(iter,epoch,avg_loss))\n",
    "            avg_loss.backward()\n",
    "            opt.minimize(avg_loss)\n",
    "            model.clear_gradients()\n",
    "    # 保存模型\n",
    "    fluid.save_dygraph(model.state_dict(),'Regressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-09T01:44:13.299857Z",
     "iopub.status.idle": "2022-01-09T01:44:13.300112Z",
     "shell.execute_reply": "2022-01-09T01:44:13.299996Z",
     "shell.execute_reply.started": "2022-01-09T01:44:13.299983Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5,模型测试\n",
    "# 模型测试包括四步: 指定运行的机器资源 ——> 定义模型结构，加载模型参数 ——> 设置为eval模式 ——> 预测\n",
    "_,test_data = load_data()\n",
    "test_sample = test_data[10,:]#选择第十个样本进行测试\n",
    "test_x = np.array(test_sample[:-1]).astype(np.float32)\n",
    "test_y = test_sample[-1]\n",
    "\n",
    "with dygraph.guard():\n",
    "    model_dict,_ = fluid.load_dygraph('Regressor.pdparams')\n",
    "    model.load_dict(model_dict)\n",
    "    model.eval()\n",
    "    test_x_p_tensor = dygraph.to_variable(test_x)\n",
    "    results = model(test_x_p_tensor).numpy()\n",
    "    print('label:{},label:{}   --(after BN)'.format(test_y,results[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3,mnist(CIFAR)识别\n",
    "- MNIST数据集是LeCun找250位不同标注员手写的\n",
    "- paddle.dataset.cifar.train10(cycle=False)里的数据是(3,32,32)的\n",
    "- 大量实现发现，模型对最后出现的数据印象更加深刻，训练数据导入后，越接近模型训练结束最后几个批次对模型的影响越大。为了避免模型记忆影响训练效果，每个epoch之前需要对数据进行乱序操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 异步读取数据fluid.io.DataLoader.from_generator()\n",
    "- 异步数据读取只是在数据规模巨大时会带来性能的显著提升，对于多数应用场景采用同步数据读取已经足够\n",
    "- capacity表示都队列容量\n",
    "- return_list在动态图模式下设置为True\n",
    "\n",
    "```python\n",
    "place = fluid.CPUPlace() #fluid.CUDAPlace()时，代表数据读取到GPU上\n",
    "with fluid.dygraph.guard(place):\n",
    "    trainset = paddle.dataset.cifar.train10(cycle=False) #enumerate(trainset())\n",
    "    #train_reader = paddle.batch(trainset(),batch_size=8)\n",
    "    data_loader = fluid.io.DataLoader.from_generator(capacity=5,return_list=True)\n",
    "    '''\n",
    "    创建一个Dataloader对象用于加载python生成器产生的数据，数据会由于python线程预先读取,异步送入到\n",
    "    队列中\n",
    "    '''\n",
    "    data_loader.set_batch_generator(trainset,places=place)\n",
    "    '''\n",
    "    用创建的Dataloader对象设置一个数据生成器set_batch_generator,输入的参数是一个数据生成器和服务器\n",
    "    象类型\n",
    "    '''\n",
    "```\n",
    "\n",
    "- trainset是一个返回 all_img,all_label的函数，要用enumerate(trainset())\n",
    "- data_loader.set_batch_generator(trainset,places=place)之后，要用enumerate(data_loader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-09T04:55:25.689311Z",
     "iopub.status.busy": "2022-01-09T04:55:25.688230Z",
     "iopub.status.idle": "2022-01-09T04:55:25.705904Z",
     "shell.execute_reply": "2022-01-09T04:55:25.705043Z",
     "shell.execute_reply.started": "2022-01-09T04:55:25.689253Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_loader = load_data()\\ndata_loader = fluid.io.DataLoader.from_generator(capacity=50,return_list=True)\\ndata_loader.set_batch_generator(train_loader,places=place)\\nfor batch_id,data in enumerate(data_loader):\\n    print(data[0].shape)\\n    img_batch,label_batch = data\\n    #img_batch.shape=(8,3,32,32)\\n    if batch_id > 2:\\n        break\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "from paddle.fluid.dygraph.nn import Linear\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# 1,数据预处理\n",
    "# 飞桨支持很多常用的数据集，并把它们封装在paddle.dataset下(mnist,cifar,imdb...)\n",
    "#对于容量大的数据集，我们使用异步读取的方式，即数据读取和模型训练并行\n",
    "#把读取到的数据不断放入缓存，当训练完一个batch之后，自动读取下一个batch的数据\n",
    "place = fluid.CPUPlace() #fluid.CUDAPlace()时，代表数据读取到GPU上\n",
    "\n",
    "#检验下面的代码能不能正常运行，省的出错\n",
    "\n",
    "with fluid.dygraph.guard(place):\n",
    "    def load_data():\n",
    "        def data_generator():\n",
    "            trainset = paddle.dataset.cifar.train10(cycle=False) #enumerate(trainset())\n",
    "            train_reader = paddle.batch(trainset,batch_size=100)\n",
    "            for i,data in enumerate(train_reader()):\n",
    "                #data是8*[3072,1]的数据\n",
    "                img_batch = np.array([x[0].reshape((3,32,32)) for x in data]).astype('float32') #img_batch.shape=(8,3072)\n",
    "                label_batch = np.array([np.array([x[1]]) for x in data]).astype('int64')\n",
    "                yield np.array(img_batch),np.array(label_batch)\n",
    "        return data_generator\n",
    "'''\n",
    "train_loader = load_data()\n",
    "data_loader = fluid.io.DataLoader.from_generator(capacity=50,return_list=True)\n",
    "data_loader.set_batch_generator(train_loader,places=place)\n",
    "for batch_id,data in enumerate(data_loader):\n",
    "    print(data[0].shape)\n",
    "    img_batch,label_batch = data\n",
    "    #img_batch.shape=(8,3,32,32)\n",
    "    if batch_id > 2:\n",
    "        break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-09T05:46:44.766388Z",
     "iopub.status.busy": "2022-01-09T05:46:44.765665Z",
     "iopub.status.idle": "2022-01-09T05:46:44.772973Z",
     "shell.execute_reply": "2022-01-09T05:46:44.772194Z",
     "shell.execute_reply.started": "2022-01-09T05:46:44.766333Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n展示图片 of cifar\\nfor batch_id,data in enumerate(train_reader()):\\n    img_batch = np.array([x[0] for x in data]).astype('float32') #img_batch.shape=(8,3072)\\n    label_batch = np.array([x[1] for x in data]).astype('float32')\\n    break\\nk = 5\\nr,g,b = img_batch[k].reshape((3,32,32))[0][:,:,np.newaxis],img_batch[k].reshape((3,32,32))[1][:,:,np.newaxis],img_batch[k].reshape((3,32,32))[2][:,:,np.newaxis]\\na = np.concatenate((r,g,b),axis=2)\\nimport matplotlib.pyplot as plt\\nprint(label_batch[k])\\nplt.imshow(a)\\n\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "展示图片 of cifar\n",
    "for batch_id,data in enumerate(train_reader()):\n",
    "    img_batch = np.array([x[0] for x in data]).astype('float32') #img_batch.shape=(8,3072)\n",
    "    label_batch = np.array([x[1] for x in data]).astype('float32')\n",
    "    break\n",
    "k = 5\n",
    "r,g,b = img_batch[k].reshape((3,32,32))[0][:,:,np.newaxis],img_batch[k].reshape((3,32,32))[1][:,:,np.newaxis],img_batch[k].reshape((3,32,32))[2][:,:,np.newaxis]\n",
    "a = np.concatenate((r,g,b),axis=2)\n",
    "import matplotlib.pyplot as plt\n",
    "print(label_batch[k])\n",
    "plt.imshow(a)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用VisualDL\n",
    "- ifro mvisualdl import LogWriter\n",
    "- （1） log_writer = LogWriter('./log')\n",
    "- （2） log_writer.add_scalar(tag='acc',step=iter,value=acc.numpy()) #iter是一个递增对象\n",
    "\n",
    "- 如果没找到左侧工具栏中的可视化，在shell输入：\n",
    "```python\n",
    "visualdl --logdir ./random_log --port 8080\n",
    "#之后就可以看到地址\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-09T05:58:27.086246Z",
     "iopub.status.busy": "2022-01-09T05:58:27.085774Z",
     "iopub.status.idle": "2022-01-09T06:09:15.845055Z",
     "shell.execute_reply": "2022-01-09T06:09:15.843274Z",
     "shell.execute_reply.started": "2022-01-09T05:58:27.086181Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:100 of epoch:0,loss:[3.1957912],acc:[0.15]\n",
      "batch:200 of epoch:0,loss:[3.525582],acc:[0.09]\n",
      "batch:300 of epoch:0,loss:[3.4123232],acc:[0.13]\n",
      "batch:400 of epoch:0,loss:[3.1204336],acc:[0.11]\n",
      "batch:500 of epoch:0,loss:[3.1946807],acc:[0.12]\n",
      "batch:100 of epoch:1,loss:[3.0950537],acc:[0.16]\n",
      "batch:200 of epoch:1,loss:[3.4093294],acc:[0.09]\n",
      "batch:300 of epoch:1,loss:[3.3155084],acc:[0.13]\n",
      "batch:400 of epoch:1,loss:[3.025226],acc:[0.11]\n",
      "batch:500 of epoch:1,loss:[3.1037273],acc:[0.12]\n",
      "batch:100 of epoch:2,loss:[3.014048],acc:[0.16]\n",
      "batch:200 of epoch:2,loss:[3.3139253],acc:[0.1]\n",
      "batch:300 of epoch:2,loss:[3.2360172],acc:[0.12]\n",
      "batch:400 of epoch:2,loss:[2.9480615],acc:[0.12]\n",
      "batch:500 of epoch:2,loss:[3.0288596],acc:[0.12]\n",
      "batch:100 of epoch:3,loss:[2.9467843],acc:[0.15]\n",
      "batch:200 of epoch:3,loss:[3.2332547],acc:[0.09]\n",
      "batch:300 of epoch:3,loss:[3.1687624],acc:[0.13]\n",
      "batch:400 of epoch:3,loss:[2.8838072],acc:[0.12]\n",
      "batch:500 of epoch:3,loss:[2.9655874],acc:[0.14]\n",
      "batch:100 of epoch:4,loss:[2.889446],acc:[0.15]\n",
      "batch:200 of epoch:4,loss:[3.16345],acc:[0.09]\n",
      "batch:300 of epoch:4,loss:[3.1105278],acc:[0.13]\n",
      "batch:400 of epoch:4,loss:[2.8290052],acc:[0.12]\n",
      "batch:500 of epoch:4,loss:[2.9109457],acc:[0.14]\n",
      "batch:100 of epoch:5,loss:[2.8395324],acc:[0.15]\n",
      "batch:200 of epoch:5,loss:[3.1019535],acc:[0.08]\n",
      "batch:300 of epoch:5,loss:[3.0592139],acc:[0.13]\n",
      "batch:400 of epoch:5,loss:[2.7814221],acc:[0.12]\n",
      "batch:500 of epoch:5,loss:[2.8629315],acc:[0.14]\n",
      "batch:100 of epoch:6,loss:[2.7954159],acc:[0.14]\n",
      "batch:200 of epoch:6,loss:[3.047007],acc:[0.08]\n",
      "batch:300 of epoch:6,loss:[3.013378],acc:[0.13]\n",
      "batch:400 of epoch:6,loss:[2.7395465],acc:[0.12]\n",
      "batch:500 of epoch:6,loss:[2.8201933],acc:[0.14]\n",
      "batch:100 of epoch:7,loss:[2.7559447],acc:[0.15]\n",
      "batch:200 of epoch:7,loss:[2.9973764],acc:[0.08]\n",
      "batch:300 of epoch:7,loss:[2.972005],acc:[0.12]\n",
      "batch:400 of epoch:7,loss:[2.702362],acc:[0.11]\n",
      "batch:500 of epoch:7,loss:[2.7818022],acc:[0.13]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_156/2621637535.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mlog_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_iter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch:{} of epoch:{},loss:{},acc:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mavg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-230>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, retain_graph)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mwrapped_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m         assert in_dygraph_mode(\n\u001b[1;32m    224\u001b[0m         ), \"We only support '%s()' in dynamic graph mode, please call 'paddle.disable_static()' to enter dynamic graph mode.\" % func.__name__\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/varbase_patch_methods.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, retain_graph)\u001b[0m\n\u001b[1;32m    175\u001b[0m                                           retain_graph)\n\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dygraph_tracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from paddle.fluid.dygraph.nn import Conv2D\n",
    "from paddle.fluid.dygraph.nn import Pool2D\n",
    "from visualdl import LogWriter\n",
    "\n",
    "# 2,模型设计\n",
    "class SimpleCNN(fluid.dygraph.Layer):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN,self).__init__()\n",
    "        self.conv1 = Conv2D(num_channels=3,num_filters=20,filter_size=5,stride=1,padding=2,act='relu')\n",
    "        self.pool1 = Pool2D(pool_size=2,pool_stride=2,pool_type='max')\n",
    "        self.conv2 = Conv2D(num_channels=20,num_filters=20,filter_size=5,stride=1,padding=2,act='relu')\n",
    "        self.pool2 = Pool2D(pool_size=2,pool_stride=2,pool_type='max')\n",
    "        '''使用以下代码查看Conv，Pool输出层的shape\n",
    "        net = SimpleCNN()\n",
    "        param_info = paddle.summary(net,input_size=(1,3,32,32),dtypes='float32')\n",
    "        '''\n",
    "        self.fc = Linear(input_dim=1280,output_dim=10,act='softmax')\n",
    "\n",
    "    def forward(self,x,label):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = fluid.layers.reshape(x,[x.shape[0],-1])\n",
    "        x = self.fc(x)\n",
    "        acc = fluid.layers.accuracy(input=x,label=label)\n",
    "        return x,acc\n",
    "\n",
    "# 3，训练过程\n",
    "with fluid.dygraph.guard(place):\n",
    "    use_VisualDL = True#是否使用VisualDL\n",
    "    if use_VisualDL:\n",
    "        log_writer = LogWriter(\"./log\")\n",
    "\n",
    "    train_loader = load_data()\n",
    "    model = SimpleCNN()\n",
    "    model.train()\n",
    "    #可以在optimizer中加入正则化\n",
    "    #optimizer = fluid.optimizer.AdamOptimizer(learning_rate=0.00001,regularization=fluid.regularizer.L2Decay(regularization_coeff=0.1),parameter_list=model.parameters())\n",
    "    optimizer = fluid.optimizer.SGDOptimizer(learning_rate=0.000001,parameter_list=model.parameters())\n",
    "\n",
    "    EPOCH_NUM = 20\n",
    "    log_iter = 0\n",
    "\n",
    "    for epoch in range(EPOCH_NUM):\n",
    "        data_loader = fluid.io.DataLoader.from_generator(capacity=50,return_list=True)\n",
    "        data_loader.set_batch_generator(train_loader,places=place)\n",
    "        for batch_id,data in enumerate(data_loader):\n",
    "            img_batch,label_batch = data\n",
    "            img_batch_tensor = fluid.dygraph.to_variable(img_batch)\n",
    "            label_batch_tensor = fluid.dygraph.to_variable(label_batch)\n",
    "            predict,acc = model(img_batch_tensor,label_batch_tensor)\n",
    "            loss = fluid.layers.cross_entropy(predict,label_batch_tensor)\n",
    "            avg_loss = fluid.layers.mean(loss)\n",
    "            if (batch_id+1)% 100 == 0:\n",
    "                if use_VisualDL:\n",
    "                    log_writer.add_scalar(tag='acc',step=log_iter,value=acc.numpy())\n",
    "                    log_writer.add_scalar(tag='loss',step=log_iter,value=avg_loss.numpy())\n",
    "                    log_iter = log_iter + 100\n",
    "                print('batch:{} of epoch:{},loss:{},acc:{}'.format(batch_id+1,epoch,avg_loss.numpy(),acc.numpy()))\n",
    "            avg_loss.backward()\n",
    "            optimizer.minimize(avg_loss)\n",
    "            model.clear_gradients()\n",
    "    fluid.save_dygraph(model.state_dict(),'simplecnn_of_cifar')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对于分类问题，直接以标签和概率作比较也不合理，绝大多数图像分类任务都使用交叉熵损失函数。\n",
    "- fluid.layers.cross_entropy()要求输入是int64。\n",
    "\n",
    "$$\n",
    "L = -[\\sum_{k = 1}^{n}t_{k}lgy_{k} + (1-t_{k})lg(1-y_{k})]\n",
    "$$\n",
    "\n",
    "- 由于损失函数本身的数学意义上的不同，我们无法对比损失值来对比那种损失函数效果更好，在图片分类问题中可以用的方法就只有分类准确率\n",
    "- fluid.layers.accuracy(input=x,label=label)可以计算分类准确率,label是2D的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 资源配置:多卡训练、多机训练\n",
    "\n",
    "```python\n",
    "# 单卡训练\n",
    "use_gpu = False\n",
    "place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()\n",
    "\n",
    "with fluid.dygraph.guard(place)\n",
    "\n",
    "```\n",
    "\n",
    "- 在工业实践中，训练一个在ImageNet上精度表现良好的模型，大概需要一周的时间。\n",
    "> 我们采用分布式训练方法：(1)模型并行 (2)数据并行\n",
    ">>(1)模型并行  \n",
    "      将一个网络拆成多份，比如2012年的Alexnet就是采用模型并行的方式训练而成的。\n",
    "      现常用在很大的，各模块相对独立的模型中，即模型过大无法完整的放入单个GPU。\n",
    "> \n",
    ">>(2)数据并行\n",
    "      每次读取多份数据，读取到的数据给多个设备上的相同模型，\n",
    "      **飞桨用的就是数据并行方式的分布式训练**\n",
    "\n",
    "#### 数据并行\n",
    "\n",
    "- 1 我们需要一个梯度同步机制，梯度同步有两种方式：PRC通信，NCCL2通信\n",
    "- 基本大家都是用的NCCL2(Nvidia Collective multi-GPU Communication Library)\n",
    "![Parameter Server](https://ai-studio-static-online.cdn.bcebos.com/560af46fd88140e8bc357dfad0d21e547e4703073c834c6c99c342b79e5076e4)\n",
    "![NCCL2](https://ai-studio-static-online.cdn.bcebos.com/a27f1873e4934a0f8cda436b33830268ef4621cf6b994deb839db0a272e75de1)\n",
    "- PRC通信方式常用于**CPU**分布训练，他有两个节点：参数服务器(Parameter server)和训练节点(Trainer)\n",
    "参数服务器收集来自各个设备的梯度更新信息，并计算出一个全局的梯度更新，Trainer用于训练，每个Trainer上的程序相同，但数据不同，当参数服务器收到来自训练节点的梯度更新请求时，统一的更新全局的梯度。\n",
    "- NCCL2方式不需要启动Parameter Server进程，每个Trainer进程保存一份完整的模型参数，在完成梯度计算之后通过Trainer相互通信，Reduce之间的相互通信，Reduce梯度数据到所有结点的所有设备，然后每个节点再各自完成参数更新。\n",
    "\n",
    "#### 在\n",
    "> place  \n",
    " 模型实例化  \n",
    " train_loader定义  \n",
    " loss聚合\n",
    " \n",
    "#### 四个方面做出改变即可  \n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "```python\n",
    "# 1,从环境变量获取设备的ID\n",
    "device_id = fluid.dygraph.parallel.Env().dev_id\n",
    "place = fluid.CUDAPlace(device_id)\n",
    "\n",
    "# 2,对定义的网络做预处理，设置为并行模式\n",
    "strategy = fluid.dygraph.parallel.prepare_context()\n",
    "model = SimpleCNN()\n",
    "model = fluid.dygraph.parallel.DataParallel(model,strategy)\n",
    "\n",
    "# 3,定义多GPU训练的reader\n",
    "valid_loader = paddle.batch(paddle.dataset.mnist.test(),batch_size=16,drop_last=True)\n",
    "valid_loader = fluid.contrib.reader.distributed_batch_reader(valid_loader)\n",
    "\n",
    "# 4,收集每批次训练的loos,并聚合参数的梯度\n",
    "avg_loss = mnist.scale_loss(avg_loss)\n",
    "avg_loss.backward()\n",
    "mnist.apply_collective_grads()\n",
    "```\n",
    "\n",
    "  \n",
    "- 训练集：用于训练模型的参数\n",
    "- 验证集：用于模型超参数的选择，比如网络结构的调整，正则化权重的选择。\n",
    "- 测试集：用于模拟模型在应用后的真实效果.因为测试集没有参与任何模型优化或参数训练工作，所以它对模型来说是完全未知的样本，在不调整超参数或者优化网络结构时，测试集和验证集的效果是类似的，均能更真实的反应模型效果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型参数及优化器保存\n",
    "\n",
    "- 1,使用动态学习率\n",
    "```pytho1\n",
    "lr = fluid.dygraph.PolynominalDecay(0.01,total_steps,0.00001)\n",
    "```\n",
    "- 2,保存模型的参数和优化器的参数\n",
    "\n",
    "```python\n",
    "fluid.save_dygraph(model.state_dict(),'./checkpoint/epoch{}'.format(epoch_id))\n",
    "fluid.save_dygraph(optimizer.satte_dict(),'./checkpoint/epoch{}'.format(epoch_id))\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "params_dict,opt_dict = fluid.load_dygraph(para,s_path)\n",
    "                   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
