{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 视觉问题中的注意力机制 \n",
    "> 1,ViT （图片分类）模型定义  \n",
    "> 2,DeiT (图片分类)  \n",
    "> 3,自己写paddle.vision.transforms  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练ViT很不容易\n",
    "  - 1,vit需要大量的GPU资源\n",
    "  - 2，ViT与训练数据集JFT-300M没有公开 \n",
    "  - 3，超参数设置不好很容易Train不出效果  \n",
    "- 如何更有效的训练ViT模型？   \n",
    "DeiT方法(Data-efficient image Transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 把[3,28,28]的图片裁剪成16*[3,7,7]个小图片\n",
    "- 对每个[3,7,7]的小图片拉直成为[None，147]整张大图变为[16,147]的图片，我们管这个[16,147]的东西叫patches\n",
    "- patches[16,147] * PatchEmbedding[147,96] = image tokens[16,96]这里的Embedding层中参数是可学习的\n",
    "- 把得到的结果image tokens[16,96]和图片[3,28,28]一起输入到transformer  \n",
    "\n",
    "这里的[3,7,7]方块就相当于一个单词，整张[3,28,28]的图片就相当于一个16个单词的句子。最后根据每个“单词”的特点生成[96]的特征向量。  \n",
    "在单个序列中使用不同位置的注意力用于实现该序列的表征方法  \n",
    "  \n",
    "#### Seq2seq注意力机制\n",
    "![title](http://tiebapic.baidu.com/forum/w%3D580/sign=70182e764d3853438ccf8729a313b01f/f5b2f803918fa0ec4dc8b9177b9759ee3d6ddb04.jpg)  \n",
    "这里的x1就代表一个单词或者我们上面一个小图转化成的[96]的特征向量\n",
    "  \n",
    "注意力机制中x1--> [q1,k1,v1]的过程就叫Embeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](http://tiebapic.baidu.com/forum/w%3D580/sign=401e08b6d1eef01f4d1418cdd0ff99e0/a248c95c103853430f6336f3d613b07ecb808813.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T05:07:20.028237Z",
     "iopub.status.busy": "2022-02-16T05:07:20.027396Z",
     "iopub.status.idle": "2022-02-16T05:07:20.047613Z",
     "shell.execute_reply": "2022-02-16T05:07:20.046895Z",
     "shell.execute_reply.started": "2022-02-16T05:07:20.028198Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention(\n",
      "  (qkv): Linear(in_features=96, out_features=288, dtype=float32)\n",
      "  (softmax): Softmax(axis=-1)\n",
      "  (proj): Linear(in_features=96, out_features=96, dtype=float32)\n",
      ")\n",
      "[8, 16, 96]\n"
     ]
    }
   ],
   "source": [
    "import paddle \n",
    "import paddle.nn as nn \n",
    "\n",
    "class Attention(nn.Layer):\n",
    "    '''\n",
    "    输入是patches:变成特征向量之后的[8,num_batches,96]\n",
    "    输出是tokens:[8,num_batches,96]\n",
    "    '''\n",
    "    def __init__(self,embed_dim,num_heads,qkv_bias=False,qk_scale=None,dropout=0.,attention_dropout=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads  = num_heads\n",
    "        self.head_dim = int(embed_dim/num_heads)\n",
    "        self.all_head_dim = self.head_dim * num_heads\n",
    "        self.qkv = nn.Linear(embed_dim,self.all_head_dim*3,bias_attr=False if qkv_bias is False else None)\n",
    "        self.scale = self.head_dim ** -0.5 if qk_scale is None else qk_scale \n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        self.proj = nn.Linear(self.all_head_dim,embed_dim)\n",
    "    \n",
    "    def transpose_multi_head(self,x):\n",
    "        new_shape = x.shape[:-1] + [self.num_heads,self.head_dim] \n",
    "        x = x.reshape(new_shape)\n",
    "        x = x.transpose([0,2,1,3])\n",
    "        # x [B,num_heads,num_patches,head_dim]\n",
    "        return x \n",
    "    def forward(self,x):\n",
    "        B, N, _ = x.shape # B是batch_size,N是num_patches\n",
    "        qkv = self.qkv(x).chunk(3,-1)\n",
    "        # qkv [B,N,all_head_dim] * 3\n",
    "        q,k,v = map(self.transpose_multi_head,qkv)\n",
    "        # q,k,v [B,num_heads,num_patches,head_dim]\n",
    "       \n",
    "        attn = paddle.matmul(q,k,transpose_y=True) # q*k' \n",
    "        attn = self.scale * attn \n",
    "        attn = self.softmax(attn) \n",
    "        # attn [B,num_heads,N]\n",
    "\n",
    "        out = paddle.matmul(attn,v)\n",
    "        # out [B,num_heads,num_patches,head_dim]\n",
    "        out = out.transpose([0,2,1,3])\n",
    "        # out [B,num_patches,num_heads,head_dim] [8,16,4,24]\n",
    "        out = out.reshape([B,N,-1])\n",
    "        out = self.proj(out)\n",
    "\n",
    "        return out \n",
    "\n",
    "def main():\n",
    "    t = paddle.randn([8,16,96]) # batchsize=8,16个单词\n",
    "    model = Attention(embed_dim=96, num_heads=4, qkv_bias=False, qk_scale=None)\n",
    "    print(model)\n",
    "    out = model(t)\n",
    "    print(out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有两种方案，PostNorm和PreNorm，大量实验表明，PreNorm效果更好。  \n",
    "- PostNorm   \n",
    "$$\n",
    "BatchNorm(input + Multi-Attention(input))\n",
    "$$   \n",
    "- PreNorm  \n",
    "$$  \n",
    "input + Multi-Attention(BatchNorm(input))\n",
    "$$\n",
    "  \n",
    "##### BN是把一摞书中每本书的同一页取出来归一化\n",
    "##### LN是对每本书自己做归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT实践 论文复现  \n",
    "![title](http://tiebapic.baidu.com/forum/w%3D580/sign=14272c957c12b31bc76ccd21b61a3674/457901f41bd5ad6eba6ed438c4cb39dbb7fd3c2b.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T14:18:25.870104Z",
     "iopub.status.busy": "2022-02-16T14:18:25.869264Z",
     "iopub.status.idle": "2022-02-16T14:18:25.888182Z",
     "shell.execute_reply": "2022-02-16T14:18:25.887245Z",
     "shell.execute_reply.started": "2022-02-16T14:18:25.870058Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ViT\n",
    "'''\n",
    "import paddle \n",
    "import paddle.nn as nn\n",
    "from encoder import MLP as MLP\n",
    "# Encoder\n",
    "class EncoderLayer(nn.Layer):\n",
    "    def __init__(self,embed_dim=768,num_heads=4,qkv_bias=True,mlp_ratio=4,dropout=0.):\n",
    "        super().__init__()\n",
    "        self.attn_norm = nn.LayerNorm(embed_dim)\n",
    "        self.attn = Attention(embed_dim,num_heads)\n",
    "        self.mlp_norm = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim,mlp_ratio)\n",
    "    def forward(self,x):\n",
    "        h = x \n",
    "        x = self.attn_norm(x)\n",
    "        x = self.attn(x) \n",
    "        x = x + h\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Layer):\n",
    "    def __init__(self,embed_dim,depth):\n",
    "        super().__init__()\n",
    "        layer_list = []\n",
    "        for i in range(depth):\n",
    "            encoder_layer = EncoderLayer()\n",
    "            layer_list.append(encoder_layer)\n",
    "        self.layers = nn.LayerList(layer_list)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "# PathchEmbedding \n",
    "class PatchEmbedding(nn.Layer):\n",
    "    def __init__(self, image_size=224, patch_size=16, in_channels=3, embed_dim=768, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        n_patches = (image_size // patch_size) * (image_size // patch_size) \n",
    "        self.patch_embedding = nn.Conv2D(in_channels=in_channels,\n",
    "                                         out_channels=embed_dim,\n",
    "                                         kernel_size=patch_size,\n",
    "                                         stride=patch_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # add class token \n",
    "        self.class_token = paddle.create_parameter(\n",
    "            shape = [1,1,embed_dim],\n",
    "            dtype='float32',\n",
    "            default_initializer=nn.initializer.Constant(0.)) #这个不是层，只是一个可学习参数 \n",
    "        # add position embedding \n",
    "        self.position_embedding = paddle.create_parameter(\n",
    "            shape = [1,n_patches+1,embed_dim],\n",
    "            dtype='float32',\n",
    "            default_initializer=nn.initializer.TruncatedNormal())\n",
    "    def forward(self,x):\n",
    "        # [n,c,h,w]\n",
    "        cls_tokens = self.class_token.expand([x.shape[0],1,self.embed_dim])\n",
    "        x = self.patch_embedding(x) # [n,embed_dim,h',w']\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose([0,2,1])\n",
    "        x = paddle.concat([cls_tokens,x],axis=1)\n",
    "        x = x + self.position_embedding \n",
    "        return x\n",
    "\n",
    "\n",
    "# main network\n",
    "class VisualTransformer(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 image_size=224,\n",
    "                 patch_size=16,\n",
    "                 in_channels=3,\n",
    "                 num_classes=1000,\n",
    "                 embed_dim=768,\n",
    "                 depth=3,\n",
    "                 num_heads=8,\n",
    "                 mlp_ratio=4,\n",
    "                 qkv_bias=True,\n",
    "                 dropout=0.,\n",
    "                 attention_dropout=0.,\n",
    "                 droppath=0.):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_embedding = PatchEmbedding(image_size,patch_size,in_channels,embed_dim)\n",
    "        self.encoder = Encoder(embed_dim,depth)\n",
    "        self.classifier = nn.Linear(embed_dim,num_classes)\n",
    "    def forward(self,x):\n",
    "        # x [N,C,H,W]\n",
    "        N,C,H,W = x.shape\n",
    "        x = self.patch_embedding(x)\n",
    "        # x [N,embed_dim,h',w'] h'=H/patch_size w'=W/patchsize\n",
    "        x = x.flatten(2) #把二三维度变成h*w\n",
    "        # x [N,embed_dim,h'*w'] = [N,embed_dim,num_patches]\n",
    "        # x = x.transpose([0,2,1])\n",
    "        # x [N,num_patches,embed_dim]\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x[:,0])\n",
    "        return x \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    vit = VisualTransformer()\n",
    "    paddle.summary(vit,(4,3,224,224))\n",
    "\n",
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ViT模型需要很强大的GPU，预训练数据集JFT-300M没有公开 \n",
    "鉴于此，DeiT模型很好地解决了此问题  \n",
    "![title](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpic1.zhimg.com%2Fv2-957ba527a96523b4882173801897b860_b.jpg&refer=http%3A%2F%2Fpic1.zhimg.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1647598499&t=c4880640ae34d415c97c077a31948c6a)  \n",
    "DeiT加入了Distillation(知识蒸馏)，就是teacher-student  \n",
    "  \n",
    "DeiT知识蒸馏 \n",
    "下图是soft-Distillation  \n",
    "![title](http://tiebapic.baidu.com/forum/w%3D580/sign=fa0191f3d01001e94e3c1407880f7b06/296b6463f6246b6055fe6a6eb6f81a4c500fa29f.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](http://tiebapic.baidu.com/forum/w%3D580/sign=3aa4a80954f79052ef1f47363cf3d738/3e153612b31bb051e21968326b7adab44aede04a.jpg)  \n",
    "Data Augmentation \n",
    "- (1)Random Erease\n",
    "- (2)Mixup ,Cutmix\n",
    "- (3)Droppath\n",
    "- (4)EMA\n",
    "- (5)AutoAug  \n",
    "![title](http://tiebapic.baidu.com/forum/w%3D580/sign=02e4de64781f95caa6f592bef9167fc5/b84cb6389b504fc2408c6b1fb8dde71191ef6db8.jpg)  \n",
    "  \n",
    "AutoAugmentation是25种变化打包在一起的一种方式，谷歌自己搞得。 每次Aug时从25种里面随机选一个用。  \n",
    "RandAug比上面的简便一些，是13个policy。  \n",
    "- Model EMA 是一种利用上一轮和这一轮模型参数 加权求和的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T16:36:14.001194Z",
     "iopub.status.busy": "2022-02-16T16:36:14.000641Z",
     "iopub.status.idle": "2022-02-16T16:36:14.493769Z",
     "shell.execute_reply": "2022-02-16T16:36:14.492959Z",
     "shell.execute_reply.started": "2022-02-16T16:36:14.001158Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "    Layer (type)          Input Shape          Output Shape         Param #    \n",
      "=================================================================================\n",
      "      Conv2D-8         [[4, 3, 224, 224]]    [4, 768, 14, 14]       590,592    \n",
      "     Dropout-26         [[4, 198, 768]]       [4, 198, 768]            0       \n",
      "PatchEmbedding_DeiT-3  [[4, 3, 224, 224]]     [4, 198, 768]         153,600    \n",
      "    LayerNorm-43        [[4, 198, 768]]       [4, 198, 768]          1,536     \n",
      "     Linear-108         [[4, 198, 768]]       [4, 198, 2304]       1,769,472   \n",
      "     Softmax-33        [[4, 4, 198, 198]]    [4, 4, 198, 198]          0       \n",
      "     Linear-109         [[4, 198, 768]]       [4, 198, 768]         590,592    \n",
      "    Attention-33        [[4, 198, 768]]       [4, 198, 768]            0       \n",
      "   EncoderLayer-20      [[4, 198, 768]]       [4, 198, 768]            0       \n",
      "    LayerNorm-45        [[4, 198, 768]]       [4, 198, 768]          1,536     \n",
      "     Linear-112         [[4, 198, 768]]       [4, 198, 2304]       1,769,472   \n",
      "     Softmax-34        [[4, 4, 198, 198]]    [4, 4, 198, 198]          0       \n",
      "     Linear-113         [[4, 198, 768]]       [4, 198, 768]         590,592    \n",
      "    Attention-34        [[4, 198, 768]]       [4, 198, 768]            0       \n",
      "   EncoderLayer-21      [[4, 198, 768]]       [4, 198, 768]            0       \n",
      "    LayerNorm-47        [[4, 198, 768]]       [4, 198, 768]          1,536     \n",
      "     Linear-116         [[4, 198, 768]]       [4, 198, 2304]       1,769,472   \n",
      "     Softmax-35        [[4, 4, 198, 198]]    [4, 4, 198, 198]          0       \n",
      "     Linear-117         [[4, 198, 768]]       [4, 198, 768]         590,592    \n",
      "    Attention-35        [[4, 198, 768]]       [4, 198, 768]            0       \n",
      "   EncoderLayer-22      [[4, 198, 768]]       [4, 198, 768]            0       \n",
      "    LayerNorm-49        [[4, 198, 768]]       [4, 198, 768]          1,536     \n",
      "   Encoder_DeiT-1       [[4, 198, 768]]    [[4, 768], [4, 768]]        0       \n",
      "     Linear-120            [[4, 768]]           [4, 1000]           769,000    \n",
      "     Linear-121            [[4, 768]]           [4, 1000]           769,000    \n",
      "=================================================================================\n",
      "Total params: 9,368,528\n",
      "Trainable params: 9,368,528\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------------\n",
      "Input size (MB): 2.30\n",
      "Forward/backward pass size (MB): 130.43\n",
      "Params size (MB): 35.74\n",
      "Estimated Total Size (MB): 168.47\n",
      "---------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 普通ViT的patch_embedding,模型主干稍微修改\r\n",
    "'''\r\n",
    "DeiT\r\n",
    "'''\r\n",
    "class Encoder_DeiT(nn.Layer):\r\n",
    "    def __init__(self,embed_dim,depth):\r\n",
    "        super().__init__()\r\n",
    "        layer_list = []\r\n",
    "        for i in range(depth):\r\n",
    "            encoder_layer = EncoderLayer()\r\n",
    "            layer_list.append(encoder_layer)\r\n",
    "        self.layers = nn.LayerList(layer_list)\r\n",
    "        self.norm = nn.LayerNorm(embed_dim)\r\n",
    "    def forward(self,x):\r\n",
    "        for layer in self.layers:\r\n",
    "            x = layer(x)\r\n",
    "        x = self.norm(x)\r\n",
    "        return x[:,0],x[:,1]\r\n",
    "\r\n",
    "class PatchEmbedding_DeiT(nn.Layer):\r\n",
    "    def __init__(self, image_size=224, patch_size=16, in_channels=3, embed_dim=768, dropout=0.):\r\n",
    "        super().__init__()\r\n",
    "        self.embed_dim = embed_dim\r\n",
    "        n_patches = (image_size // patch_size) * (image_size // patch_size) \r\n",
    "        self.patch_embedding = nn.Conv2D(in_channels=in_channels,\r\n",
    "                                         out_channels=embed_dim,\r\n",
    "                                         kernel_size=patch_size,\r\n",
    "                                         stride=patch_size)\r\n",
    "        self.dropout = nn.Dropout(dropout)\r\n",
    "        # add class token \r\n",
    "        self.class_token = paddle.create_parameter(\r\n",
    "            shape = [1,1,embed_dim],\r\n",
    "            dtype='float32',\r\n",
    "            default_initializer=nn.initializer.Constant(0.)) #这个不是层，只是一个可学习参数 \r\n",
    "        # add distill token \r\n",
    "        self.distill_token = paddle.create_parameter(\r\n",
    "            shape = [1,1,embed_dim],\r\n",
    "            dtype='float32',\r\n",
    "            default_initializer=nn.initializer.TruncatedNormal(0.02)) #这个不是层，只是一个可学习参数 \r\n",
    "        # add position embedding \r\n",
    "        self.position_embedding = paddle.create_parameter(\r\n",
    "            shape = [1,n_patches+2,embed_dim],\r\n",
    "            dtype='float32',\r\n",
    "            default_initializer=nn.initializer.TruncatedNormal(.02))\r\n",
    "    def forward(self,x):\r\n",
    "        # [n,c,h,w]\r\n",
    "        cls_tokens = self.class_token.expand([x.shape[0],1,self.embed_dim])\r\n",
    "        distill_tokens = self.distill_token.expand((x.shape[0],-1,-1))\r\n",
    "        x = self.patch_embedding(x) # [n,embed_dim,h',w']\r\n",
    "        x = x.flatten(2)\r\n",
    "        x = x.transpose([0,2,1])\r\n",
    "        x = paddle.concat([cls_tokens,distill_tokens,x],axis=1)\r\n",
    "        x = x + self.position_embedding \r\n",
    "        x = self.dropout(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "\r\n",
    "class DeiT(nn.Layer): \r\n",
    "    def __init__(self,\r\n",
    "                img_size=224,\r\n",
    "                patch_size=16, \r\n",
    "                in_channels=3,\r\n",
    "                num_classes=1000,\r\n",
    "                embed_dim=768,\r\n",
    "                depth=3,\r\n",
    "                num_heads=8,\r\n",
    "                mlp_ratio=1,\r\n",
    "                qkv_bias=True,\r\n",
    "                dropout=0.,\r\n",
    "                attention_dropout=0.,\r\n",
    "                droppath=0.):\r\n",
    "        super().__init__() \r\n",
    "        self.patch_embedding = PatchEmbedding_DeiT(224,16,3,768)\r\n",
    "        self.encoder = Encoder_DeiT(embed_dim,depth)\r\n",
    "        self.head = nn.Linear(embed_dim,num_classes)\r\n",
    "        self.head_distill = nn.Linear(embed_dim,num_classes)\r\n",
    "        \r\n",
    "    def forward(self,x):\r\n",
    "        x = self.patch_embedding(x)  \r\n",
    "        x,x_distill = self.encoder(x)\r\n",
    "        x = self.head(x)\r\n",
    "        x_distill = self.head_distill(x_distill)\r\n",
    "        if self.training:\r\n",
    "            return x,x_distill \r\n",
    "        else: \r\n",
    "            return (x + x_distill)/2 \r\n",
    "        \r\n",
    "\r\n",
    "def main():\r\n",
    "    model = DeiT()\r\n",
    "    paddle.summary(model,(4,3,224,224)) \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自己写paddle.vision.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T17:06:42.484614Z",
     "iopub.status.busy": "2022-02-16T17:06:42.484093Z",
     "iopub.status.idle": "2022-02-16T17:06:42.516920Z",
     "shell.execute_reply": "2022-02-16T17:06:42.516051Z",
     "shell.execute_reply.started": "2022-02-16T17:06:42.484580Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_129/4036375627.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_129/4036375627.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m                         \u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                         ])\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_129/4036375627.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     43\u001b[0m         '''\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_129/4036375627.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/vision/transforms/transforms.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size, interpolation, keys)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         assert isinstance(size, int) or (isinstance(size, Iterable) and\n\u001b[0;32m--> 404\u001b[0;31m                                          len(size) == 2)\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import paddle.vision.transforms as T  \r\n",
    "import numpy as np \r\n",
    "from PIL import Image \r\n",
    "import paddle \r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "\r\n",
    "def crop(img,region):\r\n",
    "    img = T.crop(img,*region)\r\n",
    "    return img\r\n",
    "class CenterCrop():\r\n",
    "    def __init__(self,size):\r\n",
    "        self.size = size \r\n",
    "    def __call__(self,image):\r\n",
    "        w,h,_ = image.size\r\n",
    "        ch,cw = self.size \r\n",
    "        crop_top = int(round(h-ch)/2.)\r\n",
    "        crop_left = int(round(w-cw)/2.)\r\n",
    "        return crop(image,(crop_top,crop_left,ch,cw))\r\n",
    "class Resize():\r\n",
    "    def __init__(self,size):\r\n",
    "        self.size=size \r\n",
    "    def __call__(self,image):\r\n",
    "        return T.Resize(image,self.size)\r\n",
    "\r\n",
    "class ToTensor():\r\n",
    "    def __init__(self):\r\n",
    "        pass\r\n",
    "    def __call__(self,image): \r\n",
    "        w,h = image.size\r\n",
    "        image = paddle.to_tensor(np.array(image))\r\n",
    "        if image.dtype == paddle.uint8:\r\n",
    "            image = paddle.cast(image,dtype='float32')/255.0 #强制类型转换 + 归一化 \r\n",
    "        image = image.transpose([2,0,1]) \r\n",
    "        return image \r\n",
    "\r\n",
    "class Compose():\r\n",
    "    def __init__(self,transforms):\r\n",
    "        self.transforms = transforms \r\n",
    "    def __call__(self,image):\r\n",
    "        '''\r\n",
    "        把class当函数用\r\n",
    "        '''\r\n",
    "        for t in self.transforms:\r\n",
    "            image = t(image)\r\n",
    "        return image\r\n",
    "\r\n",
    "def main():\r\n",
    "    img = Image.open('img.jpg')\r\n",
    "    transforms = Compose([\r\n",
    "                        Resize([256,256]),\r\n",
    "                        CenterCrop([112,112]),\r\n",
    "                        ToTensor()\r\n",
    "                        ])\r\n",
    "    out = transforms(img)\r\n",
    "ss\r\n",
    "\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
