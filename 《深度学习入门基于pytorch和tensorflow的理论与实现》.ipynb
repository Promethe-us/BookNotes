{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 以下内容来自<<深度学习入门 基于pytorch和tensorflow的理论与实现>>    \n  红色石头 著","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mxnet import nd\n#mxnet是amazon的深度学习框架，nd数据可以直接加减乘除，nd.dot(X,Y.T)是X与Y的转置作矩阵乘法。","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = nd.random.normal(0,1,shape=(3,4))\ny = nd.ones((3,4))\nz = nd.concat(x,y,dim=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch就是pytorch的核心库，torch是服务于torchvision的，用来生成图片，视频数据集，一些流行的模型类和预训练模型。\nimport torch\nimport torchvision\n#torchvision包括datasets,models,transforms,utils四个模块","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#张量可以在GPU版本pytorch上运行，但是nparray只能在CPU版本的pytorch上运行。\nx = torch.tensor([[1,2],[3,4]])\ny = torch.tensor([[1,1],[1,1]])\nx + y #torch.add(x,y) 和 x+y 都可以实现张量加法\nx.mul(y)#对应元素相乘\nx.mm(y) #矩阵相乘","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tensor与nparray可以互相转换，也可以共享内存位置，即一个发生改变另一个也变。\na = np.array([[1,1],[1,1]])\nb = torch.from_numpy(a)\nnp.add(a,1,out=a)\n#新建的张量默认保存在CPU里，可以自行移动到GPU之中。\nb_cuda = b.cuda()\nb_cuda\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pytorch具有自动求导功能，但是需要在设置张量时定义清除\nx = torch.ones(1,4,requires_grad=True)\nz = torch.sum(x)\n#在pytorch中，每个通过函数计算得到的变量都有一个.grad_fn属性，下面要求z对x,y的偏导数。\nz.backward()\nprint(x.grad)\n#在设置.requires_grad = True的时候，他们开始形成一个反向图，\n#跟踪应用于他们的每个操作，使用所谓的动态计算图(DCG)计算梯度。\n#只有设置成浮点型才可以求导。\n#https://blog.csdn.net/u011984148/article/details/99670194\n#https://blog.csdn.net/qq_27825451/article/details/89393332?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0.no_search_link&spm=1001.2101.3001.4242\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n#利用torch.nn可以很方便的定义损失函数，最简单的是nn.MSELoss\nclass net_name(nn.Module):\n#继承父类nn.Module,nn.Module是torch.nn之中一个十分重要的类，包含各层网络定义及forward方法。\n    def __init__(self):\n        super(net_name,self).__init__()\n        self.fc = nn.Linear(2,1)\n        #self.fc = nn.Linear表示对于模型的搭建，2是输入维度，1是输出维度。\n    def forward(self,x):\n        out = self.fc(x)\n        return out\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#优化算法库\n'''\noptimizer = optim.SGD(net.parameters(),lr=0.01)\noptimizer.zero_grad() #开始之前一定进行梯度清零\noutput = net(input)\nloss = nn.MSELoss(output,target)\nloss.backward()\nloos.backward()\noptimizer.step() #梯度更新\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#简单线性回归\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nx  = torch.unsqueeze(torch.linspace(-1,1,50),dim=1) #对维度进行压缩去掉维度1\ny = 3*x + 0.5*torch.randn(x.size())\nplt.scatter(x,y)\nplt.show()\nclass LinearRegression(nn.Module):\n    def __init__(self):\n        super(LinearRegression,self).__init__()\n        self.fc = nn.Linear(1,1)\n    def forward(self,x):\n        out = self.fc(x)\n        return out\n\n#模型实例化\nmodel = LinearRegression()\n\n#优化器\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(),lr=5e-3)\n\n#模型训练\nnum_epochs = 1000\nfor epoch in range(num_epochs):\n    out = model(x) #这里为什么直接调用模型就自动前向传播\n    loss = criterion(out,y)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch+1)%20 == 0:\n        print('Epoch{}/{},loss:{}'.format(epoch,num_epochs,loss))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval() #启动测试模式\ny_hat = model(x)\nplt.scatter(x.numpy(),y_hat.detach().numpy(),c='r',label='拟合直线')\nplt.show()\n#用以下代码查看模型的w0和w1参数\nlist(model.named_parameters())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tensorflow是一个采用数据流图进行数值计算的开源软件库，其基本原理是基于图运算，将一个计算图拆分成多个子图，然后并行计算，除此以外tensorflow还支持分布式计算。\nimport tensorflow as tf\nc0 = tf.constant(2,name='c0') #0阶张量\nc1 = tf.constant([1,2,3],name = 'c1') #一阶张量\n#tf中可以使用name参数对张量指定名称，这样做的好处是可以在Tensorboard中查看张量。","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = tf.constant([[1,2],[3,4]]) #tf.constant是常量，一旦创建之后其值便不能改变。\nb = tf.constant([[5,6],[7,8]])\nadd = tf.add(a,b,name='add')\nsub = tf.subtract(a,b,name='sub')\nmultiply = tf.multiply(a,b,name='multiply') #逐项相乘\nmatmul = tf.matmul(a,b,name='matmul') #矩阵乘法\n#tensorflow是一个采用数据流图进行数值计算的深度学习框架：首先在python之中定义一个用来计算的图（有向图），然后由tensorflow使用这个图，并通过C++进行计算。","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#感知机与神经网络的区别在于感知机是以线性组合为自变量输出是0或1的分段函数，神经网络是线性组合+激活函数。\n#为了解决感知机在阶跃处不可导的情况，用sigmoid函数来替代这个阶跃函数。\n#交叉熵损失函数常用于分类问题： Loss = [y_real*log(y_predict) + (1-y_real)*log(1-y_predict)] \n'''逻辑回归的python实现'''\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn.datasets\nnp.random.seed(1)\n#导入make_moon数据集\nX,Y = sklearn.datasets.make_moons(n_samples=200,noise=0.2)\nX,Y = X.T,Y.reshape(1,Y.shape[0])\nplt.scatter(X[0,Y[0,:]==0],X[1,Y[0,:]==0],c='r',marker='s')\nplt.scatter(X[0,Y[0,:]==1],X[1,Y[0,:]==1],c='b',marker='o')\nplt.show()\ndef sigmoid(x):\n    y = 1/(1+np.exp(-x))\n    return y\ndef optimizer(X,Y,num_iterations=200,learning_rate=0.01):\n    cost = []\n    m = X.shape[1]\n    dim = X.shape[0]\n    W = np.zeros((dim,1))\n    b = 0\n    for i in range(num_iterations):\n        Z = np.dot(W.T,X) + b\n        Y_hat = sigmoid(Z)\n        J = -1.0/m * np.sum(Y*np.log(Y_hat)+(1-Y)*np.log(1-Y_hat))\n        cost.append(J)\n        dW = 1.0 / m *np.dot(X,(Y_hat - Y).T)\n        db = 1.0 / m *np.sum(Y_hat-Y)\n        W = W - learning_rate * dW\n        b = b - learning_rate * db\n        if (i+1) % 20 == 0:\n            print('Iteration: {},J:{}'.format(i+1,J))\n    return W,b,cost\nW,b,cost = optimizer(X,Y,num_iterations=1000,learning_rate=0.1)\nplt.plot(cost,c='g')\nplt.xlabel('iter')\nplt.ylabel('Loss')\nplt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''神经网络的python实现'''\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn.datasets\nnp.random.seed(1)\n#导入make_moon数据集\nX,Y = sklearn.datasets.make_moons(n_samples=200,noise=0.2)\nX,Y = X.T,Y.reshape(1,Y.shape[0])\nm = X.shape[1] #样本个数\ndim = X.shape[0] #样本维度\ndef sigmoid(x):\n    y = 1/(1+np.exp(-x))\n    return y\ndef initialize_parameters(n_x,n_h,n_y):\n    np.random.seed(0)\n    W1 = np.random.randn(n_h,n_x)\n    b1 = np.zeros((n_h,1))\n    W2 = np.random.randn(n_y,n_h)\n    b2 = np.zeros((n_y,1))\n    parameters = {\n        'W1':W1,\n        'W2':W2,\n        'b1':b1,\n        'b2':b2,\n    }\n    #parameters初始化程序返回的是一个字典，这样比直接返回很多参数看起来舒服多了\n    return parameters\ndef forward_propagation(X,parameters):\n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    b1 = parameters['b1']\n    b2 = parameters['b2']\n    Z1 = np.dot(W1,X) + b1\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(W2,A1) + b2\n    A2 = sigmoid(Z2)\n    cache = {\n        'Z1':Z1,\n        'A1':A1,\n        'Z2':Z2,\n        'A2':A2,\n    }\n    return A2,cache\n#交叉熵损失函数\ndef compute_loss(A2,Y):\n    m = Y.shape[1]\n    cross_entropy = -(Y*np.log(A2)+(1-Y)*np.log(1-A2))\n    cost = 1.0/m * np.sum(cross_entropy)\n    return cost\ndef back_propagation(X,Y,parameters,cache):\n    m = X.shape[1]\n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    b1 = parameters['b1']\n    b2 = parameters['b2']\n    Z1 = cache['Z1']\n    Z2 = cache['Z2']\n    A1 = cache['A1']\n    A2 = cache['A2']\n    \n    dZ2 = A2 - Y\n    dW2 = 1.0 / m * np.dot(dZ2,A1.T)\n    db2 = 1.0 / m * np.sum(dZ2,axis=1,keepdims=True)\n    dZ1 = 1.0 / m * np.dot(W2.T,dZ2) * (1-np.power(A1,2))\n    dW1 = 1.0 / m * np.dot(dZ1,X.T)\n    db1 = 1.0 / m * np.sum(dZ1,axis=1,keepdims=True)\n    \n    grads = {\n        'dW1':dW1,\n        'dW2':dW2,\n        'db1':db1,\n        'db2':db2,\n    }\n    return grads\n#更新参数\ndef update_parameters(parameters,grads,learning_rate=0.1):\n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    b1 = parameters['b1']\n    b2 = parameters['b2']\n    dW1 = grads['dW1']\n    dW2 = grads['dW2']\n    db1 = grads['db1']\n    db2 = grads['db2']\n    W1 = W1 - learning_rate * dW1\n    W2 = W2 - learning_rate * dW2\n    b1 = b1 - learning_rate * db1\n    b2 = b2 - learning_rate * db2\n    parameters = {\n        'W1':W1,\n        'W2':W2,\n        'b1':b1,\n        'b2':b2,\n    }\n    return parameters\n\n#终于造完轮子啦！开始搭建网络！ 这是一个 dim -> 3 -> 1的分类网络，最后一层激活函数是sigmoid。\ndef nn_model(X,Y,n_h=3,num_iterations=200,learning_rate=0.1):\n    n_x = X.shape[0]\n    n_y = 1\n    parameters = initialize_parameters(n_x,n_h,n_y)\n    for i in range(num_iterations):\n        A2,cache = forward_propagation(X,parameters)\n        cost = compute_loss(A2,Y)\n        grads = back_propagation(X,Y,parameters,cache)\n        parameters = update_parameters(parameters,grads,learning_rate)\n        \n        if (i+1) % 20 == 0:\n            print('Iter:{},Loss:{}'.format(i+1,cost))\n    return parameters\n            \nparameters = nn_model(X,Y,n_h = 3,num_iterations=500,learning_rate=0.2)\n\ndef predict(X,parameters):\n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    b1 = parameters['b1']\n    b2 = parameters['b2']\n    Z1 = np.dot(W1,X) + b1\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(W2,A1) + b2\n    A2 = sigmoid(Z2)\n    Y_pred = np.zeros((1,X.shape[1]))\n    Y_pred[A2 > 0.5] = 1\n    return Y_pred\n\n#绘制\nfrom matplotlib.colors import ListedColormap\nx_min,x_max = X[0,:].min() - 0.5, X[0,:].max() + 0.5\ny_min,y_max = X[1,:].min() - 0.5, X[1,:].max() + 0.5\nstep = 0.001\nxx,yy = np.meshgrid(np.arange(x_min,x_max,step),np.arange(y_min,y_max,step))\nz = predict(np.c_[xx.ravel(),yy.ravel()].T,parameters) # np.c_按行拼接\nz = z.reshape(xx.shape)\nplt.contourf(xx,yy,z,cmap=plt.cm.Spectral) #绘制边界\nplt.scatter(X[0,Y[0,:]==0],X[1,Y[0,:]==0],c='g',marker='s',label='group 1 ')\nplt.scatter(X[0,Y[0,:]==1],X[1,Y[0,:]==1],c='y',marker='o',label='group 2 ')\nplt.legend()\nplt.show()\n\n","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-09-17T12:32:24.146456Z","iopub.execute_input":"2021-09-17T12:32:24.147036Z","iopub.status.idle":"2021-09-17T12:32:28.350989Z","shell.execute_reply.started":"2021-09-17T12:32:24.146998Z","shell.execute_reply":"2021-09-17T12:32:28.350126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----------------------------------------优化神经网络----------------------------------------\n\n传统的梯度下降算法在深层网络中表现得并不好。\n在训练神经网络时，要尽量避免欠拟合和过拟合。让模型既可以在训练集上准确率较高，同时又有很好的泛化能力。\n欠拟合 和 过拟合 分别对应 高偏差 和 高方差。偏差度量了算法的期望预测值与真实结果的偏离程度。\n                                      方差度量了训练集的变化导致学习性能的变化。\n                                      噪声表示任何学习算法的泛化能力的下界，描述了学习问题本身的难度。\n误差 = 偏差 + 方差 + 噪声\n\n一般情况下我们将数据集分成三个部分，训练集用来训练神经网络算法模型，验证集用来验证不同算法的表现情况，以便从中选择最好的算法进行测试。测试集用于测试算法的实际表现，作为该算法的无偏估计。\n一般按照60%，20%，20%划分。\n一般情况下可以根据训练集和测试集的准确率来判断是否发生了过拟合。\n欠拟合时增加网络复杂度。\n过拟合时：(1) L1,L2正则化。(2) Dropout. (3) 增加训练样本（数据增强）\n一般只对W进行正则化，不对b进行正则化。\nL1正则化的最优解边界是正方形，L2正则化是圆形，因此L1正则化更具稀疏性。\nL2正则化使模型的解偏向于范数较小的W，L1的优良性能是产生稀疏解，导致W中很多项变成0，计算量小，只保留模型的关键特征。\n\nDropout正则化的传统方法是：在模型训练阶段,每层的所有神经元都将以概率p保留，在模型测试阶段保留所有神经元，但是每层神经元输出激活值*p，这样做是为了保证训练和测试时输出同期望。\n现在更常用的一种正则化方法叫做Inverted Dropout：在模型训练阶段，每层的所有神经元以概率p保留，然后神经元的输出/p。这么做是为了提高模型测试时的运算速度，简化模型。\n而且如果改了p值，只需要修改训练阶段的代码。\n使用Dropout时候有以下几点实用性建议：\n(1)不同隐藏层的keep_prob可以不同。一般来说神经元较多的隐藏层keep_prob可以小一点，反之可以多一点。\n(2)不建议对输入层dropout.\n(3)越容易过拟合的隐藏层，kepp_prob越小，可以采用交叉验证的测试。\n\n数据增强(图片)\ncolor jittering:图像亮度，饱和度，对比度\nrandom scale:尺度变换\nrandom crop:随机裁剪缩放\nhorizontal/vertical flip：水平/垂直翻转\nshift:平移变换\nrotation/reflection:旋转/仿射变换\nnoise:高斯噪声/模糊处理\n\n经过标准化之后，本来分布极不平衡的数据，现在呈现了标准的正态分布，不同输入数值的范围变得相似了。\n注意！！！测试集要拿训练集的均值方差进行标准化。\nb可以初始化为0，但是W千万不能初始化为0.\n\n神经网络需要调节的超参数较多：学习率，网络层数，各隐藏层单元数，mini-batch样本数，动量降低因子\n                         Adam的beta1,beta2 L1，L2得正则化系数。\n选择超参数的方法目前只有网格化训练。","metadata":{}},{"cell_type":"code","source":"#为什么要进行卷积运算呢？\n#边缘检测（edge detection）是图像处理中最常用的算法之一，其他目的是检测图片中包含的边缘信息，例如水平边缘，垂直边缘等轮廓信息。\n#   垂直边缘检测算子                            水平边缘检测算子\n#   [                                           [\n#   [1,0,-1],                                   [1,1,1],\n#   [1,0,-1],                                   [0,0,0],\n#   [1,0,-1],                                   [-1,-1,-1],\n#   ]                                           ]\n#因此，对于卷积层来说，如果我们想检测图片的各种边缘特征，而不仅限于垂直边缘和水平边缘，那么卷积核的具体数值一般需要通过模型训练得到。\n#可以通过padding填充使得卷积之后的图片大小不变，避免图片边缘对输出的贡献减小\n\n#对于RGB图像我们选用3个卷积核作为1组，1组卷积核只能输出一个类灰度图，得到这个类灰度图的方式是 1组中的3个分别对RGB卷积，然后累加在一起。\n#也是说，对于一组卷积核(3个)：输入是RGB三通道，输出是三个单通道累加在一起的单通道\n#卷积完了之后的多张单通道类灰度图要分别进行激活 g(feature_map + b)\n#最浅层的卷积核倾向于学习原图的点，颜色等基础特征。深层次卷积核学习轮廓特征。\n\n#池化层的作用在于减小featuremap尺寸，提高运算速度，减小噪声影响，让各特征更具有鲁棒性。\n#最后的全连接层就是将比如 10*10*5的张量拉伸成500，然后全连接。\n#为什么卷积层最后要引入全连接呢？ 直观的说卷积层提取的是局部视野，如果类比眼睛，卷积层就是将图像翻译成神经信号的工具，而全连接层好比我们的大脑，特能利用卷积层提取的特征做出相应的决策。\n#CNN的强大之处就在于卷积层强大的特征提取能力，我们完全可以使用卷积神经网络将特征提取出来，然后实用全连接或者决策树，SVM等各种机器学习算法模型来进行分类。\n#一般来说前期每张纸大，但是纸张数目少，后面纸面小，但是纸张数目多。\n#\n#使用LeNet-5识别mnist，准确率达到99.2%，是一款简单而且功能强大的网络。\n#Alexnet比LeNet-5更复杂，但性能更强大。","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n使用pytorch书写简单的CNN\n'''\nfrom torch.utils.data import DataLoader, Dataset\nfrom urllib.request import urlopen\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n#https://blog.csdn.net/qq_38156104/article/details/107808990\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ntransform = transforms.Compose([transforms.ToTensor()]) \n#torchvision.transforms.ToTensor ：把一个取值范围是[0,255]的PIL.Image或者shape为(H,W,C)的numpy.ndarray，\n#转换成形状为[C,H,W]，取值范围是[0,1.0]的torch.FloadTensor","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:32:28.246434Z","iopub.execute_input":"2021-09-18T09:32:28.24692Z","iopub.status.idle":"2021-09-18T09:32:32.747534Z","shell.execute_reply.started":"2021-09-18T09:32:28.246838Z","shell.execute_reply":"2021-09-18T09:32:32.746727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_na = np.array(pd.read_csv('../input/mnist-in-csv/mnist_train.csv'))\n#test_na = np.array(pd.read_csv('../input/mnist-in-csv/mnist_test.csv'))\n#train_na = [train_na[:,1:786].reshape(60000,28,28),train_na[:,0]]\n#test_na = [test_na[:,1:786].reshape(10000,28,28),test_na[:,0]]\n#im = train_na[0][0,:,:]\n#plt.imshow(im, cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2021-09-18T03:20:21.452526Z","iopub.execute_input":"2021-09-18T03:20:21.4528Z","iopub.status.idle":"2021-09-18T03:20:21.456996Z","shell.execute_reply.started":"2021-09-18T03:20:21.45277Z","shell.execute_reply":"2021-09-18T03:20:21.456384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#从CSV数据到dataloader\nclass MNIST_data(Dataset):    \n    def __init__(self, file_path, transform = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,))])):\n        df = pd.read_csv(file_path)\n    \n        self.X = df.iloc[:,1:].values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n        self.y = torch.from_numpy(df.iloc[:,0].values)    \n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        if self.y is not None:\n            return self.transform(self.X[idx]), self.y[idx]\n        else:\n            return self.transform(self.X[idx])\n        \nbatch_size = 4\ntrain_dataset = MNIST_data('../input/mnist-in-csv/mnist_train.csv')\ntest_dataset = MNIST_data('../input/mnist-in-csv/mnist_test.csv')\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:32:32.74904Z","iopub.execute_input":"2021-09-18T09:32:32.749417Z","iopub.status.idle":"2021-09-18T09:32:38.174549Z","shell.execute_reply.started":"2021-09-18T09:32:32.74938Z","shell.execute_reply":"2021-09-18T09:32:38.173795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net,self).__init__()\n        self.conv1 = nn.Conv2d(1,6,5) # 输入通道：1    输出通道：6     卷积核： 5*5\n        self.pool1 = nn.MaxPool2d(2,2) # 2*2的池化核\n        self.conv2 = nn.Conv2d(6,16,5)\n        self.pool2 = nn.MaxPool2d(2,2) # 2*2的池化核\n        self.fc1   = nn.Linear(16*4*4,120)\n        self.fc2   = nn.Linear(120,84)\n        self.fc3   = nn.Linear(84,10)\n    def forward(self,x):\n        x = F.relu(self.conv1(x))\n        x = self.pool1(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool2(x)\n        x = x.view(-1,16*4*4) #拉伸成一维\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\nnet = Net()\n#print(net) 继承自nn.Module的网络可以直接打印查看网络结构。\n#回归问题常用均方差作为损失函数nn.MSELoss()，分类问题常用交叉熵作为损失函数nn.CrossEntropyLoss()。\ncriterion = nn.CrossEntropyLoss()  \noptimizer = optim.Adam(net.parameters(),lr=0.0001)\n\n\nif torch.cuda.is_available():\n     net = net.cuda()\n     criterion = criterion.cuda()\n\nnet.train() \nnum_epoches = 5\ncost = []\ntransform = transforms.Compose([transforms.ToTensor()]) \nfor epoch in range(num_epoches):\n    running_loss = 0.0\n    for i,data in enumerate(train_loader,0):\n        inputs,labels = data\n        inputs,labels = inputs.cuda(),labels.cuda()\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs,labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        if (i+1) % 2000 == 0:\n            print('epoch:{},mini-batch:{},loss:{}'.format(epoch+1,i+1,running_loss/2000))\n            cost.append(running_loss/2000)\n            running_loss = 0.0","metadata":{"execution":{"iopub.status.busy":"2021-09-18T06:24:04.099522Z","iopub.execute_input":"2021-09-18T06:24:04.099783Z","iopub.status.idle":"2021-09-18T06:29:11.412702Z","shell.execute_reply.started":"2021-09-18T06:24:04.099755Z","shell.execute_reply":"2021-09-18T06:29:11.411952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 使用GPU时一定要加上：\nif torch.cuda.is_available():\n      net = net.cuda()\n      criterion = criterion.cuda()\n      \n ......\n \n \n inputs,labels = data\n inputs,labels = inputs.cuda(),labels.cuda()","metadata":{}},{"cell_type":"code","source":"#分别验证在train_set和test_set上的准确率。\ncorrect = 0\ntotal = 0\nnet.eval()\nwith torch.no_grad():\n    for data in train_loader:\n        inputs,labels = data\n        inputs,labels = inputs.cuda(),labels.cuda()\n        outputs = net(inputs)\n        _,predicted = torch.max(outputs.data,1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    print('Acc of trainset = {}'.format(100*correct/total))\n    correct = 0\n    total = 0\n    for data in test_loader:\n        inputs,labels = data\n        inputs,labels = inputs.cuda(),labels.cuda()\n        outputs = net(inputs)\n        _,predicted = torch.max(outputs.data,1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    print('Acc of testset = {}'.format(100*correct/total))  ","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:32:52.632664Z","iopub.execute_input":"2021-09-18T09:32:52.63292Z","iopub.status.idle":"2021-09-18T09:32:52.73172Z","shell.execute_reply.started":"2021-09-18T09:32:52.632893Z","shell.execute_reply":"2021-09-18T09:32:52.730543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#list(net.named_parameters())  #打印参数","metadata":{"execution":{"iopub.status.busy":"2021-09-18T06:40:38.436854Z","iopub.execute_input":"2021-09-18T06:40:38.437115Z","iopub.status.idle":"2021-09-18T06:40:38.440327Z","shell.execute_reply.started":"2021-09-18T06:40:38.437086Z","shell.execute_reply":"2021-09-18T06:40:38.439654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"传统的前馈神经网络无法解决 样本维度不一致、label维度不一致 的问题。\n传统的前馈神经网络不具备共享特征的能力，例如“张三”在句首中是人名，那么在其他地方也可能是人名。这是共享特征的结果。\n比如输入“I am hungry”就是分别对每个单词进行onehot编码，即给每个单词找到一个对应序号。生成的向量在序号处为1，其余处为0，向量长度与单词表的长度相同。\nRNN的输出序列y与输入序列x大小相等。\na是记忆单元，是当前层的输出也是下一层的输入。这样就能实现将序列信号中单个信号的信息传递给后面的信号，实现信号的记忆和传递功能。\na<0>一般是零向量。\n有时候可能距离很远的两个单词具有很强的联系性，但是由于梯度消失无法在网络中表达出来。\n因此我们使用了GRU（Gated Recurrent Unit）解决这一问题。","metadata":{}},{"cell_type":"code","source":"'''\n使用RNN识别MNIST数据集\n'''\n#先执行前面的导入torch库，并定义train_loader,test_loader\nclass RNNnet(nn.Module):\n    def __init__(self):\n        super(RNNnet,self).__init__()\n        self.rnn = nn.LSTM(input_size=28,hidden_size=84,num_layers=2,batch_first=True)\n        self.out = nn.Linear(84,10)\n    def forward(self,x):\n        r_out,(h_n,h_c) = self.rnn(x,None)\n        #选择图片最后一行作为RNN模型输出\n        out = self.out(r_out[:,-1,:])\n        return out\nnet = RNNnet()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(net.parameters(),lr=0.0001)\nnet = net.cuda()\ncriterion = criterion.cuda()\n#optimizer没有cuda方法嗷！\nnum_epoches = 5\ncost = []\nfor epoch in range(num_epoches):\n    running_loss = 0.0\n    for i,data in enumerate(train_loader,0):\n        inputs,labels = data\n        inputs,labels = inputs.cuda(),labels.cuda()\n        inputs = inputs.view(-1,28,28)\n        optimizer.zero_grad()\n        \n        outputs = net(inputs)\n        loss = criterion(outputs,labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        if i % 2000 == 1999:\n            print(\"epoch:{},mini-batch:{},loss:{}\".format(epoch+1,i+1,running_loss/2000))\n            cost.append(running_loss/2000) \n            running_loss = 0.0\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:35:12.42185Z","iopub.execute_input":"2021-09-18T09:35:12.422098Z","iopub.status.idle":"2021-09-18T09:42:24.579454Z","shell.execute_reply.started":"2021-09-18T09:35:12.42207Z","shell.execute_reply":"2021-09-18T09:42:24.578672Z"},"trusted":true},"execution_count":null,"outputs":[]}]}