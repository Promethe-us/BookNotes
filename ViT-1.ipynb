{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-06T09:01:43.422086Z",
     "iopub.status.busy": "2022-02-06T09:01:43.421642Z",
     "iopub.status.idle": "2022-02-06T09:01:45.592388Z",
     "shell.execute_reply": "2022-02-06T09:01:45.591159Z",
     "shell.execute_reply.started": "2022-02-06T09:01:43.422049Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15260/3859520504.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_15260/3859520504.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_15260/3859520504.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(name, mode)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cifar10'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cifar10'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCifar10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/vision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_file, mode, transform, download, backend)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_file is not set and downloading automatically is disabled\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             self.data_file = _check_exists_and_download(\n\u001b[0;32m--> 123\u001b[0;31m                 data_file, self.data_url, self.data_md5, 'cifar', download)\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/dataset/common.py\u001b[0m in \u001b[0;36m_check_exists_and_download\u001b[0;34m(path, url, md5, module_name, download)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         raise ValueError('{} not exists and auto download disabled'.format(\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/dataset/common.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(url, module_name, md5sum, save_name)\u001b[0m\n\u001b[1;32m     69\u001b[0m                             if save_name is None else save_name)\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmd5file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmd5sum\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/dataset/common.py\u001b[0m in \u001b[0;36mmd5file\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mhash_md5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhash_md5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Resnet18在cifar10上训练\n",
    "'''\n",
    "import paddle\n",
    "import paddle.nn as nn \n",
    "\n",
    "class Identity(nn.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,x):\n",
    "        return x\n",
    "\n",
    "class Block(nn.Layer):\n",
    "    def __init__(self,in_dim,out_dim,stride):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2D(in_channels=in_dim,\n",
    "                               out_channels=out_dim,\n",
    "                               kernel_size=3,\n",
    "                               stride=stride,\n",
    "                               padding=1,\n",
    "                               bias_attr=False)\n",
    "        self.bn1 = nn.BatchNorm2D(out_dim)\n",
    "        self.conv2 = nn.Conv2D(in_channels=out_dim,\n",
    "                               out_channels=out_dim,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias_attr=False)        \n",
    "        self.bn2 = nn.BatchNorm2D(out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        if stride == 2 or in_dim != out_dim:\n",
    "            self.downsample = nn.Sequential(*[\n",
    "                nn.Conv2D(in_dim,out_dim,1,stride=stride),\n",
    "                nn.BatchNorm2D(out_dim)\n",
    "            ])\n",
    "        else:\n",
    "            self.downsample = Identity()\n",
    "\n",
    "    def forward(self,x):\n",
    "        h = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        identity = self.downsample(h)\n",
    "        x = x + identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet18(nn.Layer):\n",
    "    def __init__(self,in_dim=64,num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.conv1 = nn.Conv2D(in_channels=3,\n",
    "                               out_channels=in_dim,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias_attr=False)\n",
    "        self.bn1 = nn.BatchNorm2D(in_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        # blocks\n",
    "        self.layer1 = self._make_layer(dim=64,n_blocks=2,stride=1)\n",
    "        self.layer2 = self._make_layer(dim=128,n_blocks=2,stride=2)\n",
    "        self.layer3 = self._make_layer(dim=256,n_blocks=2,stride=2)\n",
    "        self.layer4 = self._make_layer(dim=512,n_blocks=2,stride=2)\n",
    "        \n",
    "        # head layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2D(1)\n",
    "        self.classifier = nn.Linear(8192,num_classes)\n",
    "    \n",
    "    def _make_layer(self,dim,n_blocks,stride):\n",
    "        layer_list = []\n",
    "        layer_list.append(Block(self.in_dim,dim,stride=stride))\n",
    "        self.in_dim = dim \n",
    "        for i in range(n_blocks):\n",
    "            layer_list.append(Block(self.in_dim,dim,stride=1))\n",
    "        return nn.Sequential(*layer_list)   \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def test_model():\n",
    "    t = paddle.randn([4,3,32,32])\n",
    "    model = ResNet18()\n",
    "    out = model(t)\n",
    "    paddle.summary(model,(4,3,32,32))\n",
    "\n",
    "# dataset----------------------------------------------------------------------------------\n",
    "from paddle.io import Dataset\n",
    "from paddle.io import DataLoader \n",
    "from paddle.vision import datasets \n",
    "import paddle.vision.transforms as T  \n",
    "\n",
    "def get_transforms(mode='train'):\n",
    "    if mode == 'train':\n",
    "        data_transforms = T.Compose([\n",
    "            T.RandomCrop(32,padding=4),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.4914,0.4822,0.4465],std=[0.2023, 0.1994, 0.2010])\n",
    "            ])\n",
    "    else:\n",
    "        data_transforms = transforms.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.4914,0.4822,0.4465],std=[0.2023, 0.1994, 0.2010])\n",
    "        ])\n",
    "    return data_transforms \n",
    "\n",
    "def get_dataset(name='cifar10',mode='train'):\n",
    "    if name == 'cifar10':\n",
    "        dataset = datasets.Cifar10(mode=mode,transform=get_transforms(mode=mode))\n",
    "    return dataset \n",
    "\n",
    "def get_dataloader(dataset,batch_size=128,mode='train'):\n",
    "    dataloader = DataLoader(dataset,batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "# utils----------------------------------------------------------------------------------\n",
    "class AverageMeter():\n",
    "    def __init__(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.cnt = 0\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.cnt = 0\n",
    "    def update(self,val,n=1):\n",
    "        self.sum += val * n\n",
    "        self.cnt += n\n",
    "        self.avg = self.sum / self.cnt \n",
    "\n",
    "# start train!\n",
    "def train_one_epoch(model,dataloader,criterion,optimizer,epoch):\n",
    "    model.train()\n",
    "    loss_meter = AverageMeter()\n",
    "    acc_meter = AverageMeter()\n",
    "    for batch_id,data in enumerate(dataloader):\n",
    "        image = data[0]\n",
    "        label = data[1]\n",
    "\n",
    "        out = model(image)\n",
    "        loss = criterion(out,label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        pred = nn.functional.softmax(out,axis=1)\n",
    "        acc = paddle.metric.accuracy(pred,label.unsqueeze(-1))\n",
    "\n",
    "        batch_size = image.shape[0]\n",
    "        loss_meter.update(loss.cpu().numpy()[0],batch_size)\n",
    "        acc_meter.update(acc.cpu().numpy()[0],batch_size)\n",
    "\n",
    "        if batch_id > 0:\n",
    "            print(f'----- Batch {batch_id}, Loss {loss_meter.avg}, Acc {acc_meter.avg}')\n",
    "            \n",
    "def main():\n",
    "    EPOCH_NUM = 200\n",
    "    BATCH_SIZE = 16 \n",
    "    \n",
    "    model = ResNet18(num_classes=10)\n",
    "    train_dataset = get_dataset(mode='train')\n",
    "    train_dataloader = get_dataloader(train_dataset,mode='train',batch_size=BATCH_SIZE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scehduler =  paddle.optimizer.lr.CosineAnnealingDecay(0.02,EPOCH_NUM)\n",
    "    optimizer = paddle.optimizer.Momentum(learning_rate=scehduler,\n",
    "                                          parameters=model.parameters(),\n",
    "                                          momentum=0.9,\n",
    "                                          weight_decay=5e-4)\n",
    "    for i in range(EPOCH_NUM):\n",
    "        train_one_epoch(model,train_dataloader,criterion,optimizer,i)  \n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](http://tiebapic.baidu.com/forum/w%3D580/sign=4bb503daaa36afc30e0c3f6d8318eb85/6fbede00baa1cd113a713c3ae412c8fcc2ce2d43.jpg)\n",
    "  \n",
    "- vit并没有使用decoder,只用了encoder  \n",
    "- ViT的结构很简单: \n",
    "image tokens -> encoders -> class label\n",
    "- 如何得到图像分词(image tokens):\n",
    "![title](http://tiebapic.baidu.com/forum/w%3D580/sign=abcddbc5931b9d168ac79a69c3deb4eb/82ac39d3d539b600e7f846adb450352ac65cb75d.jpg)\n",
    "- 通常在Feed Forward中我们用的激活函数是Gelu \n",
    "![title](https://pic4.zhimg.com/v2-efda13b22f7bee7fbb02eb529ba017c3_r.jpg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-06T09:06:33.348145Z",
     "iopub.status.busy": "2022-02-06T09:06:33.347797Z",
     "iopub.status.idle": "2022-02-06T09:06:33.458309Z",
     "shell.execute_reply": "2022-02-06T09:06:33.457595Z",
     "shell.execute_reply.started": "2022-02-06T09:06:33.348112Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 10]\n"
     ]
    }
   ],
   "source": [
    "import paddle \r\n",
    "from PIL import Image \r\n",
    "import numpy as np \r\n",
    "import matplotlib.pyplot as plt \r\n",
    "import paddle.nn as nn\r\n",
    "'''\r\n",
    "预备内容教学\r\n",
    "'''\r\n",
    "if True:\r\n",
    "    '''创建'''\r\n",
    "    # t = paddle.zeros([3,3])\r\n",
    "    # t = paddle.randn([3,3])\r\n",
    "    img = np.array(Image.open('./bingdundun.jpg').resize((560,560)) )\r\n",
    "    #plt.imshow(img)\r\n",
    "    t = paddle.to_tensor(img)\r\n",
    "    '''type是tensor类型，dtype是里面存的数据的类型'''\r\n",
    "    # t = paddle.randint(0,10,[5,15])\r\n",
    "    # qkv = t.chunk(3,-1)\r\n",
    "    # q,k,v = qkv\r\n",
    "    '''patch_embeded'''\r\n",
    "    #----------------------------------\r\n",
    "    class Identity(nn.Layer):\r\n",
    "        def __init__(self):\r\n",
    "            super().__init__()\r\n",
    "        def forward(self,x):\r\n",
    "            return x\r\n",
    "    #----------------------------------\r\n",
    "    class MLP(nn.Layer):\r\n",
    "        def __init__(self,embed_dim,mlp_ratio=4.0,dropout=0.):\r\n",
    "            super().__init__()\r\n",
    "            self.fc1 = nn.Linear(embed_dim,int(embed_dim * mlp_ratio))\r\n",
    "            self.fc2 = nn.Linear(int(embed_dim * mlp_ratio),embed_dim)\r\n",
    "            self.act = nn.GELU()\r\n",
    "            self.dropout = nn.Dropout() \r\n",
    "\r\n",
    "        def forward(self,x):\r\n",
    "            x = self.fc1(x)\r\n",
    "            x = self.act(x)\r\n",
    "            x = self.dropout(x)\r\n",
    "            x = self.fc2(x)\r\n",
    "            x = self.dropout(x)\r\n",
    "\r\n",
    "            return x           \r\n",
    "    #----------------------------------\r\n",
    "    class PatchEmbedding(nn.Layer):\r\n",
    "        def __init__(self,image_size,patch_size,in_channels,embed_dim,dropout=0):\r\n",
    "            super().__init__()\r\n",
    "            self.patch_embed =  nn.Conv2D(in_channels,\r\n",
    "                                          embed_dim,\r\n",
    "                                          kernel_size=patch_size,\r\n",
    "                                          stride = patch_size,\r\n",
    "                                          weight_attr=paddle.ParamAttr(initializer=nn.initializer.Constant(1.0)),\r\n",
    "                                          bias_attr=False)\r\n",
    "            self.dropout = nn.Dropout(dropout)\r\n",
    "        def forward(self,x):\r\n",
    "            # x:[1,3,560,560]\r\n",
    "            x = self.patch_embed(x)\r\n",
    "            # x:[n,embeded_dim,h',w']\r\n",
    "            x = x.flatten(2)# [n,embeded_dim,h'*w']\r\n",
    "            x = x.transpose([0,2,1])# [n,h'*w',embeded_dim]\r\n",
    "            x = self.dropout(x)\r\n",
    "            return x \r\n",
    "    #----------------------------------\r\n",
    "    class Encoder(nn.Layer):\r\n",
    "        def __init__(self,embed_dim):\r\n",
    "            super().__init__()\r\n",
    "            self.attn = Identity()\r\n",
    "            self.attn_norm = nn.LayerNorm(embed_dim)\r\n",
    "            self.mlp = MLP(embed_dim)\r\n",
    "            self.mlp_norm = nn.LayerNorm(embed_dim)\r\n",
    "        def forward(self,x):\r\n",
    "            h = x \r\n",
    "            x = self.attn_norm(x)\r\n",
    "            x = self.attn(x)\r\n",
    "            x = h + x\r\n",
    "            h = x \r\n",
    "            x = self.mlp_norm(x)\r\n",
    "            x = self.mlp(x)\r\n",
    "            x = h + x\r\n",
    "            return x\r\n",
    "\r\n",
    "    #----------------------------------\r\n",
    "    class ViT(nn.Layer):\r\n",
    "        def __init__(self):\r\n",
    "            super().__init__()\r\n",
    "            self.patch_embed = PatchEmbedding(224,7,3,16)\r\n",
    "            layer_list = [Encoder(16) for i in range(5)]\r\n",
    "            self.encoders = nn.LayerList(layer_list)\r\n",
    "            self.head = nn.Linear(16,10)\r\n",
    "            self.avgpool = nn.AdaptiveAvgPool1D(1)\r\n",
    "\r\n",
    "        def forward(self,x):\r\n",
    "            x = self.patch_embed(x)\r\n",
    "            for encoder in self.encoders:\r\n",
    "                x = encoder(x)\r\n",
    "            x = x.transpose([0,2,1]) # [n,h'*w',c] ---> [n,c,h'*w']\r\n",
    "            x = self.avgpool(x) # [n,c,1]\r\n",
    "            x = x.flatten(1) # [n,c]\r\n",
    "            x = self.head(x)\r\n",
    "            return x\r\n",
    "    # ====================================================\r\n",
    "    # sample = paddle.randn([28,28]).astype('float32')\r\n",
    "    # sample = sample.reshape([1,1,28,28])\r\n",
    "    # patch_embeded = PatchEmbedding(image_size=28,patch_size=7,in_channels=1,embed_dim=1)\r\n",
    "    # out = patch_embeded(sample)\r\n",
    "    # mlp = MLP(embed_dim=1)\r\n",
    "    # out = mlp(out)\r\n",
    "    # print(out.shape)\r\n",
    "    img = np.array(Image.open('./bingdundun.jpg').resize((224,224)))\r\n",
    "    img = img.transpose(2,1,0)\r\n",
    "    imgs = np.array([img,img,img,img]).astype('float32')\r\n",
    "    imgs_t = paddle.to_tensor(imgs,dtype='float32')\r\n",
    "    model = ViT()\r\n",
    "    out = model(imgs_t)\r\n",
    "    print(out.shape)\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ViT的本质就是使用CNN将图片转为S而群策，然后再使用Attention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
